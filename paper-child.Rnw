\newcommand{\st}[1]{{\color{orange} #1}}
\newcommand{\hh}[1]{{\color{magenta} #1}}

<<setup, echo = FALSE, message = FALSE, warning = FALSE, purl=FALSE>>=
options(replace.assign=TRUE,width=70, digits=3)
require(knitr)
opts_chunk$set(fig.path='figure/', cache.path='cache/', fig.align='center', fig.pos='h', out.width='.99\\textwidth', par=TRUE, cache=FALSE, concordance=TRUE, autodep=TRUE, message=F, warning=F, echo = FALSE, dev="cairo_pdf", fig.width = 6, fig.height = 6)#, root.dir = "~/Desktop/Dissertation/SAOM-removing-blindfold/")
@

<<pkgs>>=
# if (packageVersion("ggplot2") > "2.2.0"){
#   remove.packages("ggplot2")
#   install.packages("ggplot2")
# }
library(ggplot2)
library(tidyverse)
library(RSiena)
library(network)
library(sna)
library(geomnet)
library(GGally)
# devtools::install_github("sctyner/netvizinf")
library(netvizinf)
library(RColorBrewer)
library(gridExtra)
library(cowplot)
library(extrafont)
loadfonts(quiet = T)

ThemeNoNet <- theme_bw() %+replace%
            theme(plot.title = element_text(size = 10,
                                            face = 'plain',
                                            angle = 0,
                                            family="Times New Roman"),
                  axis.title.x = element_text(size = 10,
                                            face = 'plain',
                                            angle = 0,
                                            family="Times New Roman"),
                  axis.title.y = element_text(size = 10,
                                            face = 'plain',
                                            angle = 90,
                                            family="Times New Roman"),
                  axis.text.x = element_text(size = 10,
                                            face = 'plain',
                                            angle = 0,
                                            family="Times New Roman"),
                  # axis.text.x.top = element_text(size = 10,
                  #                           face = 'plain',
                  #                           angle = 0,
                  #                           family="Times New Roman"),
                  # axis.text.x.bottom = element_text(size = 10,
                  #                           face = 'plain',
                  #                           angle = 0,
                  #                           family="Times New Roman"),
                  axis.text.y = element_text(size = 10,
                                            face = 'plain',
                                            angle = 0,
                                            family="Times New Roman"),
                  # axis.text.y.left = element_text(size = 10,
                  #                           face = 'plain',
                  #                           angle = 0,
                  #                           family="Times New Roman"),
                  # axis.text.y.right = element_text(size = 10,
                  #                           face = 'plain',
                  #                           angle = 0,
                  #                           family="Times New Roman"),
                  strip.text.x = element_text(size = 10,
                                            face = 'plain',
                                            angle = 0,
                                            family="Times New Roman",
                                            margin = margin(t = 3, r = 0, b = 3, l = 0, unit = "pt")),
                  strip.text.y =element_text(size = 10,
                                            face = 'plain',
                                            angle = 90,
                                            family="Times New Roman",
                                            margin = margin(t = 3, r = 3, b = 3, l = 3, unit = "pt")),
                  strip.background = element_rect(colour = "black", fill = "white")
                    )
ThemeNet <- theme_net() %+replace%
            theme(plot.title = element_text(size = 10,
                                            face = 'plain', angle = 0, family = "Times New Roman"),
                  strip.text.x = element_text(size = 10,
                                            face = 'plain', angle = 0, family = "Times New Roman",
                                            margin = margin(t = 3, r = 0, b = 3, l = 0, unit = "pt")),
                  strip.text.y =element_text(size = 10,
                                            face = 'plain', angle = 0, family = "Times New Roman",margin = margin(t = 3, r = 0, b = 3, l = 0, unit = "pt")),
                  strip.background = element_rect(colour = "black", fill = "white"))
@

<<getdata, echo=FALSE>>=
set.seed(823746)
load("data/ansnullpaper.rda")
ansnullchains <- get_chain_info(ansnull)
load("data/M1sims1000.RData")
load("data/M2sims1000.RData")
load("data/M3sims1000.RData")
@

\section{Introduction \& Background}
% peices of old intro
%Social networks, such as collaboration networks between academic researchers or friendship networks 
% new intro 
%As humans and researchers, it is our instint to try to learn as much as we can about the world around us. 
Scholars have been trying to understand relationships between people, or \emph{social networks}, for decades, especially in the social sciences. There are a host of quantitative methods available for analyzing social networks, including statistical models, such as exponential random graph models (ERGM) and latent space models (LSM), that strive to capture the underlying network-generating mechanism \citep{ergm, lsm}. The ERGM and LSM families are examples of static network models, which focus on a single ``snapshot" of a network and ``global network statistics" such as outdegree distribution \citep{goldenberg09}. Other static network model families include the Erd\"{o}s-R\'{e}nyi random graph model and the exchangeable graph model \citep{goldenberg09}. 
% describe more about the ergms. cite goldenberg. answer the questions: how are they analyzed? What can they do? what can't they do? HOw are they applied? 
These static models, however, are not optimal for modeling real networks because they do not account for changes in the network as time passes. In real networks, edges may be created or destroyed, and nodes can be born or die \citep{goldenberg09}. Dynamic models for networks, which model networks observed at many points in time, are the more realistic alternative to static network models. 

Dynamic network models do not have as rich of a history as static network models, largely due to the historical lack of data and available analytical tools \citep{goldenberg09}. With the advent of social media and ever-increasing computational power, however, more and more researchers are turning their attention toward dynamic network models.  Some of the classical models, such as the Erd\"{o}s-R\'{e}nyi model and the small-world model of \citet{watts} are pseudo-dynamic because they could be applied in a dynamic way but rarely are \citep{goldenberg09}.  
% talk up dynamic social network models. they are much more realistic. they don't treat nodes as exchangeable, they allow the nodes to affect the ties. They account for both structural and covariate effects. They are very flexible. They are the ideal model because they are the only ones that have any chance of actually capturing the mechanisms. 
Fully dynamic social network models, however, are more complicated: they incorporate both the network structure and its changes in time.  If we can capture this change mechanism in time, we can truly begin to model the underlying network dynamics that drive social connections. A comprehensive review of dynamic network models is presented in \citet{goldenberg09}. 

\par We are particularly interested one truly dynamic family of network models: continuous time Markov chain (CTMC) models, first introduced in \citet{ctmcorig}. We chose these models because they correspond to our intuition about how social networks are formed, and they are very flexible and widely applicable. In these models, network changes are modeled hierarchically at the node level: nodes are given opportunities to change via a rate function, then the node chooses which edge to change according to its utility function. %first step is the changes happen at the smallest possible level (one edge between two nodes), and subsequent changes condition on past changes. 
In each step of the hierarchy, many possible effects can be implemented to capture the change mechanism. For example,  the rate function could be a simple waiting-time model, or it could depend on a node's indegree or outdegree: we may have reason to believe that the more popular nodes change their ties more frequently. In addition, we can incorporate node covariate information into the hierarchy, such as gender or education level of people in a social network. For instance, the genders of two people could effect whether and how quickly they establish a relationship, and people will likely form more relationships with those who share their education level. Thus, the transition probabilities between two states of a network depend on the network's structure, characteristics of the nodes, or both \citep{saompaper}.  For example, \citet{depression} use a CTMC model to learn about the tendency of depressed adolescents to form friendships and found that depressed students tended to avoid creating new friendships over time. The CTMC model family can also be expanded to model node behavior, as in \citet{chyzh2016}, who applies CTMC models to show that more indirect trade relationships in a country decrease its overall respect for human rights. % This means that node-level covariate effects, in addition to commonly used effects such as reciprocity or transitivity, can be incorporated into the model. 
% Cite some of olga's work. what can you do with these models? cite other work with social networks. 
Thus, with CTMC models we can examine assumptions made about social networks. %For example, we may assume that teenagers who drink heavily are more likely to be friends with other teenagers who drink heavily.  We can assess our assumptions on shared interests into the modeling process through a particular subfamily of CTMC models known as stochastic actor-oriented models (SAOMs) \citep{saompaper}.

\par With the flexibility of CTMC models comes increased complexity and estimation difficulty \citep{saomreview}. Likelihood functions quickly become very complex due to the inherent dependency structure in the data, and therefore computational methods, such as Markov chain Monte Carlo (MCMC), are used to fit models. Methods for maximum likelihood estimation (MLE) for these models were not introduced until \citet{saomsml2010}, and method of moments (MM) estimation is frequently used \citep{RSiena}. %SAOMs are typically fit to dynamic social network data using a series of MCMC phases for finding method of moments (MoM) estimates of model parameters. 
For CTMC parameter estimation, we use the software SIENA, and its \texttt{R} implementation \texttt{RSiena} \citep{rstats, RSiena}. This software is a considerable contribution to the field of social network analysis, allowing for estimation of structural effects, node covariate effects, as well as modeling both network and node changes in a wide variety of CTMC models. This extensive parameter estimation process is largely hidden from the user, and the primary outputs are parameter estimates. Direct simulation from a given network model with fixed parameter values is also made easy with \texttt{RSiena}. This software opens up a great deal of possibilities for network modeling and simulation.% need to emphasize the need for visualization. 

% ok. we know why ctmcs but WHY visualize them? where is the motivation?? who cares and why? 
% talk about lack of goodness of fit diagnostics. 
Since we chose the CTMC models for dynamic social networks, we would like to determine how well they fit data. We chose these models because they match our intuition about how social networks form in reality, and after we fit a model, we will want to assess its goodness-of-fit. In statistical network models generally, the lack of large-sample theory leads to a lack of goodness-of-fit tests \citep{goldenberg09}. Most methods for assessing goodness-of-fit for network models, therefore, involve comparing statistics computed on simulated networks to the same statistics computed on the data, for example in \citet{hunter2008}. More recently, \citet{gofschwein} used score-type tests for forward model selection in the CTMC model context. The \texttt{RSiena} software also contains methods for goodness-of-fit based on network statistics, such as outdegree distribution, where the data values are compared to simulated values as in \citet{hunter2008} \citep{RSiena}. % need to transition to model vis. % say something about how important visualization is in network analysis. 

We propose new methods for goodness-of-fit and other diagnostics that use visualization as a supplement to computational methods. Network visualization, also called graph drawing, is a prominent subfield of network analysis with a rich history: early drawings of complete graphs date back to 1280 CE \citep{knuth2000}. We use this visualization foundation to combine modeling with visualization. Note that we do not propose new methods of visualizing networks, and for a complete review of the field of graph drawing we direct the reader to \citet{graphdrawing}. What is novel in our work is the use of common visualization methods to diagnose network model with model visualization

\emph{Model visualization} complements traditional model diagnostic tools such as $R^2$ values and residual plots to enrich our understanding and interpretation of statistical models \citep{modelvis}. We apply the three principles of model-vis: view the model in the data space, visualize collections of models, and explore the process of fitting the model, to the models and data introduced in Section~\ref{sec:exdatamods}.  By translating the model visualization work of \citet{modelvis} to the dynamic network model space, we provide network researchers with a new suite of tools for visualizing and diagnosing performance of their network models. Though we apply the tools to CTMCs because we find the models intuitive, the methods we use can be applied to any network model, static or dynamic.% do something awesome


In the remainder of this section, we provide an overview of the structure of network data and discuss two common network visualization (net-vis) techniques. Then, we detail the hierarchical model structure of a CTMC following the definition in \citet{saompaper} and ouline the CTMC model fitting process in \texttt{RSiena} \citep{SienaAlgs}.


\subsection{Networks and their Visualizations} \label{sec:netintro}

%We provide a brief introduction to the structure of network data and common network visualization methods. \\

%\noindent \textbf{Network Data Structure:} 
Network data across fields have similar structure: they consist of units of observation and connections between those units. In a social network, the units of observations, called \textit{nodes} or \textit{actors}, are people, while the connections, called \textit{edges} or \textit{ties}, are their relationships with each other. %Networks change over time, like when new relationships are formed in a social network. 
Other network data include inherent variables of interest on the nodes, such as age or gender, and the type of relationship (friends, coworkers, etc) that an edge represents. Dynamic networks are observed at many discrete points in time, where each observation of a network is often called a ``wave" of data.
%These two network data structures pose unique problems to analysts, such as, ``How does the strength of a tie between two nodes affect the overall structure of the network?" or ``Do differences in node variables affect the formation of edges?" We propose to answer these questions and more through visual exploration of network data and models. \\


\noindent \textbf{Visualizing Network Data:} Visualizing network data is inherently challenging due to the structure of the data itself.  Most data visualizations rely on well-defined axes inherited from the data, such as Cartesian coordinates or spatial locations, but network data typically do not have such built-in structure. 

<<toyex, echo = FALSE>>=
set.seed(81393)
nodes <- data.frame(id = 1:5, group = sample(LETTERS[1:2], size = 5, replace = T), stringsAsFactors = F)
g <- igraph::as_data_frame(igraph::random.graph.game(n = 5, p = .4))
toynet <- list(nodes = nodes, edges = rbind(g, c(5,4)))
@
<<toyplots, fig.show='hold', fig.cap="On the left, a node-link diagram of our toy network, with nodes placed using the Kamada-Kawai algorithm. On the right, the corresponding adjacency matrix visualization.", out.width='.49\\textwidth', fig.height=4>>=
toynetdat <- fortify(as.edgedf(toynet$edges), toynet$nodes)
ggplot(data = toynetdat) +
  geom_net(aes(from_id=from_id, to_id=to_id), labelon = T, size = 11, labelcolour = 'white', ecolour = 'black', vjust = .5, hjust = .5, fontsize = 9, arrowgap = .05, directed = T) +
  ThemeNet +
  theme(plot.title = element_text(size = 20)) +
  labs(title = "Node-Link Diagram")

ggplot(data = toynet$edges) +
  geom_tile(aes(x = to, y = -from), color = 'grey20', fill = 'black') +
  geom_segment(aes(x = x, xend = xend), y = -0.5, yend=-5.5, colour="grey30", data = data.frame(x = 0:5+.5, xend= 0:5+.5)) +
  geom_segment(aes(y=-y, yend=-yend), x = 0.5, xend=5.5, colour="grey30", data = data.frame(y = -0:5+.5, yend= -0:5+.5)) +
  scale_x_continuous(breaks = 1:5, labels = 1:5, limits = c(0.5,5.5), position = "top") +
  scale_y_continuous(breaks = (-5):(-1), labels = 5:1, limits = c(-5.5,-.5)) +
  ThemeNoNet +
  theme(panel.grid = element_blank(), aspect.ratio = 1,
        plot.title = element_text(size = 20),
        axis.title.x = element_text(size = 18),
        axis.title.y = element_text(size = 18),
        axis.text.x  = element_text(size = 18),
        axis.text.y  = element_text(size = 18)) +
  labs(title = "Adjacency Matrix Visualization", x = "\nto (alter)", y = "from (ego)\n")
@

The lack of a native coordinate system has given rise two primary net-vis methods: the node-link diagram and the adjacency matrix visualization \citep{knuth2000, adjmatpro}. We demonstrate these with a simpe example. Consider the network with five nodes, $\{1,2,3,4,5\}$, connected by five directed edges:  $\{2 \to 4, 3 \to 4, 1 \to 5, 3 \to 5, 5 \to 4\}$ shown in Figure~\ref{fig:toyplots}.

The first net-vis method, the node-link diagram, represents nodes with points in two dimensions and then represents edges by connecting the points with lines. These lines can also have arrows on them indicating the direction of the edge for directed networks, as shown in Figure~\ref{fig:toyplots}. Because there is often no natural placement of the nodes, they are placed in 2D at random, then adjusted with a layout algorithm. For a complete overview of available node-link layout algorithms, we direct the reader to \citet{drawingalgs}. Some commonly used algorithms, such as the Kamada-Kawai (KK) layout \citep{kamadakawai} and the Fruchterman-Reingold (FR) layout \citep{fruchterman-reingold}, are designed to mimic physical systems, drawing the graphs based on the ``forces" connecting them. %Other algorithms use multi-dimensional scaling or properties of the adjacency matrix (e.g. its eigenstructure), to place the nodes \citep{drawingalgs}. 
In Figure~\ref{fig:toyplots}, we use the KK algorithm, and unless otherwise stated, all other node-link diagrams in this paper are laid out using the KK algorithm \citep{kamadakawai}. 

\par The second method for net-vis uses the adjacency matrix of the network. The adjaceny matrix of a network, $\mathbf{A}(x)$ is defined as:
\begin{equation}\label{eq:adjmat}
  A_{ij}(x) =
  \begin{cases}
             1 &  \quad \text{if } x_{ij} = 1, \quad i \neq j \\
             0 & \quad \text{otherwise}
  \end{cases}
\end{equation}
%In this paper, we only consider the presence or absence of an edge, but if the network has weighted edges, $A_{ij}$ is the weight of the edge from $i \to j$. 
The adjacency matrix visualization for our toy example is also shown in Figure~\ref{fig:toyplots}. This visualization shows the network as a grid, with each row representing the ``from" or ``ego" node and each column representing the ``to" or ``alter" node of an edge. The grid space for $x_{ij}$ (the edge $i\rightarrow j$) is filled in if $A_{ij} = 1$ and is empty if $A_{ij} = 0$. For undirected networks $\mathbf{A}$ is symmetric, so the adjacency matrix visualization is also symmetric, with each edge drawn twice. 

\par Each net-vis method has its own advantages and disadvantages. For instance, paths between two nodes in a network are easier to determine with node-link diagrams than with adjacency matrix visualizations \citep{adjmatviz}. Furthermore, in node-link diagrams, node information can be incorporated into the visualization by coloring or changing the shape of the points, and edge-level information can be incorporated by coloring the lines or changing their thickness. Visualizing a node variable in an adjacency matrix visualization is not as simple, because this type of net-vis features the edges. An adjacency matrix visualization can, however, be useful when the network is very complex, dense, or large \citep{adjmatviz}. % Experimental studies have shown adjacency matrix visualization to be superior to node-link diagrams for large networks. For example, for basic perceptual tasks on networks, including node and edge count, adjacency matrix visualizations outperform node-link diagrams as the size and density of the network increases \citep{adjmatviz}. One drawback of the adjacency matrix visualization that \citeauthor{adjmatviz} found was that edges are overrepresented for undirected graphs, due to the symmetry of $\mathbf{A}$: the edge $i \rightarrow j$ for $i \neq j$ appears in $\mathbf{A}$ twice: in $A_{ij}$ and $A_{ji}$, and so it also appears twice in the adjacency matrix visualization. This is an advantage, however, for \emph{directed} graphs, where exactly the correct number of edges is represented in a matrix visualization. A node-link diagram, however, may underrepresent the edge count if the edges $x_{ij}$ and $x_{ji}$ both exist and are drawn on top of one another. 
Ultimately, there is no one ``correct" way to visualize network information, and we will be using both the node-link and adjacency matrix net-vis methods throughout this paper to visualize our example data and CTMC models.

\subsection{Defining CTMC Network Models} \label{sec:saoms}

The continuous-time Markov chain (CTMC) model family is for analyzing dynamic networks, so we can use them to study the change of a network in time while also examining the structural and node covariate efects. To define CTMCs for our purposes, we follow the general hierarchical definition outlined in \citet{saomreview} and elsewhere (see e.g.\ \citet{snijders01} and \citet{snijdersetal2010}). CTMC models reflect our intution about social networks: most social networks, even holding constant the set of actors over time, are ever-changing as relationships form or dissolve. Actors in social networks also have properties that could change their role within the network, and existing ties may affect formation or dissolution of other ties. In this section, we define the data structure, the hierarchal model, and intensity matrix of the CTMC. 

\subsubsection{Definitions, Terminology, and Notation}

A dynamic network consists of a fixed set of $n$ nodes that is changing over time, and is observed at $M$ discrete time points, $t_1, \dots, t_M$ with $t_1 < t_2 < \dots < t_M$. We denote the network observation at timepoint $t_m$ by $x(t_m)$ for $m = 1, \dots M$. The model assumes that this longitudinal network of discrete observations is embedded within a CTMC, which we denote by $X(T)$. The CTMC is almost entirely unobserved: we assume the first and last network observations are the starting and ending points of the CTMC, i.e. $x(t_1) \equiv X(0)$ and $x(t_M) \equiv X(\infty)$. Nearly all other steps in the chain are unobserved, with the exception of $x(t_2), \dots, x(t_{M-1})$. Unlike the first and last observations of the network, $x(t_2), \dots, x(t_{M-1})$ do not have direct correspondence with steps in the CTMC. Thus, the observations $x(t_2), \dots, x(t_{M-1})$ treated as ``snapshots" of the network at some point between two steps in the CTMC. 

The CTMC process $X(T)$ is a series of single tie changes that happen according to the model's \textit{rate function}, the first piece of the hierarchy. One actor at a time is ``selected" to make a change, then in the second piece of the hierarchy, it randomly changes its ties according to probabilities derived from the \textit{objective function}.

\noindent \textbf{Rate Function:} In general, the \emph{rate function}, $\rho(x, \mathbf{Z}, \boldsymbol{\alpha})$, is a function of the network at its current state $x$, covariates of interest, $\mathbf{Z}$, and some parameters, $\boldsymbol{\alpha}$. With this general formulation, each node can have a different rate of change. Throughout this paper, we assume a simple rate function, $\rho(x, \mathbf{Z}, \boldsymbol{\alpha}) \equiv \alpha_m$ that is constant across all actors between time $t_m$ and $t_{m+1}$. With this simple rate function, we have $M-1$ rate parameters, one for each transition from $x(t_{m})$ to $x(t_{m+1})$ for $m = 1, \dots, M-1$. The rate parameter, $\alpha_m$ dictates how often an actor $i$ is selected to change one of its ties, $x_{ij}$, for $j \neq i \in \{1, \dots, n\}$ in the time period from $t_{m}$ to $t_{m+1}$ for $m = 1, \dots, M-1$. The CTMC model assumes that the actors $i$ are conditionally independent given their ties $x_{i1}, \dots, x_{in}$ at the current network state. Let $\tau(i|x,m)$ be the wait time until actor $i$ gets to the opportunity to change its ties. For any time point $T$, where $t_{m} \leq T < t_{m+1}$, the waiting time to actor $i$'s next change follows an exponential distribution,
\begin{equation} \label{eq:c1}
 \tau(i | x_{i1}(m), \dots, x_{in}(m)) \stackrel{\text{iid}}{\sim} Exp(\alpha_m)
\end{equation}
\noindent with expected value $\alpha_m^{-1}$. From Equation~\ref{eq:c1}, we can derive the waiting time to the next change opportunity by \textit{any} actor in the network, $\tau(x)$. The value $\tau(x)$ is also exponentially distributed, and has expected value $(n\alpha_m)^{-1}$. The estimation of $\alpha_m$ in \texttt{RSiena} is simple: method of moments (MoM) is used with the statistic 
\begin{equation}\label{eq:cdiff}
C_m = \sum_i\sum_j |x_{ij}(t_{m+1}) - x_{ij}(t_m)|,
\end{equation}
which is the total number of edge differences between $x(t_m)$ and $x(t_{m+1})$. 

\noindent \textbf{Objective Function:} Because of the conditional independence assumptions given in Equation~\ref{eq:c1}, the objective function can be written separately for each node. Suppose node $i$ is selected to change at the next step in the CTMC. This node, called the \textit{ego} node, has the potential to interact with all other nodes in the network, $j \neq i$. These nodes $j$, are referred to as \textit{alter} nodes, and will not change any of their ties at the next step in the CTMC. The ego node $i$ in the current network state $x$, prioritizes making changes that maximize its \emph{objective function}:
\begin{equation} \label{eq:of1}
f_i(\boldsymbol{\beta}, x, \mathbf{Z}) = \sum_{k=1}^K \beta_k s_{ik}(x, \mathbf{Z}),
\end{equation}
where $x$ is the current network state, $\mathbf{Z}$ is a matrix of node covariates, and $K$ is the total number of parameters in the objective function. The parameters of the model, $\beta_1, \dots, \beta_K$ correspond to network statistics, $s_{ik}(x, \mathbf{Z})$, for $k = 1,\dots, K$. To maximize $f_i$, $i$ will either destroy a tie, create a tie, or make no change.% if all changes will result in lower $f_i$. 

The parameters of $f_i(\boldsymbol{\beta}, x, \mathbf{Z})$ are either structure or covariate parameters. The structure parameters correspond to statistics that are \emph{only} functions of the current network state, $x$, while the covariate parameters are also functions of the covariates, $\mathbf{Z}$. According to \citet[p. 371]{snijders01}, there should be at least two parameters in the objective function: $\beta_1$ for the outdegree of nodes, and $\beta_2$ for the reciprocity of nodes. The former measures the propensity of nodes with a lot of outgoing ties to form more outgoing ties (the ``rich get richer" effect), while the latter measures the tendency of outgoing ties to be returned within a network. The statistics corresponding to these two parameters, $s_{i1}(x)$ and $s_{i2}(x)$ are given in Figure~\ref{fig:structures} with visualizations of the corresponding effects. In the \texttt{RSiena} software, there are over 80 possible $\boldsymbol{\beta}$ parameters with corresponding statistics $s_{ik}(x, \mathbf{Z})$ to add to the model. The definition of the statistics for all possible parameters are provided in \citet{RSienamanual}, and two additional parameters and their corresponding statistics are discussed in Section~\ref{sec:exdatamods} and shown in Figure~\ref{fig:structures}.

% \begin{table}
% \centering
% \begin{tabular}{m{4.5cm}m{4.5cm}m{3cm}}
% \multicolumn{3}{l}{\textbf{Structural Effects}} \\
% Name & Statistic & Figure \\
% \hline\hline
% outdegree  & $s_{i1}(x) = \sum_j x_{ij}$ & \includegraphics[width=2cm]{img/outdegree}\\ \hline
% reciprocity  & $s_{i2}(x) = \sum_j x_{ij}x_{ji}$ & \includegraphics[width=2cm]{img/reciprocity}\\ 
% %\hline
% %transitive triplets  & $s_{i3}(x) = \sum_{j,h} x_{ij}x_{jh}x_{ih}$ & \includegraphics[width=1.5cm]{img/transtrip}  \\
% %\\
% % \multicolumn{3}{l}{\textbf{Covariate Effects}}\\  
% % Name & Statistic & Figure \\
% % \hline \hline
% % covariate-alter  & $s_{i4}(x) = \sum_j x_{ij}z_j$ & \includegraphics[width=3cm]{img/covaralter} \\  \hline
% % covariate-ego  & $s_{i5}(x) = z_i\sum_j x_{ij}$ & \includegraphics[width=3cm]{img/covarego} \\ % \hline
% %same covariate & $s_{i6}(x) = \sum_j x_{ij} \mathbb{I}(z_i = z_j)$ & \includegraphics[width=3cm]{img/covarsim}%\\
% %jumping transitive triplets  & $s_{i7}(x) = \sum_{j \neq h} x_{ij}x_{ih}x_{hj} \mathbb{I}(z_i = z_h \neq z_j)$
% \end{tabular}
% \caption{\label{tab:effects} Examples of structural effects included in the objective function. The darker nodes in the figures are the ego nodes ($i$), and all others are alters ($j$). The dotted lines are ties the ego node is en(dis)couraged to make when the parameter corresponding to each effect is positive (negative).}
% \end{table}

In the CTMC, when actor $i$ can make a change, it chooses which tie $x_{i1}, \dots, x_{in}$ to change at random according to the probability $p_{ij}(\boldsymbol{\beta}, x, \mathbf{Z})$. Let $x(i \leadsto j)$ be the network identical to network $x$ with the exception of tie $x_{ij}$, which is equal to $1-x_{ij}$; i.e.\ the presence or absence of node $x_{ij}$ is flipped in the new network. The probability that the tie $x_{ij}$ changes is defined as:
\begin{equation}\label{eq:pij}
p_{ij}(\boldsymbol{\beta}, x, \mathbf{Z}) = \dfrac{\exp\left\{f_i(\boldsymbol{\beta}, x(i\leadsto j), \mathbf{Z})\right\}}{\sum_{h \neq i} \exp\left\{f_i(\boldsymbol{\beta}, x(i \leadsto h), \mathbf{Z})\right\}}
\end{equation}%When $i = j$, the numerator represents the exponential of the value of the objective function when evaluated at the current network state. 
%When the value of the objective function is high at the current state, the probability of not making a change is also high.
We explore these probabilities further in Section~\ref{sec:algorithms}. 

\par \noindent \textbf{Intensity Matrix:} The probabilities $p_{ij}$ combined with the rate parameters $\alpha_m$ fully characterize the underlying CTMC that models network change. In the CTMC literature, see for instance \citet{ctmc}, chains are characterized by their  \textit{intensity matrix} $\mathbf{Q}$. This matrix describes the rate of change between two states of the CTMC process, and the rows of this matrix always add to zero. The state space for a network, denoted $\mathcal{X}$, contains $2^{n(n-1)}$ states: there are two possible states for an edge, $\{0,1\}$, and $n(n-1)$ possible edge relationships, as the network is directed and we exclude self-ties.  The intensity matrix is then a square matrix of dimension $2^{n(n-1)}$. Only one tie changes at a time in the CTMC, resulting in $n(n-1)$ reachable states from the current network state. Thus, the intensity matrix $\mathbf{Q}$ is very sparse, with only $n(n-1) + 1$ non-zero entries in each row. Note that $n(n-1)$ of these entries represent the possible states that are one edge different from a given state, while the additional non-zero entry is the diagonal entry for the current state.% All other entries in a row are structural zeroes because those network states cannot be reached from the current state in a single change.

The two pieces of the model hierarchy each contribute to the intensity matrix to describe the rate of change between two network states. The entries of $\mathbf{Q}$ are defined as follows: let $b \neq c \in \{1, 2, \dots, 2^{n(n-1)} \}$ be indices of two different possible states of the network, $x^b, x^c \in \mathcal{X}$.  Then the $bc^{th}$ entry of $\mathbf{Q}$ is:
\[ q_{bc} = \begin{cases}
     q_{ij} = \alpha_m p_{ij}(\boldsymbol{\beta}, x^b) & \text{ if } x_{ij}^c = 1-x_{ij}^b \text{ for some } i \neq j \in \{1, 2, \dots, n \} \\
      -\sum_{i\neq j} q_{ij} & \text{ if } x^b = x^c \\
      0 & \text{ otherwise} 
   \end{cases}
\]
Thus, the rate of change between any two states, $x^b$ and $x^c$, that differ by only one tie $x_{ij}$, is the product of the rate at which actor $i$ gets to change a tie and the probability that the tie that will change is the tie to node $j$. The intensity matrix $\mathbf{Q}$ fully defines the CTMC model for network change. %, and is the foundation required for parameter estimation. %Estimating the parameters in these models is difficult, but thanks to the SIENA software \citep{RSiena}, we have an accessible way to fit SAOMs to network observations.

\subsubsection{Fitting Models to Data}

To fit a CTMC to dynamic network data, we apply the \texttt{RSiena} software, which uses simulation methods to estimate parameter values using either MoM or maximum likelihood (ML) estimation \citep{RSiena}. We chose to use MoM estimation, so we describe that process here, while more information on ML estimation can be found in \citet{SienaAlgs}. %because the theory behind it was established in \citet{saompaper}, while the maximum likelihood estimation methods were not fully established until \citet{saomsml2010}, though \texttt{RSiena} contains capabilities to use maximum likelihood estimation. We also use the score function method for estimating the derivatives of the expected values, as opposed to the finite differences method, both of which are outlined in detail in .
The underlying SIENA software uses a Robbins-Monro algorithm (see \citet{robbinsmonro}) to estimate the solution of the moment equation
\begin{equation}\label{eq:mom}
E_{\theta}S = s_{obs}
\end{equation}
where $\theta = (\boldsymbol{\alpha}, \boldsymbol{\beta})$ is the vector of rate and objective function parameters, and $s_{obs}$ is the observed vector of the corresponding statistics summed over all network observations. Estimation conditions on the first observed network, $x(t_1)$, then proceeds to estimate model parameters for all future observations. % awkward. fix it. 
The full SIENA MoM estimation algorithm operates in three phases, as described in \citet{RSienamanual} and \citet{SienaAlgs}, which we briefly summarize here. The first phase obtains initial estimates of the score functions for use in the second phase. The second phase is the Robbins-Monro algorithm, and results in parameter estimates through iterative updates and simulation from the CTMC at the iterations of the parameter values. The third phase uses the parameters from phase two to estimate the score functions and covariance matrix of the parameter estimates and carries out convergence checks. In Section~\ref{sec:algorithms}, we further explore these phases in the SIENA MoM algorithm through visualization. For a more detailed description of the fitting process, we direct the reader to \citet{RSienamanual, SienaAlgs}%, bringing them out of their ``black box" and into the light.

The rest of this paper is strucured as follows. In Section \ref{sec:exdatamods}, we introduce the motivating data and CTMC models we assess using model-vis. Then, in Section~\ref{sec:minds}, we explore the first step in model-vis by placing the models in the data space to determine how well the models fit the data. Section~\ref{sec:collectmodels} follows, in which we visualize collections of models to explore their behavior. Finally, in Section~\ref{sec:algorithms}, we visualize the underlying algorithms in the CTMC fitting process to gain insight into the fitting process. We conclude with a discussion in Section~\ref{sec:discussion}.


\section{Example Data and Models}\label{sec:exdatamods}


% This is a canonical example in the social network modeling literature. That's why we chose it. 
% It's easy to understand: everyone was once a teenager and knows how fragile those relationships are and how 
% much and how quickly they can change. 
% data has previously been used to quantify behavior & identify which ones will take risks. @pearson2003drifting


<<alldataviz, fig.cap="The small friendship network data we model throughout this paper. The FR layout is used. The teenagers' drinking behavior changes and the network becomes less densely connected over time. These are the mechanisms we hope to model with our chosen CTMCs.", out.width='.99\\textwidth', fig.height=3, results='hide'>>=
cols <- RColorBrewer::brewer.pal(8, "Greys")

source("code/00e_small_friends.R")
view_rs2$wave <- factor(view_rs2$wave)
levels(view_rs2$wave) <- c("Wave 1", "Wave 2", "Wave 3")
actual1 <- merge(data.frame(as.edgelist(as.network(fd2.w1))),
                 data.frame(id = 1:16, drink = drink2[,1]),
                 by.x = "X1", by.y = "id", all = T)
actual1$Time <- 1
actual2 <- merge(data.frame(as.edgelist(as.network(fd2.w2))),
                 data.frame(id = 1:16,drink = drink2[,2]),
                 by.x = "X1", by.y = "id", all = T)
actual2$Time <- 2
actual3 <- merge(data.frame(as.edgelist(as.network(fd2.w3))),
                 data.frame(id = 1:16,drink = drink2[,3]),
                 by.x = "X1", by.y = "id", all = T)
actual3$Time <- 3
alldat <- rbind(actual1, actual2, actual3)
alldat$X1 <- as.factor(alldat$X1)
alldat$X2 <- as.factor(alldat$X2)
set.seed(35291)
alldataviz <- ggplot(data = alldat,
       aes(from_id = X1, to_id = X2, color = as.ordered(drink), group = as.ordered(drink))) +
  geom_net(fiteach = FALSE, directed=TRUE, layout.alg = 'fruchtermanreingold', size = 4, arrowsize = .25, arrowgap = .05, ecolour = 'black', labelon=T, labelcolour = 'white', fontsize = 2, vjust = .5, linewidth=0.25) +
  scale_color_manual(values = cols[5:8], name = "Drinking\nBehavior", labels = c("never", "1-2 times per year", "once per month", "once per week")) +
  ThemeNet +
  facet_wrap(~Time, labeller = "label_both") +
  theme(legend.position = 'bottom',
        panel.background = element_rect(fill = NA, color = "grey30"),
        legend.text = element_text(size = 10, family = "Times New Roman"),
        legend.title = element_text(size = 10, family = "Times New Roman"))
alldataviz
@

\noindent \textbf{Example Data:} To guide our visual exploration of CTMC models for dynamic networks, we use a canonical example in the social network literature: a subset of the 50 actor dataset from the ``Teenage Friends and Lifestyle Study"  \citet{friendsdata}. We chose this data because it is a well-known and easy to understand example. We were all teenagers once, and we know the volatility and fragility of those relationships, as well as why they might change. We chose to only work with a subset of the data to make node-link diagrams easier to see and to make any changes in the network more noticeable. 
We visualize this data with a node-link diagram in Figure~\ref{fig:alldataviz}, where %For model fitting, we condition on wave 1 and estimate the parameters of the models for the second and third waves. 
nodes are colored according to the girl's reported drinking behavior: does not drink, drinks once or twice a year, drinks once a month, and drinks once a week. We can see that, as time passes, the students seem to drink more and become increasingly isolated into smaller groups. An analysis of this type of data with a CTMC model should capture these dynamics in a way that allows the researcher to draw conclusions about the nature of the network and the behavioral forces at play.

%The second data example we use is a much more recent dataset on collaboration in the United States Senate during the $111^{th}$ through $114^{th}$ Congresses, shown in Figure~\ref{fig:senateplot}. These sessions of congress correspond to the years of Barack Obama's presidency, from 2009-2016. (Details of how this data can be downloaded are provided by FranÃ§ois Briatte at \url{https://github.com/briatte/congress}.) In this network, ties are directed from senator $i$ to senator $j$ when senator $i$ cosponsors the bill that senator $j$ authored. There are hundreds of ties between senators when they are connected in this way, so we simplify the network by computing a single value for each pair of senators called the \textit{weighted propensity to cosponsor} (WPC). This value is defined in \citet{senate} as the total number of times senator $i$ cosponsors senator $j$'s bill, weighted by the number of total cosponsors:  
% \begin{equation}\label{eq:sen1}
% WPC_{ij} = \dfrac{\sum\limits_{k=1}^{n_j} \frac{Y_{ij(k)}}{c_{j(k)}}}{\sum\limits_{k=1}^{n_j} \frac{1}{c_{j(k)}}}
% \end{equation}
%where $n_j$ is the number of bills in a congressional session authored by senator $j$, $c_{j(k)}$ is the number of cosponsors on senator $j$'s $k^{th}$ bill, where $k \in \{1,\dots, n_j\}$, and $Y_{ij(k)}$ is a binary variable that is 1 if senator $i$ cosponsored senator $j$'s $k^{th}$ bill, and is 0 otherwise. This measure ranges in value from 0 to 1, where $WPC_{ij} = 1$ if senator $i$ is a cosponsor on every one of senator $j$'s bills and $WPC_{ij} = 0$ if senator $i$ is never a cosponsor any of senator $j$'s bills.
<<senateplot, fig.cap='The collaboration network in the four senates during the Obama years, 2009-2016. Edges are shown only if the WPC between two senators is greater than 0.25. We use the FR layout algorithm here.', fig.width=8, fig.height=8, out.width='\\textwidth', eval = FALSE>>=
seobama <- read_csv("data/congress/senateobamapres_gsw_25.csv")
to_label <- c("Joseph R. Biden Jr.", "John S. McCain", "Ted Cruz","Marco Rubio",
  "Lindsey O. Graham","Rand Paul", "Bernard Sanders", "Jim Webb", "Mitch McConnell",
  "Harry M. Reid", "Hillary Rodham Clinton", "Amy Jean Klobuchar")
seobama$label <- as.factor(ifelse(seobama$source %in% to_label, seobama$source, ""))
levels(seobama$label) <- c("", "Amy Klobuchar", "Bernie Sanders", "Harry Reid",
                           "Hillary Clinton", "Jim Webb", "John McCain", "Joe Biden",
                           "Lindsey Graham", "Marco Rubio", "Mitch McConnell",
                           "Rand Paul", "Ted Cruz")
set.seed(56049382)
ggplot(data = seobama) +
  geom_net(directed = T, labelon=T, arrowsize = .25, singletons= T, fiteach = T, linewidth = .25, layout.alg = 'fruchtermanreingold', fontsize = 3, repel=T,
           aes(from_id = source, to_id = target, color = party, label = label),
           size = 1) +
  ThemeNet +
  scale_color_manual(values = c("black", "grey30","#ef3b2c")) +
  theme(legend.position = 'bottom', strip.text = element_text(size = 20)) +
  facet_wrap(~senate, nrow = 2, labeller = "label_both") +
  xlim(c(0,1.05)) +
  ylim(c(0,1.05))
@
% Because CTMC models require binary edges, we focus only on strong collaborations, where $WPC$ is high. For a senate collaboration network $x$, edges are defined as
% \begin{equation}\label{eq:wpc2}
%   x_{ij} =
%   \begin{cases}
%                                    1 &  \text{if } WPC_{ij} > 0.25 \\
%                                    0 & \text{if } WPC_{ij} \leq 0.25.
%   \end{cases}
% \end{equation}
% The node-link diagram for the four senates during the Obama administration are shown in Figure~\ref{fig:senateplot}. We can see that Senate 111 is much more densely connected than the other three, which have one large, sparsely connected component with a few smaller connected groups throughout. Some prominent names are shown to demonstrate their place in the network.  As with the friendship example, analyzing the senate data with a CTMC model should capture the underlying dynamics, giving insight into the impact that the actor covairates have on the senate's collaboration structure. By using data sets from very different contexts, we hope to assess some differences in model performance. 

\noindent \textbf{Models:} To our example data, we fit three different models. Each model uses a simple rate function, $\alpha_m$, and an objective function with two or three parameters. Though our models are very simple, the model-vis techniques we present can easily be applied to much more complex models. The first model of interest, M1, contains the absolute minimum number of parameters in the objective function $f_i(x)$: 
\begin{equation}\label{eq:m1}
M1: \quad f_i(x) = \beta_1s_{i1}(x) + \beta_2s_{i2}(x),
\end{equation}
where $s_{i1}(x)$ is the density network statistic and $s_{i2}(x)$ is the reciprocity network statistic for actor $i$ at the current network state $x$ (see Figure~\ref{fig:structures}). The second and third models, M2 and M3, contain one additional parameter each in the objective function. We chose the additional parameters because they were significant ($p$-value $<$ 0.05) according to a Wald-type test (\texttt{Wald.RSiena()}) provided in the \texttt{RSiena} software \citep{RSienaTest}. The model M2 contains an actor-level covariate parameter, and the model M3 contains an additional strutural effect in the objective function.
\begin{align}\label{eq:m2m3}
M2: \quad f_i(x,z) & = \beta_1s_{i1}(x) + \beta_2s_{i2}(x) + \beta_3s_{i3}(x, z) \\
M3: \quad f_i(x) & = \beta_1s_{i1}(x) + \beta_2s_{i2}(x) + \beta_4s_{i4}(x),
\end{align}

\begin{figure}
\begin{subfigure}[t]{.45\textwidth}
<<si1, fig.width=2, fig.height=2, message = FALSE, warning = FALSE, fig.align='center',out.width='.5\\textwidth'>>=
s1e <- data.frame(from = rep('i',4), to = letters[10:13])
s1n <- data.frame(id = letters[9:13], group = c(1,1,1,1,2))
s1net <- merge(s1e,s1n, by.x = "from", by.y = "id", all = T)
set.seed(1456)
ggplot(data = s1net, aes(from_id = from, to_id = to)) +
  geom_net(directed = T, labelon = T, arrowsize = .5,
           labelcolour='white',vjust = 0.5, hjust =0.5, arrowgap = .15,
           size=6, fontsize = 5, ecolour="black",
           linetype = c("solid", "solid","dotted", "solid","solid", "solid","solid", "solid")) +
  expand_limits(x=c(-0.2,1.2), y=c(-0.1,1.1)) +
  ThemeNet +
  theme(legend.position = "none") + 
  labs(title = expression(paste(s[i1], "(x)")))
@
\caption{\label{fig:beta1}Demonstrating the outdegree effect $\beta_1$. The dotted tie is encouraged if $\beta_1$ is positive. $s_{i1}(x) = \sum_{j} x_{ij}$}
\end{subfigure}\hfill
\begin{subfigure}[t]{.45\textwidth}
<<s2, fig.width=2, fig.height=2, message = FALSE, warning = FALSE, fig.align='center',out.width='.5\\textwidth'>>=
s2e <- data.frame(from = letters[9:10], to = letters[10:9])
s2n <- data.frame(id = letters[9:10], group = c(1,2))
s2net <- merge(s2e,s2n, by.x = "from", by.y = "id", all = T)
set.seed(13902)
ggplot(data = s2net, aes(from_id = from, to_id = to)) +
  geom_net(directed = T, labelon = T, curvature=.25, arrow = arrow(type = "closed",length = unit(4,"mm")),
           labelcolour='white',vjust = 0.5, hjust =0.5, arrowgap = .15,
           size=7, fontsize = 4, ecolour="black",
           linetype = c("solid","dotted")) +
  expand_limits(x=c(-0.2,1.2), y=c(-0.1,1.1)) +
  ThemeNet +
  theme(legend.position = "none") + 
  labs(title = expression(paste(s[i2], "(x)")))
@
\caption{\label{fig:beta2}Demonstrating the reciprocity effect $\beta_2$. The dotted tie is encouraged if $\beta_2$ is positive. $s_{i2}(x) = \sum_{j} x_{ij}x_{ji}$}
\end{subfigure}
\begin{subfigure}[t]{.45\textwidth}
<<jtt, echo=FALSE, fig.width=2, fig.height=2, message = FALSE, warning = FALSE, fig.align='center',out.width='.49\\textwidth'>>=
jTTe <- data.frame(from = c('i', 'i', 'h'), to = c('h', 'j', 'j'))
jTTn <- data.frame(id = letters[8:10], group = c(1,1,2))

jTT <- merge(jTTe, jTTn, by.x = 'from', by.y = "id", all = T)

set.seed(12345)
ggplot(data = jTT, aes(from_id = from, to_id = to)) +
  geom_net(aes(shape = as.factor(group)), directed = T, labelon = T,
           labelcolour='white',vjust = 0.5, hjust =0.5, arrowgap = .15,
           size=10, fontsize = 5, ecolour="black",
           linetype = c("solid", "solid","dotted", "solid")) +
  expand_limits(x=c(-0.2,1.2), y=c(-0.1,1.1)) +
  ThemeNet +
  theme(legend.position = "none") + 
  labs(title = expression(paste(s[i3], "(x,z)")))
@
\caption{\label{fig:jtt}Demonstrating a JTT, where the covariate is represented by the shape of the nodes. The dotted tie is encouraged if $\beta_3$ is positive. $s_{i3}(x,z) = \sum_{j \neq h} x_{ij}x_{ih}x_{hj} \mathbb{I}(z_i = z_h \neq z_j)$}
\end{subfigure}\hfill
\begin{subfigure}[t]{.45\textwidth}
<<dab, echo=FALSE, fig.width=2, fig.height=2, message = FALSE, warning = FALSE, fig.align='center', out.width='.49\\textwidth'>>=
dade <- data.frame(from = c('i', 'i', 'h', 'k', 'i'), to = c('h', 'k', 'j', 'j','j'))
dadn <- data.frame(id = letters[8:11], group = c(1,1,1,1))

dad <- merge(dade, dadn, by.x = 'from', by.y = "id", all = T)

set.seed(12345)
ggplot(data = dad, aes(from_id = from, to_id = to)) +
  geom_net(directed = T, labelon = T, ecolour="black",
           linetype = c("solid", "solid", "dotted", "solid","solid" ,"solid"),
           labelcolour='white',vjust = 0.5, hjust =0.5, arrowgap = .15, size=10, fontsize = 5) +
  expand_limits(x=c(-0.1,1.1), y=c(-0.1,1.1)) +
  ThemeNet +
  theme(legend.position = "none") + 
  labs(title = expression(paste(s[i4], "(x)")))
@
\caption{\label{fig:dad}Demonstracting a N22 between actors $i$ and $j$. The dotted tie is encouraged if $\beta_4$ has a positive value. $s_{i4}(x) =  |\{j : x_{ij} = 0, \sum\limits_h x_{ih}x_{hj} \geq 2\}|$}
\end{subfigure}
\caption{\label{fig:structures} The network statistics whose corresponding effects are included in the models we fit to the friends data.}
\end{figure}

\noindent where $s_{i3}(x,z)$ and $s_{i4}(x)$ are defined in Figure~\ref{fig:structures}. These statistics are known as the \emph{number of jumping transitive triplets} (JTT) and the \emph{number of doubly achieved distances two effect} (N22), respectively. The JTT effect emphasizes triads formed between actors with \emph{different} covariate values, while the N22 effect emphasizes \emph{indirect} ties between actors. %The covariate for the friendship data is the drinking behavior, and is the number of bills authored for the senate data. 
All model effects are visualized in Figures~\ref{fig:beta1}-\ref{fig:dad}, where the ties affected by the parameter value are represented by dotted lines. If the estimates of these parameters $\hat{\beta}_1, \dots, \hat{\beta}_4$ are positive, the model encourages the tie represented by the dotted line to form, while the opposite is true if the parameter estimate is negative. Mean estimates from 1,000 repeated fittings are given in Table~\ref{tab:meanests}. Any time we simulate from a network model, these are the parameters we use. 

% table of fitted M1 M2 M3
<<parammeans>>=
simu2 <- read.csv("data/simulation-1000-M1-M2-M3.csv")
SFE <- simu2 %>% group_by(Model, parameter) %>%
  summarize(meanEst = mean(estimate), sdEst = sd(estimate)) %>%
   mutate(val = paste0(sprintf("%.3f",meanEst), " ", "(", sprintf("%.3f", sdEst), ")")) %>%
  select(Model, parameter, val) %>%
  dplyr::select(Model, parameter, val) %>%
  spread(Model, val) %>% arrange(parameter)
@
\begin{table}
\centering
\begin{tabular}{lccc}
  \toprule
  & M1 & M2 & M3 \\
  \midrule
  $\alpha_1$ & \Sexpr{SFE$M1[1]} & \Sexpr{SFE$M2[1]} & \Sexpr{SFE$M3[1]}  \\
  $\alpha_2$ & \Sexpr{SFE$M1[2]} & \Sexpr{SFE$M2[2]} & \Sexpr{SFE$M3[2]}   \\
  \midrule
  $\beta_1$ & \Sexpr{SFE$M1[3]} & \Sexpr{SFE$M2[3]} & \Sexpr{SFE$M3[3]}  \\
  $\beta_2$ & \Sexpr{SFE$M1[4]} & \Sexpr{SFE$M2[4]} & \Sexpr{SFE$M3[4]} \\
$\beta_3$ & -- & \Sexpr{SFE$M2[5]} & --  \\
  $\beta_4$ & -- & --  & \Sexpr{SFE$M3[6]} \\
  \bottomrule
\end{tabular}

\caption{\label{tab:meanests} The means (std.\ dev.)\ of parameter values estimated from 1,000 repeated fittings of M1, M2, and M3 to the example data.}
\end{table}

We encourage interested readers to consult the \texttt{RSiena} manual for a complete definition and description of these and all other possible effects available to use in CTMC models \citep{RSienamanual}. Our goal here is to assess the fit of these models to our data, not to find the best model for this data, which is why we have kept both the data and the models very small. A true analysis of these data would include all of the available data, including additional covariates related to smoking and other behaviors, and would use sociological research to select and test the most pertinent effects at each step in the model hierarchy. The exact model structure and effects we choose are not as important to us as our ability to assess their impact on model fit. We keep the data and model simple in order to apply the more complicated model visualization tools that we discuss in Sections~\ref{sec:minds}-\ref{sec:algorithms}. 

% need a way better transition here. 

\section{Model in the data space}\label{sec:minds}

%Every good \emph{data} analysis includes both numerical and visual summaries of the data, so why restrict \emph{model} analysis, description, and diagnostics to numerical summaries? 


Placing the model in the data space is crucial to understanding how well the model represents the observed data. %First, we put models in the data space to assess the model fit. %In \citeauthor{modelvis}, they chose define the \textit{data space} as ``the region over which we can reliably make inferences, usually a hypercube containing the data" \citep[p.~206]{modelvis}. But what does this definition mean for dynamic network models?
We define the \emph{data space} for our models as the combined data structures of the network: the node, edge, and time information. We use the \texttt{R} package \texttt{geomnet} to visualize the three data spaces together  \citep{geomnet}. The package draws the network using node-link diagrams, and the network's node or edge data are then mapped to different visual features.  The color, size, and shape of the points can be used to represent variables in the node data, while the color, linewidth, and linetype of the edges between nodes can be used to represent variables in the edge data. Finally, the networks observed at each time point are placed side-by-side, showing the network's evolution in time. Bringing all network information together in \texttt{geomnet} allows us to show the entire data space at once. %To view temporal changes, we place the network at different timepoints side-by-side to see the evolution. By pulling all of this information together using \texttt{geomnet}, we can show the entire data space at once.

<<funsenatevis, fig.height=3, fig.cap='The 111$^{th}$ Senate at two times: while Clinton was in the senate in 2009 (left) and after she departed (right). We map sex, party, and bills authored to the shape, color, and size of the nodes, respectively. We also map $WPC$ to the edge width.', eval = FALSE>>=
se111clint <- filter(seobama, senate == 111)
se111noclint <- filter(seobama, senate == 111)
se111noclint$target[which(se111noclint$target == "Hillary Rodham Clinton")] <- NA
se111clint$Clinton <- 'Yes'
se111noclint$Clinton <- 'No'
clintonSenate <- rbind(se111clint, se111noclint)
clintonSenate$Clinton <- as.factor(clintonSenate$Clinton)
clintonSenate$Clinton <- ordered(clintonSenate$Clinton, levels = c("Yes", "No"))

set.seed(12981120)
clintonSenate %>% filter(!(source %in% c("Roland Burris", "Bernard Sanders", "Frank R. Lautenberg", "Mary L. Landrieu") & is.na(target))) %>%
ggplot() +
  geom_net(directed = T, labelon=F, arrowsize = .3, singletons= F, fiteach = T, arrowgap = .01, layout.alg = 'fruchtermanreingold', ecolour = "grey70",
           aes(from_id = source, to_id = target, color = party, linewidth = gsw, shape = sex,size = n_au/30)) +
  scale_size_continuous(name="# bills\nauthored" , guide = guide_legend(nrow= 2), breaks = c(0,1,2,3), labels = c(0,30, 60, 90), range = c(.5,3)) +
  ThemeNet +
  scale_shape_manual(values = c(17,16), guide = guide_legend(nrow= 2)) +
  scale_color_manual(values = c("black", "grey80","#ef3b2c"), guide = guide_legend(nrow=2)) +
  facet_wrap(~Clinton, nrow = 1, labeller = 'label_both')  +
  theme(legend.position = "bottom")
@

<<summnetM1, message=FALSE,results='hide'>>=
friendshipData <- array(c(fd2.w1, fd2.w2, fd2.w3), dim = c(16, 16, 3))
friendshipData <- array(c(fd2.w1, fd2.w2, fd2.w3), dim = c(16, 16, 3))
drink1 <- drink[20:35,1]
drink2 <- drink[20:35,2]
drink3 <- drink[20:35,3]
friendship <- sienaDependent(friendshipData)
alcohol <- varCovar(cbind(drink1,drink2, drink3))
mydata <- sienaDataCreate(friendship, alcohol)
M1eff <- getEffects(mydata)
M2eff <- includeEffects(M1eff, jumpXTransTrip, interaction1 = "alcohol")
M3eff <- includeEffects(M1eff, nbrDist2twice)
simu2 <- read.csv("data/simulation-1000-M1-M2-M3.csv")
means <- simu2 %>% group_by(Model, parameter) %>% summarize(
  means = mean(estimate)
)
M1parms <- (means %>% filter(Model == "M1"))$means
M2parms <- (means %>% filter(Model == "M2"))$means
M3parms <- (means %>% filter(Model == "M3"))$means
set.seed(4231352)
M1simsdf <- read_csv("data/M1simsdf.csv")
M2simsdf <- read_csv("data/M2simsdf.csv")
M3simsdf <- read_csv("data/M3simsdf.csv")

M1avgW2 <- M1simsdf %>% filter(!is.na(to) & wave == 1) %>%
  group_by(from, to) %>%
  summarise(count = n()) %>%
  mutate(weight = ifelse(from == to, 0, count))
M2avgW2 <- M2simsdf %>% filter(!is.na(to) & wave == 1) %>%
  group_by(from, to) %>%
  summarise(count = n()) %>%
  mutate(weight = ifelse(from == to, 0, count))
M3avgW2 <- M3simsdf %>% filter(!is.na(to) & wave == 1) %>%
  group_by(from, to) %>%
  summarise(count = n()) %>%
  mutate(weight = ifelse(from == to, 0, count))

# make a df of wave 2, wave 1, and the three averages and facet.
names(actual1)[1:2] <- c("from", "to")
names(actual2)[1:2] <- c("from", "to")
actual1$count <- 1
actual1$weight <- 1
actual2$count <- 1
actual2$weight <- 1
actual1 <- actual1 %>% dplyr::select(from,to, count, weight)
actual2 <- actual2 %>% dplyr::select(from,to, count, weight)
actual1$cat <- "1st Wave"
actual2$cat <- "2nd Wave"

avgW2M1 <- M1avgW2 %>% ungroup() %>%
  filter(weight > 50) %>%
  mutate(from = as.factor(from), to = as.factor(to),
         cat = "Model1")
add1 <- setdiff(as.character(1:16), unique(c(as.character(avgW2M1$from), as.character(avgW2M1$to))))
avgW2M1 %>% add_row(from = add1, to = NA, count = NA, weight = NA, cat = "Model1") -> avgW2M1
avgW2M2 <- M2avgW2 %>% ungroup() %>%
  filter(weight > 50) %>%
  mutate(from = as.factor(from), to = as.factor(to),
         cat = "Model2")
add2 <- setdiff(as.character(1:16), unique(c(as.character(avgW2M2$from), as.character(avgW2M2$to))))
avgW2M2 %>% add_row(from = add2, to = NA, count = NA, weight = NA, cat = "Model2") -> avgW2M2
avgW2M3 <- M3avgW2 %>% ungroup() %>%
  filter(weight > 50) %>%
  mutate(from = as.factor(from), to = as.factor(to),
         cat = "Model3")
add3 <- setdiff(as.character(1:16), unique(c(as.character(avgW2M3$from), as.character(avgW2M3$to))))
avgW2M3 %>% add_row(from = add3, to = NA, count = NA, weight = NA, cat = "Model3") -> avgW2M3

combinedavgs <- rbind(actual1, actual2, avgW2M1, avgW2M2, avgW2M3)
combinedavgs %>% group_by(cat) %>%
  mutate(linewidth = weight / max(weight,na.rm = T)) -> combinedavgs

combinedavgs %>% filter(cat == "Model1") %>% mutate(logweight = log(weight)) -> t1
colors <- tweenr::tween_color(data = c("#969696", "#d73027"), n = table(t1$cat) %>% as.numeric, ease = 'linear')
t1 %>% arrange(logweight) -> t1
t1$color <- NA
t1$color <- colors[[1]]

p1 <- ggplot(data = t1) +
  geom_net(aes(from_id = from, to_id = to, linewidth= linewidth),
           directed = T, curvature = .2, labelon=T, size = 3,
           arrow = arrow(angle = 20, type = "open", length = unit(.2, "cm")), ecolour = "black",
           color = "#252525", hjust = .5, vjust = .5, labelcolour = 'white', fontsize = 2, arrowgap = .02,
           singletons = T) +
  labs(title = "M1 Expected Network") + ThemeNet + 
  theme(plot.background = element_rect(color = 'black'))

p2 <- ggplot(data = filter(combinedavgs, cat == "1st Wave")) +
  geom_net(aes(from_id = from, to_id = to, linewidth= linewidth/3),
           directed = T, curvature = .2, labelon=T, size = 3,
           arrow = arrow(angle = 20, type = "open", length = unit(.2, "cm")), ecolour = "#969696",
           color = "#252525", hjust = .5, vjust = .5, labelcolour = 'white', fontsize = 2, arrowgap = .03,
           singletons = T) +
   labs(title = "Time 1") + ThemeNet +
    theme(plot.background = element_rect(color = 'black'))
p3 <- ggplot(data = filter(combinedavgs, cat == "2nd Wave")) +
  geom_net(aes(from_id = from, to_id = to, linewidth= linewidth/3),
           directed = T, curvature = .2, labelon=T, size = 3,
           arrow = arrow(angle = 20, type = "open", length = unit(.2, "cm")), ecolour = "#969696",
           color = "#252525", hjust = .5, vjust = .5, labelcolour = 'white', fontsize = 2, arrowgap = .03,
           singletons = T) +
  labs(title = "Time 2")  +  ThemeNet +
  theme(plot.background = element_rect(color = 'black'))
#grid.arrange(p2, p1, p3, nrow = 1)
@

<<averagenet, results='hide', out.width='\\textwidth', fig.cap='In the bottom left, the network at the first time point ($x(t_1)$), which is conditioned on in the model. In the bottom right, the network at the second time point($x(t_2)$). In the top row, the three expected networks for time point 2 ($\\bar{x}(t_2)$) according to model M1, M2, M3. Legend shows edge weights in the three expected networks.', fig.height=5>>=

combinedavgs$linewidth[combinedavgs$cat %in% c("1st Wave", "2nd Wave")] <- .5
combinedavgs$cat <- as.factor(combinedavgs$cat)
combinedavgs$cat <- ordered(combinedavgs$cat, levels = c("Model1", "Model2", "Model3", "1st Wave", "2nd Wave"))
levels(combinedavgs$cat) <- c("M1 expectation", "M2 expectation", "M3 expectation", "Time 1", "Time 2")

summary(combinedavgs$linewidth)

combinedavgs %>% filter(!is.na(to)) %>% 
  ggplot() +
  geom_point(aes(x = from, y = to)) + 
  geom_line(aes(x = from, y = to, size = linewidth), color = "#969696") +
  scale_size_continuous(name = "Edge frequency", range = c(0,1)) + 
  ThemeNet -> forlineleg  
legendlines <- cowplot::get_legend(forlineleg)
set.seed(35291)
allp <- ggplot(data = combinedavgs) +
  geom_net(aes(from_id = from, to_id = to, linewidth= linewidth),
           directed = T, curvature = .2, labelon=T, size = 3,
           arrow = arrow(angle = 20, type = "open", length = unit(.2, "cm")), ecolour = "#969696", fiteach = T,
           color = "#252525", hjust = .5, vjust = .5, labelcolour = 'white', fontsize = 2, arrowgap = .02,
           singletons = T) +
  ThemeNet + theme(panel.background = element_rect(color = 'black')) +
  facet_wrap(~cat)

ggdraw() + draw_plot(allp, width = 1) + 
  draw_grob(legendlines,x=.7, y=.1, width = .2, height = .4)
@

To view our models in the data space, we first simulate from the fitted model. No single network instance simulated from a CTMC will fully represent the model, so instead we look at an \textit{expectation} from the network model. We frequently rely on expected values in statistics, but network models, especially those as complex as our CTMC models, lack an expected network measure. We could talk about expected values of parameters, but the parameters can be hard to interpret and do not illuminate the overall structure of the network. How then, can we arrive at an expected network? We propose the following method:
\begin{enumerate}
\item Condition on $x(t_1)$ and simulate 1,000 instances of $x(t_2)$ and $x(t_3)$ from the model of interest using previously estimated parameter values (see Table~\ref{tab:meanests}). Call these simulations $x^{(1)}(t_m), x^{(2)}(t_m),\dots x^{(1000)}(t_m)$ for $m=2,3$.  
\item Count the number of times each edge $i \rightarrow j$ appeared in one of the 1,000 network simulations. i.e.\ compute $x_{ij}^{(B)}(t_m) = \sum_{b=1}^{1000} x_{ij}^{(b)}(t_m)$ for $m=2,3$.
\item Construct one weighted summary network for time points 2 and 3. Each summary network has edge weights equal to the proportion of the simulated networks in which the edge appeared.i.e.\ $\bar{x}_{ij}(t_m) = x_{ij}^{(B)}(t_m) /1000$. 
\item Create the \textit{expected network} by including only the edges that appeared in more than 5\% of the simulations. i.e.\ $\bar{x}_{ij}(t_m) := \bar{x}_{ij}(t_m) \cdot \mathbb{I}(\bar{x}_{ij}(t_m) > 0.05)$. We write the expected networks as $\bar{x}(t_2)$ and $\bar{x}(t_3)$.
\end{enumerate}

\noindent The above method can be generalized and applied to any network model, static or dynamic, from which we can simulate, for any number of simulations from the network. The expected networks from our three models of interest are shown in the top row of Figure~\ref{fig:averagenet}. Edges that appear with higher frequency are drawn with thicker lines. On the bottom row of Figure~\ref{fig:averagenet}, we show the data for comparison: $x(t_1)$ on the left, and $x(t_2)$ on the right. Visualizing the expected network in this way places models M1, M2, and M3, in the data space, and helps assess model fit.  For model M1 we can see that the overall structure of the average network is much more similar to $x(t_1)$  than to $x(t_2)$ . In both $x(t_1)$ and $\bar{x}(t_2)$, there are two clusters of nodes: $\{2,3,4,5,6,12,13,15\}$ and $\{7,8,9,10,11,14\}$. The expected network, however, does not resemble  $x(t_2)$, which it is supposed to represent. In $x(t_2)$, there is one small, strongly connected cluster and one larger, sparsely connected cluster, with four unconnected nodes. Because the expected network $\bar{x(t_1)}$ more closely resembles $x(t_1)$ than $x(t_2)$, we take this as evidence that the simple model, M1, is not a good fit to the data.
Comparing the three expected networks to $x(t_1)$ and $x(t_2)$, we see that they all have nodes that are connected in the same groups as $x(t_1)$. Model M2, which included the transitive triplet parameter, results in more strongly connected components, as shown by the increased number of edges and the thicker lines between nodes. The lack of fit in all models is clear:  not one of the three model expectations show node 16 gaining ties as it does in $x(t_2)$, nor do they show nodes 4 and 7 becoming isolated. In model M2, however, the ties to node 7 appear much weaker than in model M1 or model M3, and the nodes $\{10,11,14\}$ are also more strongly connected, as they are in wave 2 of data. This suggests that, despite the fact that neither of the three models is a good fit, M2 may be the best fitting model of the three. In a complete data analysis, we can use this information to select more models for analysis that utilize transitivity and covariate effects, as opposed to structural and indirect effects.  
% We can use the average network to help determine model goodness-of-fit. Because the the expected  network looks more like the first wave than the second wave, we can use the visualization in Figure~\ref{fig:summnetM1} as evidence of poor model fit.  

<<summary2, fig.height=2.5, fig.cap="Differences between the expected network  $\\bar{x}(t_2)$ and $x(t_1)$  and  $x(t_2)$. Some edges appear in both the data and the expected network $\\bar{x}(t_2)$ (dotted lines), while others only appear in the data (solid lines) or only in the expectation (dashed lines). Width of non-data edges represent the proportion of appearances in the 1,000 simulations. The FR layout is used.">>=

red <- "#fd8d3c"
green <- "#41ab5d"

am_t1 <- as.matrix(igraph::as_adj(igraph::graph_from_data_frame(arrange(t1, parse_number(from), parse_number(to)))))[-17,-17]

diff1 <- am_t1 - fd2.w1
diff1[which(diff1==1)] <- 2
diff1[which(fd2.w1 == 1 & am_t1 == 1)]  <- 1
diff1 <- diff1 %>% dplyr::as_data_frame() 
names(diff1) <- paste0("V", 1:16)
diff1 %>% mutate(from =paste0("V", 1:16)) %>% gather(to, value, -from) %>%
  mutate(from = as.character(parse_number(from)), to = as.character(parse_number(to)), 
         var = recode(value, `0` = "Neither", `2` = "InAvgNotW1",  `1`= "InBoth"),
         color2 = recode(value, `0`="white", `1`= "black", `2` = green)) %>%
  filter(value !=0) %>% 
  full_join(t1, c("from","to")) -> t1_1 
t1_1 <- arrange(t1_1, from, to)
p1 <- ggplot(data = t1_1) +
  geom_net(aes(from_id = from, to_id = to, linewidth= linewidth, linetype = color2),
           directed = T, curvature = .2, labelon=T, size = 3,
           arrow = arrow(angle = 20, type = "open", length = unit(.15, "cm")), 
           color = "#252525", hjust = .5, vjust = .5, labelcolour = 'white', fontsize = 2, arrowgap = .02, layout.alg = "fruchtermanreingold",
           singletons = T) +
  scale_linetype_manual(values =c("longdash", "dotted") ) + 
  labs(title = "Difference between Time 1 Data &\n Expectation from M1") + ThemeNet +
  theme(plot.background = element_rect(color = 'black'), legend.position = "none")

pforleg <- ggplot(data = mtcars) + 
  geom_line(aes(x = mpg, drat, group = as.factor(cyl), linetype = as.factor(cyl))) + 
  scale_linetype_manual(values = c("dotted", "solid", "longdash"), name = "Tie is...", labels = c("in data, in\nexpectation", "in data,not in\nexpectation", "not in data, in\nexpectation")) + 
  theme_grey() + 
  theme(legend.key = element_rect(color = "black", fill = "white"), 
        legend.key.height = unit(25, "pt"))


diff2 <- -am_t1 + fd2.w2 
diff2[which(diff2==1)] <- 2
diff2[which(fd2.w2 == 1 & am_t1 == 1)]  <- 1
diff2 <- dplyr::as_data_frame(diff2)
names(diff2) <- paste0("V", 1:16)
diff2 %>% mutate(from =paste0("V", 1:16)) %>% gather(to, value, -from) %>% 
  mutate(from = as.character(parse_number(from)), to = as.character(parse_number(to)), 
         var = recode(value, `0` = "Neither", `-1` = "InAvgNotW2",  `1`= "InBoth", `2` = "InW2notAvg"),
         color2 = recode(value, `0`="white", `1`= "black", `2` = red, `-1` = green)) %>% 
  filter(value !=0) %>% 
  full_join(t1, c("from","to")) -> t1_2 
t1_2$linewidth[is.na(t1_2$linewidth) & !is.na(t1_2$to)] <- 1
t1_2 <- arrange(t1_2, from, to)
t1_2 <- t1_2 %>% filter(!(from == 16 & is.na(to)))
p2 <- ggplot(data = t1_2) +
  geom_net(aes(from_id = from, to_id = to, linewidth= linewidth, linetype = color2),
           directed = T, curvature = .2, labelon=T, size = 3,
           arrow = arrow(angle = 20, type = "open", length = unit(.15, "cm")), 
           color = "#252525", hjust = .5, vjust = .5, labelcolour = 'white', fontsize = 2, arrowgap = .02, layout.alg = "fruchtermanreingold",
           singletons = T) +
  scale_linetype_manual(values = c("longdash", "solid", "dotted")) + 
  labs(title = "Difference between Time 2 Data &\n Expectation from M1") + ThemeNet +
  theme(plot.background = element_rect(color = 'black'), legend.position = "none")
legendsumm <- cowplot::get_legend(pforleg)
ggdraw() + draw_plot(p1, width = .4) +  draw_plot(p2, width = .4, x=.4) + 
  draw_grob(legendsumm,x=.8, y=.25, width = .2, height = .4)
@

Another way to view our models in the data space is with \textit{difference networks} between the expected network and the data, as shown in Figure~\ref{fig:summary2}. We created these networks by computing the difference between $\bar{x}(t_2)$ and $x(t_1)$, $\mathbf{A}(\bar{x}(t_2)) - \mathbf{A}(x(t_1))$, and the difference between $\bar{x}(t_2)$ and $\bar{x}(t_2)$, $\mathbf{A}(x(t_2)) - \mathbf{A}(\bar{x}(t_2))$ for model M1. The lack of model fit is also apparent in Figure~\ref{fig:summary2}, where the dashed edges represent ties that only show up in the expected network, and the solid edges representing ties in the data that the model fails to capture.  Dotted edges are in both the expectation and the data and represent where the model is correct.  On the left in Figure~\ref{fig:summary2}, we see the difference between $x(t_1)$ and the expected network. We see mostly dotted edges, meaning that the simulated networks consist primarily of edges in $x(t_1)$, plus some new edges represented by the dashed lines that are not in the data. The unobserved ties are all reciprocating ties in the data, so the model is predicting more reciprocity between actors than there actually is. On the right in Figure~\ref{fig:summary2}, we see the difference between $x(t_2)$ and the expected network. Because the model is simulating  $x(t_2)$, we would hope to see very few differences between the expected network and the data. However, there are six solid edges, which are in the data but do not appear at all in the expected network, and there are many dashed edges that appear in the expected network but are not in $x(t_2)$. Although there are some edges that appear in both the expected network and $x(t_2)$, the majority of edges in the expected network are not in the observed data, and there are several edges in the data missing from the expectation. We therefore conclude that the model M1 is a poor fit to our example data, which we have seen very clearly by viewing the model in the data space. As with the expected network, we can apply the difference network computation other network models to determine how closely the model fits the data. 


\section{Collections of models}\label{sec:collectmodels}

Now that we have assessed model fit, we want to learn about model behavior in order to choose better fitting models in the future. To assess model behavior, we employ the second principle of model visualization: ``collections are more informative" \citep{modelvis}. We rely on three methods for constructing collections of CTMC models. First, we explore the space of all possible models to determine significant model parameters.  Second, we vary model settings to uncover relationships between parameters and their effects on the fitted model. Finally,  we fit the model many times to assess the behavior of the underlying simulations. We chose these three collections because they each explore something different about model behavior: the choice of and relationships between parameters in the objective function and the stochastic effects from simulation. %, and the resulting fitted values under different circumstances. %Using the tools in our visualization toolbox, we view these collections to learn about model behavior.%The first considers the many parameters to include in a SAOM, while the second results in different parameter estimates, so we can see how parameter estimates vary. The third collection shows how those many possible parameters affect the values of parameters in the fitted models. Finally, we explore how the same model behaves under two completely different circumstances. 

<<getergmeffects, eval = FALSE>>=
url <-"https://cran.r-project.org/web/packages/ergm/vignettes/ergm-term-crossRef.html"
library(rvest)
(read_html(url) %>% html_nodes("table"))[[2]] %>% html_table() %>% select(`Term name`) %>% distinct() %>% count()

@


\noindent \textbf{Exploring the space of all possible models:} The \texttt{RSiena} manual contains over 80 effects to include in the objective function \citep{RSienamanual}. To select objective function parameters for modeling our example data, we searched through all available parameters to find  significant effects. The \texttt{RSiena} package includes the function \texttt{Wald.Rsiena} for Wald-type tests of parameter significance \citep{RSiena}. We start by including the outdegree and reciprocity parameters in the model then add one effect to the objective function, fit the model to the example data, test the added effect for significance, and repeat for all possible model parameters. %We performed this procedure, which we used to pick M2 and M3, for both the small friendship and the senate collaboration data. A visualization of the significant effects for the two example data sets are available in the Supporing Information. 
Using this one-at-a-time testing method results in many possible significant parameters of different type and magnitude, demonstrating the flexibility of these models. We chose models M2 and M3 in addition to model M1 to examine more closely because their added effects had the smallest $p$-values from the Wald-type tests. This particular type of model exploration is applicable to any network model with an \texttt{R} package or other software that allows the user to identify model parameters, such as the \texttt{ergm} package, which has well over 100 effects for fitting exponential-family models to networks \citep{ergm1, ergm2}. %We see in both the friendship data and the senate data results that most of the significant effects have absolute value less than ten. In addition, the $p$-values for the effects from the friendship data are more spread out than the $p$-values for the senate data, which are concentrated at about 0.02 or less. This may suggest that larger data sets tend to result generally in smaller $p$-values, just like a larger sample size results in smaller $p$-values in a $t$-test.

%\subsection{Varying model settings}

\noindent \textbf{Varying model settings:} The second method for obtaining collections of models is to vary model settings. We have done this already by choosing three different models M1, M2, M3 to fit to our example data sets. CTMC models, however, are much more flexible than the three simple models we have chosen. Other models could have included different rate functions for each node, where for example the rate of change could depend on drinking behavior if we have reason to believe that girls who drink more change friends more frequently. We could also model the behavior changes, not just the network changes, if we believe that friends influence behavior, via peer pressure.  And as before, this method of gathering a collection of models can be used for any network model, such as the hierarchichal Bayesian network estimation model in the \texttt{R} package \texttt{sna}, which allows the user to vary prior and model parameters \citep{sna}.
% took out this next bit just because it doesn't really add much and we can just put the appendix figure into the previous section. % add more later.

%To explore the behavior of these three models, we simulate from them. Using the estimated parameter values in Table~\ref{tab:meanests}, we simulated 1,000 observations of $x(t_2)$ and $x(t_3)$ of the friendship data from each of the three models. From the simulations, we created an expected network for $x(t_2)$ from models M1, M2, and M3 using the same method described in Section~\ref{sec:minds}. These expected networks are presented in the Supporting Information, and we see that all models are failing to capture the structure of $x(t_2)$, even though the individual parameters were all significant according to the statistical tests. %In Section~\ref{sec:samemodsamedat}, we fit these three models to the data 1,000 times, and examine the parameter densities. Here, . For simulation, we used the mean parameter values from the 1,000 fits of the corresponding model. 

%\subsection{Fitting the same model to the same data}\label{sec:samemodsamedat}
\begin{figure}
\begin{subfigure}[t]{.4\textwidth}
<<distests, fig.height=7.5>>=
levels(simu2$parameter) <- c("alpha[1]","alpha[2]","beta[1]","beta[2]","beta[3]","beta[4]")
filter(simu2, parameter %in% c("beta[3]","beta[4]")) %>% 
ggplot() +
  geom_density(aes(x = estimate)) +
  facet_wrap(~parameter, scales = 'free', labeller= label_parsed, nrow = 2) +
  ThemeNoNet +
  scale_fill_brewer(palette = "Greys") + 
  xlab("estimate value") + 
  theme(legend.position = 'bottom')
@
\caption{\label{fig:beta34} Distribution of fitted $\beta_3, \beta_4$ values.}
\end{subfigure}
\begin{subfigure}[t]{.6\textwidth}
<<img, out.width = "\\linewidth">>=
# correlation for other params. 
simsM2 <- filter(simu2, Model == "M2") %>% spread(parameter, estimate)
corm2 <- cor(simsM2[, 3:7])
simsM3 <- filter(simu2, Model == "M3") %>% spread(parameter, estimate)
corM3 <- cor(simsM3[, 3:7])

knitr::include_graphics("img/gray_corplot-1.png")
@
\caption{\label{fig:corplots} Relationships between parameters common to all three models. \nocite{ggally}}
\end{subfigure}
\caption{\label{fig:distests} Distribution of fitted parameter values for our three models. The inclusion of $\beta_3$ or $\beta_4$ clearly has an effect on the distributions of the rate parameters, $\alpha_1$ and $\alpha_2$, and on the other objective function parameters $\beta_1, \beta_2$.}
\end{figure}

\noindent \textbf{Fitting the model repeatedly:} The simulations required to fit the CTMC network models lead to inherent variability in the fitted values. To examine this variability, we fit models M1, M2, and M3 to the friendship network 1,000 times each. The resulting means and standard deviations of the estimates are given in Table~\ref{tab:meanests}, and the distributions of all parameter estimates are shown in Figure~\ref{fig:distests}. In Figure~\ref{fig:corplots}, we can see that the addition of $\beta_3$ to the model results in very different estimates the of the other four parameters, $\alpha_1$, $\alpha_2$, $\beta_1$, and $\beta_2$. In fact, the distributions of $\alpha_1$ and $\beta_1$ for M2 do not overlap at all with the distribution of the same parameters from model M1. On the other hand, the inclusion of the indirect tie parameter, $\beta_4$, does not lead to such drastically different estimates of other model parameters. When $\beta_3$ is included, its estimate is postitive, meaning that friendships between two girls with different drinking behaviors tend to form when there is third girl who is already friends with the other two. The inclusion of $\beta_3$ also leads to increases in the estimates of $\alpha_1, \alpha_2$. Thus, when modeling the transitive triplet behavior in this way, the model concludes that girls change friends more frequently. It is not clear, however, that the addition of a parameter to the objective function should effect the estimates of the rate parameters. %This strong and possibly problematic relationship between rate parameter estimates and objective function parameters is further explored in the Supporting Information. When visualizing the results of the same models fit many times to the same data, we can to see potential model behavior problems right away.   
In Figure~\ref{fig:corplots}, we further examine the relationship between each of pair of parameters common to all three models. The strongest correlation within each model is between $\beta_1$ and $\beta_2$, with absolute correlation greater than 0.90 in all three models. The $\beta_1$ parameter is also highly correlated with the $\beta_3$ parameter within model M2 ($\rho = $\Sexpr{round(corm2[5, 3])}), but it is not as highly correlated with the $\beta_4$ parameter in model M3 ($\rho = $\Sexpr{round(corM3[5, 3])}). The inherent relationship between effects is a known problem in network analysis \citep{goldenberg09}. 
% need a better reference here. 

%It might therefore be advisable to consider only models that either allow $\beta_1$ or $\beta_2$ but not both. Looking at the high correlation with $\alpha$, we might switch to a model without $\beta_1$.

% maybe talk about dependence of statistics, inherent correlation, etc. 


%In comparison with models M1 and M3, model M2 typically has higher estimates of the rate parameter, meaning that the inclusion of the covariate statistic in the model leads to higher estimates of the number of times, on average, a node gets to change its ties. 

%\subsection{Fitting the same model to different data}\label{sec:samemoddiffdat}

<<tableofmeans, eval = FALSE>>=
senM1est <- read.csv("data/congress/senateM1ests.csv")
senM2est <- read.csv("data/congress/senateM2ests.csv")
senM3est <- read.csv("data/congress/senateM3ests.csv")
senM1est$Model <- "M1"
senM2est$Model <- "M2"
senM3est$Model <- "M3"
senM1est$Sim <- 1:nrow(senM1est)
senM2est$Sim <- 1:nrow(senM2est)
senM3est$Sim <- 1:nrow(senM3est)
senM1est %>% gather(parameter, estimate, Rate:recip) %>% dplyr::select(Sim, Model, parameter, estimate) -> senM1est2
senM2est %>% gather(parameter, estimate, Rate:jumpXTransTrip) %>% dplyr::select(Sim, Model, parameter, estimate) -> senM2est2
senM3est %>% gather(parameter, estimate, Rate:nbrDist2twice) %>% dplyr::select(Sim, Model, parameter, estimate) -> senM3est2
senateEsts <- rbind(senM1est2, senM2est2, senM3est2)
senateEsts$parameter <- as.factor(senateEsts$parameter)
levels(senateEsts$parameter) <- c("beta1", "beta3", "beta4", "alpha1", "alpha2", "alpha3", "beta2")
SSE <-
  senateEsts %>% group_by(Model, parameter) %>%
  summarize(meanEst = mean(estimate), sdEst = sd(estimate)) %>%
  mutate(val = paste0(sprintf("%.3f",meanEst), " ", "(", sprintf("%.3f", sdEst), ")")) %>%
  dplyr::select(Model, parameter, val) %>%
  spread(Model, val) %>% arrange(as.character(parameter))
SFE <- simu2 %>% group_by(Model, parameter) %>%
  summarize(meanEst = mean(estimate), sdEst = sd(estimate)) %>%
   mutate(val = paste0(sprintf("%.3f",meanEst), " ", "(", sprintf("%.3f", sdEst), ")")) %>%
  select(Model, parameter, val) %>%
  dplyr::select(Model, parameter, val) %>%
  spread(Model, val) %>% arrange(parameter)
@

% % table of fitted M1 M2 M3
% \begin{table}
% \scalebox{0.9}{
% \begin{tabular}{lccc|ccc }
% 
%   & \multicolumn{3}{c}{Friendship Data} & \multicolumn{3}{c}{Senate Data} \\
%   \hline
%   & M1 & M2 & M3 & M1 & M2 & M3 \\
%   \hline
%   $\alpha_1$ & \Sexpr{SFE$M1[1]} & \Sexpr{SFE$M2[1]} & \Sexpr{SFE$M3[1]} & \Sexpr{SSE$M1[1]} & \Sexpr{SSE$M2[1]} & \Sexpr{SSE$M3[1]} \\
%   $\alpha_2$ & \Sexpr{SFE$M1[2]} & \Sexpr{SFE$M2[2]} & \Sexpr{SFE$M3[2]} & \Sexpr{SSE$M1[2]} & \Sexpr{SSE$M2[2]} & \Sexpr{SSE$M3[2]}  \\
%   $\alpha_3$ & -- & -- & -- & \Sexpr{SSE$M1[3]} & \Sexpr{SSE$M2[3]} & \Sexpr{SSE$M3[3]} \\
%   \hline
%   $\beta_1$ & \Sexpr{SFE$M1[3]} & \Sexpr{SFE$M2[3]} & \Sexpr{SFE$M3[3]} & \Sexpr{SSE$M1[4]} & \Sexpr{SSE$M2[4]} & \Sexpr{SSE$M3[4]} \\
%   $\beta_2$ & \Sexpr{SFE$M1[4]} & \Sexpr{SFE$M2[4]} & \Sexpr{SFE$M3[4]} & \Sexpr{SSE$M1[5]} & \Sexpr{SSE$M2[5]} & \Sexpr{SSE$M3[5]}\\
% $\beta_3$ & -- & \Sexpr{SFE$M2[5]} & -- & -- & \Sexpr{SSE$M2[6]} & -- \\
%   $\beta_4$ & -- & --  & \Sexpr{SFE$M3[6]} & -- & -- & \Sexpr{SSE$M3[7]} \\
%   \hline
% \end{tabular}
% }
% \caption{\label{tab:meanests} The means (std.\ dev.)\ of parameter values estimated from repeated fittings of $M1, M2, M3$ to the two example data sets.}
% \end{table}




% \noindent \textbf{Fitting the same model to different data:} Finally, we view collections of models by fitting the same models to different data to understand how the model behaves in different contexts. As we did for the friendship data, we estimated the parameters for M1, M2, and M3, when fit to the senate data (see Figure \ref{fig:senateplot}) 100 times each. The means and standard deviations of the repeated parameter estimates for all six model-data combinations are given in Table~\ref{tab:meanests}, while corresponding density plots of the objective function parameters are provided in the Supporting Information. Looking at the estimates for both data sets, a few patterns emerge. First, we can see the same relationship between the outdegree parameter, $\beta_1$ and the reciprocity parameter, $\beta_2$ in both data sets and across all three models. The estimates of $\beta_1$ are all negative and range from about -5 and -3, while the estimates of $\beta_2$ are all positive and range from about four to five. %In addition, the outegree parameter, $\beta_1$, \textit{decreases} when $\beta_3$ is included, while the reciprocity parameter, $\beta_2$ increases. This inverse relationship implies that the girls prefer to form small, closely knit friend groups, as opposed to being popular and having many friends. Tight-knit friend groups are more encouraged by model M2 than by models M1 and M3.
% This suggests that in both data sets, nodes prefer outgoing ties that are reciprocated to those that are not. When we consider the context, this seems obvious: the students want to have close mutual friendships, and senators want mutual support for their bills. We are also concerned, however, with the extremely high correlation between the estimates of $\beta_1, \beta_2$. In all six model-data combinations, the absolute correlation between $\hat{\beta}_1, \hat{\beta}_2$ is greater than 0.93. %We explore the relationship between $\beta_1$ and $\beta_2$ further in Section~\ref{sec:samemodsamedat}.
% % We have seen how the same model forms behave in different context, with some  differences and some similarities. 
% After seeing the high correlation between parameter estimates in very different data contexts, we might question the guideline that both parameters need to be included in the objective function. 
% 
% In Figure~\ref{fig:distests}, we saw that the inclusion of $\beta_3$ for the friendship data has a noticeable effect on the other parameters in the model. In Figure~\ref{fig:compareModels}, however, we see that this effect is not present in the senate data. Furthermore, the estimates of $\beta_4$ for the senate data are near zero, suggesting that this effect, which considers indirect ties, is not informative for the senate data. In contrast, for the friendship data the mean estimate of $\beta_4$ is about -7.5, which suggests that indirect ties are \textit{strongly} discouraged from forming. Again, this is fairly intuitive given the context: teenage girls likely want to have direct friendships with each other. Senators, on the other hand, have to wheel and deal to pass their legislation, and indirect relationships may be beneficial. By examining the results of models fitted to different data, we better understand the interpretation of the objective function parameters, and we have learned of some potential parameter correlation problems affecting the fit of the models. 

%All in all, we have used three different ways of viewing collections of models, which have given us insight into the behavior of CTMC network models.

%Although this indirect relationship variable does not help explain the senate collaboration structure, it does help clarify the teenage friendship structure.

%We look at collections produced ``from exploring the space of all possible models," ``by varying model settings", ``by fitting the same model to different datasets", and ``by treating each iteration as a model" \citep[p.~210-11]{modelvis}. Because fitting a SAOM requires MCMC methods, we also fit the same model many times, resulting in distributions of fitted parameters for one data set.


% taken out.
<<getdatam, eval = FALSE, purl=FALSE>>=
nulls <- read.csv("data/distribution_null_model.csv")
alt <- read.csv("data/distribution_jumpTT_model.csv")
alt2nd <- read.csv("data/distribution_dblpairs_model.csv")
means <- simu2 %>% group_by(Model, parameter) %>% summarize(
  means = mean(estimate)
)
null_mod_sv <- as.numeric(nulls[,-c(1,6,7)] %>% summarise_all(funs(mean)))
alt_mod_sv <- as.numeric(alt[,-c(1, 7,8)] %>% summarise_all(funs(mean)))
alt_mod2_sv <- as.numeric(alt2nd[,-c(1,7,8)] %>% summarise_all(funs(mean)))
@

% taken out
<<corplotsSenate, eval = FALSE, purl = FALSE>>=
senM1est <- read.csv("data/congress/senateM1ests.csv")
senM2est <- read.csv("data/congress/senateM2ests.csv")
senM3est <- read.csv("data/congress/senateM3ests.csv")
senM4est <- read.csv("data/congress/senateM4ests.csv")
senM5est <- read.csv("data/congress/senateM5ests.csv")
names(senM1est) <- c("alpha1", "alpha2", "alpha3", "beta1", "beta2")
names(senM2est) <- c("alpha1", "alpha2", "alpha3", "beta1", "beta2", "beta3")
names(senM3est) <- c("alpha1", "alpha2", "alpha3", "beta1", "beta2", "beta4")
names(senM4est) <- c("alpha1", "alpha2", "alpha3", "beta1", "beta2", "beta5")
names(senM5est) <- c("alpha1", "alpha2", "alpha3", "beta1", "beta2", "beta6")
senM1est$Model <- "M1"
senM2est$Model <- "M2"
senM3est$Model <- "M3"
senM4est$Model <- "M4"
senM5est$Model <- "M5"
senM1est$Sim <- 1:nrow(senM1est)
senM2est$Sim <- 1:nrow(senM2est)
senM3est$Sim <- 1:nrow(senM3est)
senM4est$Sim <- 1:nrow(senM4est)
senM5est$Sim <- 1:nrow(senM5est)
senM1est2 <- senM1est %>% gather(parameter, estimate, alpha1:beta2)
senM2est2 <- senM2est %>% gather(parameter, estimate, alpha1:beta3)
senM3est2 <- senM3est %>% gather(parameter, estimate, alpha1:beta4)
senM4est2 <- senM4est %>% gather(parameter, estimate, alpha1:beta5)
senM5est2 <- senM5est %>% gather(parameter, estimate, alpha1:beta6)
senM45est <- rbind(senM4est2, senM5est2)
senM123est <- rbind(senM1est2, senM2est2, senM3est2)
senM123est %>% spread(parameter, estimate)-> senM123est2

senM45est %>% spread(parameter, estimate)-> senM45est2
ggpairs(senM123est2, columns = 3:9, ggplot2::aes(colour=Model, alpha = .5))
@

\section{Explore algorithms, not just end result}\label{sec:algorithms}

The third and final model visualization principle, ``explore the process of model fitting," is crucial to understanding the CTMC models \citep{modelvis}. When fitting models in \texttt{RSiena}, several simulation processes are hidden from the user. We refer to one of these hidden pieces in the SIENA MoM algorithm as the \textit{microstep process}, which is a series of state changes in the CTMC as described in Section~\ref{sec:saoms}. At each microstep, an ego node is selected to change, and then randomly makes one change in its ties according to to the probabilities $p_{ij}$ defined in Equation~\ref{eq:pij}. This process continues until the network differs from the $x(t_m)$ by exactly $C_m$, as defined in Equation~\ref{eq:cdiff}. Between two network observations $x(t_m)$ and $x(t_{m+1})$, there can be dozens, hundreds, or even thousands of microsteps, depending on the size of the network and the value of $C_m$. We want to simulate and view these in-between steps in order to better understand the behavior of the  CTMC model. Hiding the microsteps is practical and efficient if the goal is to fit a model to data, obtain parameter estimates, and draw conclusions or make predictions. But in order to understand how the models are fit, we need to extract and explore the different steps of the process. We use these steps to visually explore the algorithms, so that we are visualizing the model fitting process, and not only the model fitting result.  

%A series of microsteps is obtained by simulating from the model in its current state, $x(t_m)$ with current parameter values $\mathbf{\theta}_{0} = \{\alpha_{1_0}, \dots \alpha_{{(m-1)}_0}, \beta_{1_0}, \dots, \beta_{K_0}\}$, to the next state, $x(t_{m+1})$. One tie changes at a time until the simulated network has achieved the same number of differences, $C$, from $x(t_m)$ as $x(t_{m+1})$, where $C$ is definined in Equation~\ref{eq:cdiff}. 

%This simulation process follows the steps of the continuous-time Markov chain. Each tie change in the CTMC is referred to as one ``microstep".  The options for change are removing a current tie, adding a new tie, or making no change at all. 

<<makinggif, fig.cap='A selection of images in the microstep animation at \\url{https://vimeo.com/240089108}. The selected edges and nodes are emphasized by changing size and color, then they fade in or out while the nodes change color and shrink to blend in with the other nodes. Frames 15-28 show edge destruction, while frames 71-84 show edge creation. A spring layout is used.', message = FALSE, warning = FALSE, fig.height=2.5>>=
friend.data.w1 <- as.matrix(read.table("data/s50_data/s50-network1.dat"))
friend.data.w2 <- as.matrix(read.table("data/s50_data/s50-network2.dat"))
friend.data.w3 <- as.matrix(read.table("data/s50_data/s50-network3.dat"))
drink <- as.matrix(read.table("data/s50_data/s50-alcohol.dat"))
fd2.w1 <- friend.data.w1[20:35,20:35]
fd2.w2 <- friend.data.w2[20:35,20:35]
fd2.w3 <- friend.data.w3[20:35,20:35]
friendshipData <- array(c(fd2.w1, fd2.w2,fd2.w3), dim = c(16, 16, 3))

ansnullchains %>%
  dplyr::filter(period == 1) %>%  #only look at chains from wave 1 to wave 2
  dplyr::select(rep, from = ego, to = alter) %>%
  dplyr::mutate(val = as.numeric(!from == to),
         from = paste0("V", parse_number(as.character(from))+1), # make the chains
         to = paste0("V", parse_number(as.character(to))+1)) -> ansnullchainsw1w2
colnames(fd2.w1) <- paste0("V", 1:16)
rownames(fd2.w1) <- paste0("V", 1:16)
wave1friends <- fortify(as.adjmat(fd2.w1))
ms1 <- netvizinf::listMicrosteps(dat = wave1friends,
                      microsteps = dplyr::filter(ansnullchainsw1w2, rep == 1))
microsteps <- ms1[1:20]

microsteps_df <- getMicrostepsDF(microsteps)
pte <- pretween_edges(microsteps = microsteps)
ptv <- pretween_vertices(pte = pte, layoutparams = list(n = 16))
# step 1 is remove, step 9 is add.
# get tweened data for all microsteps
pall <- tween_microsteps(ptv, pte, microsteps_df)

static_pall <- dplyr::filter(pall, .frame %in% c(15,20,22,24,26:28, 71:73, 75, 79, 81, 84))

static_pall$type <- NA
static_pall$type[which(static_pall$.frame %in% c(15,20,22,24,26:28))] <- "Remove Edge"
static_pall$type[which(static_pall$.frame %in% c(71:73, 75, 79, 81, 84))] <- "Add Edge"

set.seed(34569234)
froms <- static_pall %>% dplyr::select(id = from, microstep, x = from.x,
        y = from.y, ms, .frame, size, color) %>% unique
tos <- static_pall %>% dplyr::select(id = to, microstep, x = to.x, y = to.y,
        ms, .frame) %>% unique
tos <- dplyr::left_join(tos, froms)
nodedata <- rbind(froms, tos) %>% unique
nodedata <- nodedata %>% tidyr::replace_na(replace = list(size = 1,
        color = "#333333"))
to_ani <- ggplot() +
  geom_curve(data = static_pall, aes(x = from.x,
        y = from.y, xend = to.x, yend = to.y, frame = .frame,
        colour = ecolor, size = esize), curvature = 0.1, arrow = arrow(angle = 20,
        length = unit(0.1, "inches"))) +
  geom_point(data = nodedata,
        aes(x = x, y = y, colour = color, size = size, frame = .frame)) +
  scale_colour_identity() + scale_size_identity() + theme_void() +
        coord_fixed()
to_ani +
  facet_wrap(~.frame, nrow=2, labeller = 'label_both') +
  ThemeNet +
  theme(panel.background = element_rect(color = 'black'))
@


To visualize the underlying algorithm, we animate the microsteps that form the CTMC from $x(t_1)$ to $x(t_2)$ for the example data. The microstep animation, some frames of which are shown in Figure~\ref{fig:makinggif}, represent one possible path from $x(t_1)$ to $x(t_2)$ according to the model M1. We show each change in the network as follows: we first increase the size and change the color of the ego node to focus attention to its ties. Then, the changing tie either fades in or fades out. If there are no changes at a particular microstep, the microstep does not appear. Some frames of the animations are shown in Figure~\ref{fig:makinggif}, and the full movie, created with \texttt{tweenr} and \texttt{gganimate} can be viewed at \url{https://vimeo.com/240089108} \citep{tweenr, gganimate}. Movies similar to this animation were used to visualize the changes of dynamic networks in \citet{dynnetviz}. Frames 15-28 of Figure~\ref{fig:makinggif} show tie that is destroyed, while frames 71-84 show a tie that is created. For both changes, the ego node is emphasized with color and size. When a tie is removed, it is first emphasized with the same color and size as the node. As the animation proceeds, the tie shrinks and disappears as the ego node shrinks and changes color to match the others. If a tie is being added, it appears red from nothing, grows large for emphasis, then changes color and size to match the rest of the edges. In this network animation, we show the modeled steps of the unobserved CTMC process between two observed networks. We see each part of the hierarchical model: first, the selection of the ego node via the rate function, then the result of the actor maximizing its objective function by either deleting or adding a node. Finally, the layout of the network transitions as edges are removed or added, demonstrating how the overall network structure changes with each of the individual tie changes. Combining the tie changes in this animation makes the CTMC process the focus, demonstrating the underlying change mechanism of the dynamic network according to the model. % need more about what we learned. 

<<adjmatorder, message = FALSE, warning = FALSE, fig.cap="On the left, the first observation of the friendship network represented in adjacency matrix form, ordered by node ID. On the right, the same adjacency matrix is presented after seriation", fig.width=8, fig.height=3, fig.align='center'>>=
# first present the original wave 1 adj mat, then present the adj mat after being ordered by the 1st pc of the big adjmat A.
fd.w12 <- data.frame(fd2.w1)
fd.w12 <- data.frame(fd2.w1)

numordersf <- fd.w12 %>%
  mutate(from = names(fd.w12)) %>%
  gather(key = to ,value = value, -from)

pcaordersf <- numordersf

numordersf$from <- factor(numordersf$from)
levels(numordersf$from) <-  1:16
numordersf$to <- factor(numordersf$to)
levels(numordersf$to) <- 1:16

p1 <- ggplot(data = numordersf, aes(y = rev(from), x = to)) + geom_tile(color = 'black', aes(fill = as.factor(value))) +
  scale_fill_manual(values = c('white', 'black')) +
  scale_x_discrete(labels = 1:16, position = "top", name = "alter (j)") +
  scale_y_discrete(labels =16:1, name = "ego (i)")  +
  ThemeNoNet +
  theme(legend.position = 'none', panel.grid = element_blank(), 
        panel.border = element_blank(), 
        axis.text.y = element_text(margin = margin(r=0))) +
  coord_fixed() 

# histogram of number of MS in 1000 reps
sfmsall <- read_csv("data/smallfriendsmicrosteps.csv")

sfmsall %>%
  group_by(from, to) %>%
  filter(rep == 1) %>%
  summarise(count = n()) -> sfmsall_summ
sfmsall_summ <- sfmsall_summ %>% ungroup %>% mutate(from = parse_number(from), to = parse_number(to))
sfmsall_summ$from <- as.factor(sfmsall_summ$from)
levels(sfmsall_summ$from) <- paste0('V', 1:16)
sfmsall_summ$to <- as.factor(sfmsall_summ$to)
levels(sfmsall_summ$to) <- paste0('V', 1:16)
sfmsall_summ %>%
  spread(to, count, fill = 0) %>%
  data.frame -> allmsrep1_adjmat
allmsrep1_adjmat$V16 <- 0
allmsrep1_adjmat <- allmsrep1_adjmat[,-1]
allmsrep1_adjmat <- as.matrix(allmsrep1_adjmat)
rownames(allmsrep1_adjmat) <- paste0("V", 1:16)

pca1 <- prcomp(allmsrep1_adjmat)
order1 <- names(sort(pca1$rotation[,1], decreasing = T))
order1 <- gsub("V", "",order1)

pcaordersf$from <- as.factor(pcaordersf$from)
levels(pcaordersf$from) <- gsub("V", "", levels(pcaordersf$from))
pcaordersf$from <- factor(pcaordersf$from , levels = levels(pcaordersf$from)[match( order1,levels(pcaordersf$from) )])
pcaordersf$to <- as.factor(pcaordersf$to)
levels(pcaordersf$to) <-gsub("V", "", levels(pcaordersf$to))
pcaordersf$to <- factor(pcaordersf$to , levels = levels(pcaordersf$to)[match( order1,levels(pcaordersf$to) )])

p2 <- ggplot(data = pcaordersf, aes(x = to, y = from)) +
  geom_tile(color = 'black', aes(fill = as.factor(value))) +
  scale_fill_manual(values = c('white', 'black'), name = "Tie is...", labels = c("not present","present")) +
  scale_x_discrete(position = "top", name = "alter (j)") +
  ThemeNoNet +
  coord_fixed()
p2a <- ggplot(data = pcaordersf, aes(x = to, y = from)) +
  geom_tile(color = 'black', aes(fill = as.factor(value))) +
  scale_fill_manual(values = c('white', 'black')) + 
  scale_x_discrete(position = "top", name = "alter (j)") +
  scale_y_discrete(name = "ego (i)",labels = rev(str_replace(order1, "V", ""))) +
  ThemeNoNet + theme(legend.position = "none") + 
  coord_fixed()
legendp2 <- cowplot::get_legend(p2)
ggdraw() + draw_plot(p1, width = .4) +  draw_plot(p2a, width = .4, x=.4) + 
  draw_grob(legendp2,x=.8, y=.25, width = .2, height = .4)
@

We continue to use animation to view the changing structure of the network via adjacency matrix visualizations. In the first adjacency matrix visualization in Figure~\ref{fig:adjmatorder}, we use the node ID to order the rows and columns. This arbitrary ordering does not easily convey the underlying structure of the network. We use \textit{matrix seriation} to reorder the rows and columns of the adjacency matrix visualization so that the structure of the network is apparent \citep{seriation}. %This lack of perceived structure would contribute to further confusion as the network chages, so we adjust the ordering so that the viewer can better perceive the structure of the network. This process is known as matrix seriation 
To seriate the matrix, we first construct a cumulative adjaceny matrix, $\mathbf{A}^{cum}$, for the series of microsteps in the CTMC simulated from $x(t_m)$ to $x(t_{m+1})$. A single entry in the cumulative adjacency matrix, $\mathbf{A}^{cum}_{ij}$, is the total number of times the edge $x_{ij}$ appears in the network from the initial observation, $x(t_m) \equiv X(0)$ to the last microstep, $X(R)$, where $R$ is the total number of microsteps taken:
\begin{equation}\label{eq:acum}
\mathbf{A}^{cum}_{ij} = \sum_{r = 0}^R X_{ij}(r). 
\end{equation}
We then perform a principal component analysis (PCA) on $\mathbf{A}^{cum}$, and use the values of the first principal component to order the vertices on the $x$ and $y$ axes for the adjacency matrix net-vis. The resulting visualization for the first network observation is shown on the right in Figure~\ref{fig:adjmatorder}. %we present the first wave of data ordered by the arbitrary node ID order alongside the seriated adjacency matrix ordered by the first principal component loading on the cumulative adjacency matrix, $\mathbf{A}^{cum}$. 
Using matrix seriation on the %PCA on $\mathbf{A}^{cum}$ to order the rows and columns of the 
adjacency matrix visualization clearly shows the two distinct connected components in the first network, which are difficult to find in the visualization ordered by node ID. We continue to use the seriated matrix in another animation of the microstep process, which is available at \url{https://vimeo.com/240092677}. Some frames of this video are shown in Figure~\ref{fig:adjmatani}. In the animation, a square appears or disappears as that edge appears or disappears in the microstep process. %Through this animation, we can see edges appearing, and then later on disappearing. 
This vanimation, unlike the node-link visualization, shows the microsteps where the chosen node does not make a change. From start to finish, we see the network evolve: sometimes ties are removed right after they are created, and sometimes several ties are removed or created in a row. The adjacency matrix animation shows the algorithm exploring the data space. In contrast to the previous animation, all possible edges are represented in the adjacency matrix visualization. By viewing this animation, we gain a better understanding of the underlying dynamics of this model. % need more oomph 

<<adjmatani, fig.cap='A selection of frames from the adjacency matrix visualization animation for one series of microsteps. The node labels are removed to declutter the figure. Starting at frame 0 (the data), there are two clearly connected components: one in the bottom left, and one in the top right. By the end, the bottom left component has become more sparse, while the top right component has shrunk, but remains closely connected.', warning = FALSE, message = FALSE, results='hide', fig.height=3, out.width="\\linewidth">>=
adjmatall <- matrix(0, nrow = 16, ncol = 16)
rownames(adjmatall) <- paste0("V", 1:16)
colnames(adjmatall) <- paste0("V", 1:16)
dat2 <- sfmsall %>% filter(rep == 1)
adjmatall2 <- NULL
for(i in 0:55){
  MS <- dat2[dat2$ms == i,c("from", "to")]
  myadjmat <- adjmatall
  for(j in 1:nrow(MS)){
    myadjmat[MS[j,"from"][[1]],MS[j,"to"][[1]]] <- 1
  }
  adjmatall2 <- rbind(adjmatall2, myadjmat)
}

adjmatall2 <- cbind(adjmatall2, rep(0:55, each = 16))
rownames(adjmatall2) <- NULL
adjmatall2 <- as.data.frame(adjmatall2)
adjmatall2$from <- rep(paste0("V", 1:16), 56)
names(adjmatall2)[17] <- "ms"
adjmatall2 %>% gather(to, val, V1:V16) -> adjmatall3

adjmatall3$to <- as.factor(adjmatall3$to)
levels(adjmatall3$to) <- gsub("V", "", levels(adjmatall3$to))
adjmatall3$from <- as.factor(adjmatall3$from)
levels(adjmatall3$from) <- gsub("V", "", levels(adjmatall3$from))
adjmatall3$to <- factor(adjmatall3$to, levels = order1)
adjmatall3$from <- factor(adjmatall3$from, levels = order1)

adjmatall3$frame <- adjmatall3$ms

dat3 <- adjmatall3 %>% filter(ms %in% c(0,2,4,6,8,10,12,43,45,47,49,51,53,55))

dat4 <- dat3 %>% select(-ms) %>% spread(frame, val)
dat5 <- dat4
for(i in 4:16){
  for(j in 1:nrow(dat5)){
    if(dat4[j,i] != dat4[j,i-1]){
      if(dat4[j,i] > dat4[j,i-1]){
        dat5[j,i] <- 2
      } else { dat5[j,i] <- 3}
    }
  }
}
#Sys.time() - p

dat6 <- dat5 %>% gather(frame, val, 3:16) %>% mutate(frame = parse_number(frame))

ggplot() +
  geom_tile(data = dat6, aes(x = to, y = from, fill = as.factor(val), color=as.factor(val))) +
  scale_fill_manual(values = c("white", "black", "red", "white"), 
                    labels = c("not present", "present", "added", "removed"), 
                    name = "Tie is...") +
  scale_color_manual(values = c("#969696", "#969696", "red", "red"), 
                    labels = c("not present", "present", "added", "removed"), 
                    name = "Tie is...") + 
  geom_tile(data = dat6 %>% filter(val == 3), aes(x = to, y = from), fill = NA, color = 'red', size = .25) +
  scale_x_discrete(position = "bottom") +
  labs(x = "alter (j)", y = "ego (i)") +
  ThemeNoNet +
  theme(axis.text.x.bottom = element_blank(),
        axis.text.y = element_blank(), 
        strip.text = element_text(size = rel(.75)),
        strip.text.x = element_text(size = rel(.75)),
        strip.text.y = element_text(size = rel(.75)),
        legend.text = element_text(size = rel(.75)), 
        legend.title = element_text(size = rel(.75)), 
        legend.position = "bottom") +
  coord_fixed() +
  facet_wrap(~frame, labeller = 'label_both', nrow = 2)

@

% removed
<<histogramalters, fig.width = 7, fig.height=9, fig.cap="Each panel shows the theoretical (as lines) and empirical (as points) probabilities of the chosen ego node changing its tie to each of the other nodes. The color of the line indicates whether the tie from the ego to the alter node is being added, removed, or if there is no change to the network in this step.", eval = FALSE, purl=FALSE>>=
ansnullchains %>% group_by(period, rep) %>%
  mutate(ms = row_number()) %>%
  filter(ms == 1, period == 1) %>%
  mutate(alterProb = exp(logAlterProb),
         ego = as.numeric(as.character(ego)),
         alter = as.numeric(as.character(alter))) %>%
  ungroup() %>%
  group_by(ego, alter) %>%
  summarize(count = n(), alterProb = unique(alterProb)) %>% ungroup() %>%
  group_by(ego) %>%
  mutate(csum = cumsum(alterProb)) %>%
  arrange(ego, csum) %>%
  mutate(totego = sum(count),
         empprob = count / totego) -> cumprobs
cumprobs$ego <- factor(paste0("V",cumprobs$ego+1), levels = paste0("V", 1:16))
cumprobs$alter <- factor(paste0("V",cumprobs$alter+1), levels = paste0("V", 1:16))

# is the tie that's added new, removed, or does the network stay the same

changes <- cumprobs %>% dplyr::select(ego, alter) %>% unique
wave1friends <- fortify(as.adjmat(fd2.w1))
wave1friends2 <- wave1friends[,-3]
wave1friends2$type <- "remove"
changes2 <- left_join(changes, wave1friends2, c("ego" = "from", "alter" = "to"))

changes2$type[is.na(changes2$type)] <- ifelse(changes2$ego[is.na(changes2$type)] == changes2$alter[is.na(changes2$type)], "noChange", "add")

cumprobs %>% left_join(changes2) -> changes3

changes3$ego <- factor(changes3$ego, levels = paste0("V", 1:16), labels = 1:16)
changes3$alter <- factor(changes3$alter, levels = paste0("V", 1:16), labels = 1:16)

ggplot(data = changes3) +
  geom_segment(aes(x = alter, xend = alter, y = 0, yend = empprob, color = type), size = 2, alpha = .75) +
  geom_point(
    aes(x = alter , y = alterProb), shape = 4, size = 2, alpha = .75) +
  labs(x = "alter node\n", y = "\nprobability of alter node") +
  coord_flip() +
  facet_wrap(~ego, labeller = "label_both") +
  ThemeNoNet

@
\begin{figure}
\begin{subfigure}[t]{\textwidth}
<<heatmap, fig.align='center', fig.height=2.5>>=

ansnullchains %>%
  filter(period == 1) %>%
  group_by(rep) %>%
  mutate(msno = row_number()) %>%
  filter(msno == 1) %>%
  mutate(ego = as.numeric(as.character(ego))+1,
         alter = as.numeric(as.character(alter))+1,
         prob = exp(logAlterProb)) -> probsDat

ansnullchains %>%
  filter(period == 1) %>%
  group_by(rep) %>%
  mutate(msno = row_number()) %>%
  filter(msno == 1) %>%
  ungroup() %>% group_by(ego, alter) %>% 
  tally() %>% ungroup() %>% mutate(empprob = n/1000) %>% 
  mutate(ego = as.numeric(as.character(ego))+1,
         alter = as.numeric(as.character(alter))+1) ->empprobsDat

p1 <- ggplot(data = probsDat,
       aes(y = factor(ego, levels = rev(order1)),
             x = factor(alter, levels = order1),
             fill = prob)) +
  geom_tile(color = 'black') +
  scale_fill_gradient(low = 'white', high = 'black', limits = c(0,1), name = "theoretical\nprobability", labels = c("0","",".5","","1")) +
  labs(y = "ego", x = "alter")+
  scale_x_discrete(position = 'top') +
  ThemeNoNet + 
  theme(panel.grid = element_blank(), 
          axis.text.x.top = element_text(size=rel(.7)),
          axis.text.y.left = element_text(size=rel(.7)),
          legend.position = "bottom", 
          legend.title = element_text(size=rel(.7)), 
          legend.text = element_text(size=rel(.7)), 
          legend.key.height = unit(.5, "cm"), 
          legend.margin=margin(0,0,0,0),
          legend.box.margin=margin(-10,-10,-5,-10)) +
  coord_fixed() + 
  ggtitle("Theoretical Probabilities") + 
  labs(x= "alter (j)", y = "ego (i)")
p2 <- ggplot(data = empprobsDat,
       aes(y = factor(ego, levels = rev(order1)),
             x = factor(alter, levels = order1),
             fill = empprob)) +
  geom_tile(color = 'black') +
  scale_fill_gradient(low = 'white', high = 'black', name = "empirical\nprobability", labels = c(".02", ".04", ".06"), breaks = c(.02, .04, .06)) +
  labs(y = "ego", x = "alter")+
  scale_x_discrete(position = 'top') +
  ThemeNoNet + 
    theme(panel.grid = element_blank(), 
          axis.text.x.top = element_text(size=rel(.7)),
          axis.text.y.left = element_text(size=rel(.7)),
          legend.position = "bottom", 
          legend.title = element_text(size=rel(.7)), 
          legend.text = element_text(size=rel(.7)), 
          legend.key.height = unit(.5, "cm"), 
          legend.margin=margin(0,0,0,0),
          legend.box.margin=margin(-10,-10,-5,-10)) +
  coord_fixed() + 
  ggtitle("Empirical Probabilities") + 
  labs(x= "alter (j)", y = "ego (i)")
plot_grid(p1, p2)
@
\caption{\label{fig:heatmap}Adjacency matrix visualizations showing the theoretical (left) and empirical (right) transition probabilities for the first microstep taken in 1,000 simulations. There are many ties with empirical probability zero.}
\end{subfigure}
\begin{subfigure}[t]{\textwidth}
<<alledgests, fig.height = 3>>=
# get a plot of all microsteps from w1 to w2 by edge
alledges <- expand.grid(from = paste0("V", 1:16), to = paste0("V", 1:16))
alledges$eid <- 1:nrow(alledges)

sfmsall %>% left_join(alledges) -> sfmsall2

sfmsall2 %>% group_by(rep) %>% mutate(isWave2 = (ms == max(ms))) -> sfmsall2

sfmsall2 %>% group_by(from, to, eid) %>%
  summarize(total =n()) %>%
  arrange(desc(total)) %>%
  ungroup %>%
  mutate(plotorder = row_number()) -> sfmsallorder

sfmsall2 %>% group_by(ms, eid) %>%
  summarise(count = n()) %>%
  mutate(alpha = count/1000) %>%
  arrange(ms, eid) -> sfmsall3

left_join(sfmsall3, sfmsallorder, by = "eid") -> sfmsall4

#add true wave 2 to plot
wave2 <- na.omit(actual2)
wave2$from <- paste0("V", wave2$from)
wave2$to <- paste0("V", wave2$to)
wave2 <- left_join(wave2, alledges)
wave2 <- left_join(wave2, sfmsallorder)

sfmsall5 <- sfmsall4 %>% ungroup %>% add_row(ms = max(sfmsall4$ms) + 1,
        eid = wave2$eid,
        count = NA,
        alpha = 1,
        from = wave2$from,
        to = wave2$to,
        total = wave2$total,
        plotorder = wave2$plotorder)
sfmsall5$alphaFctr <- cut(sfmsall5$alpha, c(0,.01, .05, .10, .25, .5, .75, 1, 1.1))
ggplot(data = sfmsall5) +
  geom_tile(aes(x = ms, y = reorder(eid,-plotorder), fill = alphaFctr)) +
  annotate(geom = "rect", xmin=.25, xmax = 1.75, ymin = -Inf, ymax = Inf, color = "red", fill = NA, size = .01, alpha = .15) + 
  #scale_fill_gradient(low = 'grey90', high = '#0868ac', name = "occurrence %") +
  scale_fill_brewer(palette = "Greys", name = "Occurrence %", labels = c("(0-1]%", "(1-5]%", "(5-10]%","(10-25]%", "(25-50]%", "(50-75]%", "(75-100]%")) +
  labs(x = "Microstep No.", y = "Edges (ordered by total # occurrences)") +
  ThemeNoNet +
  theme(axis.ticks.y = element_blank(), axis.text.y = element_blank(), panel.grid = element_blank())
@
\caption{\label{fig:alledgests}Visualizing all microsteps taken in 1,000 simulations from the model M1. All $16\times(16-1)=240$ possible edges are represented on the y-axis, which is ordered by how often edges occur in the simulation process. The occurrence percent is split up into groups to correspond with its distribution: only about 10\% of the possible edges appear more than 10\% of the time in the 1,000 simulations, while about 60\% appear less than 1\% of the time. The network $x(t_1)$ is shown at microstep 0, and the network $x(t_2)$ is shown as the last microstep for comparison. We see that it is rare for a microstep process to last longer than 150 steps.}
\end{subfigure}
\caption{\label{fig:probabilities}Visualizing the underlying algorithms when fitting CTMC models. The emprical probability plot (on the right in Figure~\ref{fig:heatmap}) is also shown at microstep number 1 in Figure~\ref{fig:alledgests}.}
\end{figure}

We also illuminate the microstep process by visualizing the theoretical and observed transition probabilites for the first microstep in the process. These probabilities are defined in Equation~\ref{eq:pij}. We only do the first step of many because each microstep is dependent on the previous network state, and thus only the first steps can be compared across simulations.  
There are 1,000 transitions to examine: the first microstep taken in each one of the simulations. %Another way to view these transition probabilities is through the adjacency matrix visualization. 
We use another seriated adjacency matrix visualization to show theoretical and empirical transition probabilities of the first microstep in Figure~\ref{fig:heatmap} for each tie that was the first change in a simulation. The figure is noticeably sparse: %showing the lack of coverage in the model.
of the $16\cdot 16 = 256$ possible choices for the CTMC to make, only 103, or about 43\%, are made in the 1,000 simulated chains. This leaves %reinforces what we saw in Figure~\ref{fig:histogramalters}, where there are many paths not taken. This effect could only be exacerbated as more steps are taken in the CTMC, leading to 
a large area of data space unexplored by the first step in the model fitting process. We present additional evidence for lack of coverage in the Supporting Information. The Figures~\ref{fig:heatmap} and \ref{fig:alledgests} show that the model M1 and the the corresponding fitting process do not explore the data space enough to adequately capture the network change mechanism, a finding we were only able to discern thanks to model visualization techniques. 

% removed 
<<allsteps, fig.cap="Two simulations (out of 1,000) of the microstep process from $x(t_1)$ to $x(t_2)$. The $x$ axis is the microstep number, with step 0 representing the first wave of data and the final step representing the second wave of data. We can see that many edges are underrepresented in this process: they are in the second wave, but never appear in the microsteps.", fig.height=2.5, eval = FALSE, purl=FALSE>>=
smallfriends <- read_csv("data/smallfriends4Geomnet.csv")
names(smallfriends)[1:2] <- c("from", "to")
ansnullchains %>%
  dplyr::filter(period == 1) %>%  #only look at chains from wave 1 to wave 2
  #dplyr::group_by(rep) %>%
  dplyr::select(rep, from = ego, to = alter) %>%
  dplyr::mutate(val = as.numeric(!from == to),
         from = paste0("V", parse_number(from)+1), # make the chains
         to = paste0("V", parse_number(to)+1)) -> ansnullchainsw1w2
msall1 <- netvizinf::listMicrosteps(dat = wave1friends,
                      microsteps = filter(ansnullchainsw1w2, rep == 1))
msall1_df <- plyr::rbind.fill(lapply(X = seq_along(msall1), FUN = function(i, x) {
        x[[i]]$ms <- i - 1
        return(x[[i]])
    }, msall1))
msall1_df <- msall1_df %>% dplyr::select(from, to, ms)
wave2 <- smallfriends %>% filter(Wave == 2) %>% na.omit %>% data.frame()
msall1_df <- msall1_df %>% add_row(from = paste0("V",wave2$from),
                      to = paste0("V",wave2$to),
                      ms = max(msall1_df$ms)+1)
edges <- msall1_df %>%
  group_by(from, to) %>%
  summarize(edgeappears = n()) %>%
  arrange(desc(edgeappears)) %>% ungroup() %>%
  mutate(eid = row_number())

msall1_df2 <- left_join(msall1_df, edges, by = c("from" = "from", "to" = "to"))

p1 <- ggplot(data = msall1_df2) +
  geom_tile(aes(x = ms, y = -eid)) +
  annotate("text", x = c(-1, 84, 40), y = c(-30, -30, -50), label = c("Wave 1", "Wave 2", "Microsteps"), angle = c(90, 90, 0)) +
  geom_vline(xintercept = c(0.5,81.5)) +
  #scale_fill_continuous(low = "red", high = "blue") +
  ThemeNoNet +
  theme(legend.position = 'none', axis.ticks.y = element_blank(),
        axis.text.y = element_blank(), axis.title = element_blank()) +
  labs(x = "Microstep")


# rep 2
msall2 <- netvizinf::listMicrosteps(dat = wave1friends,
                      microsteps = filter(ansnullchainsw1w2, rep == 2))
msall2_df <- plyr::rbind.fill(lapply(X = seq_along(msall2), FUN = function(i, x) {
        x[[i]]$ms <- i - 1
        return(x[[i]])
    }, msall2))
msall2_df <- msall2_df %>% dplyr::select(from, to, ms)
msall2_df <- msall2_df %>% add_row(from = paste0("V",wave2$from),
                      to = paste0("V",wave2$to),
                      ms = max(msall2_df$ms)+1)

edges2 <- msall2_df %>%
  group_by(from, to) %>%
  summarize(edgeappears = n()) %>%
  arrange(desc(edgeappears)) %>% ungroup() %>%
  mutate(eid = row_number())

msall2_df2 <- left_join(msall2_df, edges2, by = c("from" = "from", "to" = "to"))

p2 <- ggplot(data = msall2_df2) +
  geom_tile(aes(x = ms, y = -eid)) +
  annotate("text", x = c(-1, 51, 25), y = c(-25, -25, -40), label = c("Wave 1", "Wave 2", "Microsteps"), angle = c(90, 90, 0))+
  geom_vline(xintercept = c(0.5,49.5)) +
  #scale_fill_continuous(low = "red", high = "blue") +
  ThemeNoNet +
  theme(legend.position = 'none', axis.ticks.y = element_blank(),
        axis.text.y = element_blank(), axis.title = element_blank()) #+
# rep 3
# msall3 <- netvizinf::listMicrosteps(dat = wave1friends,
#                       microsteps = filter(ansnullchainsw1w2, rep == 3))
# msall3_df <- plyr::rbind.fill(lapply(X = seq_along(msall3), FUN = function(i, x) {
#         x[[i]]$ms <- i - 1
#         return(x[[i]])
#     }, msall3))
# msall3_df <- msall3_df %>% dplyr::select(from, to, ms)
# msall3_df <- msall3_df %>% add_row(from = paste0("V",wave2$from),
#                       to = paste0("V",wave2$to),
#                       ms = max(msall3_df$ms)+1)
#
# edges3 <- msall3_df %>%
#   group_by(from, to) %>%
#   summarize(edgeappears = n()) %>%
#   arrange(desc(edgeappears)) %>% ungroup() %>%
#   mutate(eid = row_number())
#
# msall3_df2 <- left_join(msall3_df, edges3, by = c("from" = "from", "to" = "to"))
#
# p3 <- ggplot(data = msall3_df2) +
#   geom_tile(aes(x = ms, y = -eid, fill = ms)) +
#   geom_vline(xintercept = c(0.5,106.5)) +
#   scale_fill_continuous(low = "red", high = "blue") +
#   ThemeNoNet +
#   theme(legend.position = 'none', axis.ticks.y = element_blank(),
#         axis.text.y = element_blank(), axis.title = element_blank()) #+
#   #labs(x = "Microstep", y = "Edges (most frequent at top)")

grid.arrange(p1, p2, nrow = 2)
@



\section{Discussion}\label{sec:discussion}

We have used several visualizations, some old and some new, to explore social network models new ways. The guiding principles of model-vis: viewing the model in the data space, visualizing collections of models, and looking at the underlying algorithms, gave us knowledge and appreciation for these complicated and flexible models. When we look at the model in the data space by viewing the expected network simulated from a model, we are able to easily see the lack of model fit. When visualizing collections of models, we uncover concerning relationships between parameters. When we explore the algorithms, we gain a better understanding of the model and its stochastic behavior. Our three-pronged model-vis attack has uncovered many properties of these models. 

We have only just begun to scratch the surface of these multi-layered models for social networks. The \texttt{RSiena} software is incredibly powerful, and we can fit much more flexible hierarchical models than we have examined here. If, for example, we think the network structure or an actor covariate affects the rate of change of the network, we can incorporate that belief into the rate function. In addition, more than one actor-level covariate can be included in the model, and many more than three parameters can be included in the objective function. As we have shown, the rate and objective function parameters are often highly correlated, so researchers can now use model-vis techniques to gain a better understanding of these correlations and hopefully find ways to account for their effects in the model. In addition, some extensions of the CTMC model can aslo model behavior change of the actors in the network, which we have just treated as covariates. The CTMC family of models for dynamic networks has immense potential for additional model visualization applications.

Furthermore, the data we have used are quite small by network data standards, with only 16 nodes. Thus, some of the visualizations we have presented may not scale up to larger networks. Future network model-vis research could also include interactive and/or three-dimensional graphics, which we did not explore here. Interactivity is the state-of-the-art in data visualization, and network visualization is just starting to catch on \citep[see e.g.\ ][]{refinery}. \citet{modelvis} also relied on interactive tools like GGobi \citep{STLBC03} for interactivity, and for networks, there is some interactive support with the package \texttt{plotly} \citep{plotly} built into \texttt{geomnet}. 

We hope that the principles of model-vis continue to be used for learning more about the fit and behavior of complicated statistical models. Our model-vis application has uncovered many properties of CTMC models for dynamic social networks, and potential abounds for application of model-vis in other areas of network analysis.   

\subsection*{Online supplemental material:} 

In addition to the Supporting Information available online from the publisher, much of the code used to create the visualizations in this paper has been packaged and made available online at \url{https://github.com/sctyner/netvizinf} and \url{https://github.com/sctyner/geomnet} (also on CRAN). Additional, non-packaged code can be found at \url{http://bit.ly/ctmc-model-viz}. 

\nocite{ggplot2}