\newcommand{\st}[1]{{\color{orange} #1}}
\newcommand{\hh}[1]{{\color{magenta} #1}}

<<setup, echo = FALSE, message = FALSE, warning = FALSE>>=
options(replace.assign=TRUE,width=70, digits=3)
require(knitr)
opts_chunk$set(fig.path='figure/', cache.path='cache/', fig.align='center', fig.pos='h', out.width='.99\\textwidth', par=TRUE, cache=FALSE, concordance=TRUE, autodep=TRUE, message=F, warning=F, echo = FALSE, dev="cairo_pdf", fig.width = 6, fig.height = 6)#, root.dir = "~/Desktop/Dissertation/SAOM-removing-blindfold/")
@

<<pkgs>>=
# if (packageVersion("ggplot2") > "2.2.0"){
#   remove.packages("ggplot2")
#   install.packages("ggplot2")
# }
library(ggplot2)
library(tidyverse)
library(RSiena)
library(network)
library(sna)
library(geomnet)
library(GGally)
# devtools::install_github("sctyner/netvizinf")
library(netvizinf)
library(RColorBrewer)
library(gridExtra)
library(cowplot)
library(extrafont)
loadfonts(quiet = T)

ThemeNoNet <- theme_bw() %+replace%
            theme(plot.title = element_text(size = 10,
                                            face = 'plain',
                                            angle = 0,
                                            family="Times New Roman"),
                  axis.title.x = element_text(size = 10,
                                            face = 'plain',
                                            angle = 0,
                                            family="Times New Roman"),
                  axis.title.y = element_text(size = 10,
                                            face = 'plain',
                                            angle = 90,
                                            family="Times New Roman"),
                  axis.text.x = element_text(size = 10,
                                            face = 'plain',
                                            angle = 0,
                                            family="Times New Roman"),
                  # axis.text.x.top = element_text(size = 10,
                  #                           face = 'plain',
                  #                           angle = 0,
                  #                           family="Times New Roman"),
                  # axis.text.x.bottom = element_text(size = 10,
                  #                           face = 'plain',
                  #                           angle = 0,
                  #                           family="Times New Roman"),
                  axis.text.y = element_text(size = 10,
                                            face = 'plain',
                                            angle = 0,
                                            family="Times New Roman"),
                  # axis.text.y.left = element_text(size = 10,
                  #                           face = 'plain',
                  #                           angle = 0,
                  #                           family="Times New Roman"),
                  # axis.text.y.right = element_text(size = 10,
                  #                           face = 'plain',
                  #                           angle = 0,
                  #                           family="Times New Roman"),
                  strip.text.x = element_text(size = 10,
                                            face = 'plain',
                                            angle = 0,
                                            family="Times New Roman",
                                            margin = margin(t = 3, r = 0, b = 3, l = 0, unit = "pt")),
                  strip.text.y =element_text(size = 10,
                                            face = 'plain',
                                            angle = 90,
                                            family="Times New Roman",
                                            margin = margin(t = 3, r = 3, b = 3, l = 3, unit = "pt")),
                  strip.background = element_rect(colour = "black", fill = "white")
                    )
ThemeNet <- theme_net() %+replace%
            theme(plot.title = element_text(size = 10,
                                            face = 'plain', angle = 0, family = "Times New Roman"),
                  strip.text.x = element_text(size = 10,
                                            face = 'plain', angle = 0, family = "Times New Roman",
                                            margin = margin(t = 3, r = 0, b = 3, l = 0, unit = "pt")),
                  strip.text.y =element_text(size = 10,
                                            face = 'plain', angle = 0, family = "Times New Roman",margin = margin(t = 3, r = 0, b = 3, l = 0, unit = "pt")),
                  strip.background = element_rect(colour = "black", fill = "white"))
@

<<getdata , echo=FALSE>>=
set.seed(823746)
load("data/ansnullpaper.rda")
ansnullchains <- get_chain_info(ansnull)
load("data/M1sims1000.RData")
load("data/M2sims1000.RData")
load("data/M3sims1000.RData")
@

\section{Introduction \& Background}
% peices of old intro
%Social networks, such as collaboration networks between academic researchers or friendship networks 
% new intro 
%As humans and researchers, it is our instint to try to learn as much as we can about the world around us. 
As members of the human family, one of the hardest things we try to do is understand our relationships with one another, whether as friends, family, or colleagues. These relationships, or \emph{social networks}, have been studied for decades, in every format from self-help books to publications in academic journals. There are also statistical models, such as exponential random graph models (ERGM) and latent space models (LSM), that strive to capture the underlying relationship-generating mechanism \citep{ergm, lsm}. These static models, however, are not optimal for modeling ``real-world" social networks because they do not account for changes in the network as time passes. Models for \textit{dynamic social networks} are more realistic because they provide mechanisms for analyzing multiple observations of the same network over time. %., and models for dynamic social networks hold a great deal of promise because of their potential to capture realistic mechanisms. %Our social networks evolve over time as new relationships are made, and being able to model these evolutionary mechanisms can uncover hidden truths about our relationships with each other. 

\par One set of models for dynamic social networks are stochastic actor-oriented models (SAOMs), introduced in \citet{saompaper}. These models incorporate node-level covariate effects in addition to the traditional structural effects, such as in-degree or out-degree, common in ERGMs.
%These models are different from other dynamic network models because they allow us to add node level information into the model with the traditional network structure information. Adding node-level information into the model is a more intuitive approach to model changes in a social network because, for example 
With these models, we can examine assumptions made about social networks, i.e. that people with common interests are more likely to become friends. For example, we may assume that teenagers who drink heavily are more likely to be friends with other tennagers who drink heavily. The SAOM family allows us to incorporate this behavioral information on shared interests into the modeling process.

\par Unlike other network models, SAOMs are not analytically tractable \citep{saomreview}. Likelihood functions quickly become very complex due to the inherent dependency structure in the data, and therefore computational methods, such as Markov chain Monte Carlo (MCMC), are used to fit models. %SAOMs are typically fit to dynamic social network data using a series of MCMC phases for finding method of moments (MoM) estimates of model parameters. 
For SAOM parameter estimation, we use the software SIENA, and its \texttt{R} implementation \texttt{RSiena} by \citet{RSiena}. This software is a considerable contribution to the field of social network analysis, but the extensive parameter estimation process is largely hidden from the user. To better understand SAOMs and the fitting process in \texttt{RSiena}, we use model visualization, or model-vis \citep{modelvis}, to examine the underlying methods, hidden structures, and simulation results. Ultimately, we aim to help researchers working with SAOMs better understand the structure, behavior, fit, and results of these models. 

In the remainder of this section, we introduce the structure of network data and two different network visualization (net-vis) techniques. Then, we carefully define the many moving pieces that make up a SAOM. Finally, we briefly ouline the SAOM fitting process that we use throughout this paper. 

%\par To learn more about the SAOM fitting process, we use model visualization techniques (see \citet{modelvis}) to display the model in the data space, view collections of models instead of single models, and visually explore the algorithms fitting the SAOMs. SAOMs are a model family that can reap many benefits from the application of model visualization techniques. For instance, the models themselves include a continuous-time Markov chain (CTMC) that is completely hidden from the analyst in the model fitting process. Visualizing the CTMC can provide researchers with more insight into the underlying features of the model. In addition, SAOMs can include a great deal of parameters in the model structure, each of which is attached to a network statistic. These statistics are often somewhat, if not highly, correlated, which causes high correlation between the corresponding parameters in a SAOM. By visualizing collections of SAOMs, we gain a better understanding of these correlations and can find ways to account for their effects in the model. Furthermore, the estimation of the parameters in a SAOM rely on convergence checks based on simulations from the fitted model. These simulations are hidden and unsaved by default for computational efficiency. With the help of both static and dynamic visualizations, we bring the hidden model fitting processes into the foreground, eventually leading to a better understanding and higher accessibility of stochastic actor-oriented models for social network analysts.

%The remainder of this paper is organized as follows: In Section~\ref{sec:netintro}, we introduce basic concepts of networks and network visualizations. In Section~\ref{sec:saoms}, we formally define the family of stochastic actor-oriented models for social network analysis. In Section~\ref{sec:ModelVis}, we combine concepts from Sections~\ref{sec:netintro} and \ref{sec:saoms} in an application of the model-vis paradigm, and conclude with a discussion in Section~\ref{sec:discussion}.

\subsection{Networks and their Visualizations} \label{sec:netintro}

%We provide a brief introduction to the structure of network data and common network visualization methods. \\

%\noindent \textbf{Network Data Structure:} 
Network data across fields have similar structure. There are always units of observation and connections between those units. In a social network, the units of observations, called \textit{nodes} or \textit{actors}, are people, while the connections, called \textit{edges} or \textit{ties}, are their relationships with each other. %Networks change over time, like when new relationships are formed in a social network. 
Other network data include inherent variables of interest on the nodes and edges themselves, e.g.\ someone's age, gender, and political affiliation, and the type of relationship (friendship, coworkers, etc) that an edge represents.
%These two network data structures pose unique problems to analysts, such as, ``How does the strength of a tie between two nodes affect the overall structure of the network?" or ``Do differences in node variables affect the formation of edges?" We propose to answer these questions and more through visual exploration of network data and models. \\


\noindent \textbf{Visualizing Network Data:} Network visualization, also called graph drawing, is a prominent subfield of network analysis. For a review of graph drawing, see \citet{graphdrawing}. Visualizing network data is inherently challenging due to the structure of the data itself.  Most data visualizations rely on well-defined axes inherited from the data, such as Cartesian coordinates or spatial locations, but network data typically do not have such built-in structure. 

<<toyex, echo = FALSE>>=
set.seed(81393)
nodes <- data.frame(id = 1:5, group = sample(LETTERS[1:2], size = 5, replace = T), stringsAsFactors = F)
g <- igraph::as_data_frame(igraph::random.graph.game(n = 5, p = .4))
toynet <- list(nodes = nodes, edges = rbind(g, c(5,4)))
@
<<toyplots, fig.show='hold', fig.cap="On the left, a node-link diagram of our toy network, with nodes placed using the Kamada-Kawai algorithm. On the right, the corresponding adjacency matrix visualization.", out.width='.49\\textwidth', fig.height=4>>=
toynetdat <- fortify(as.edgedf(toynet$edges), toynet$nodes)
ggplot(data = toynetdat) +
  geom_net(aes(from_id=from_id, to_id=to_id), labelon = T, size = 11, labelcolour = 'white', ecolour = 'black', vjust = .5, hjust = .5, fontsize = 9, arrowgap = .05, directed = T) +
  ThemeNet +
  theme(plot.title = element_text(size = 20)) +
  labs(title = "Node-Link Diagram")
#ggsave("figure/toynetnld.pdf",plot = nld, width = 4, height = 4)

ggplot(data = toynet$edges) +
  #geom_tile(aes(x = from, y = -to), color = 'grey20', fill = 'black') +
  geom_tile(aes(x = to, y = -from), color = 'grey20', fill = 'black') +
  geom_segment(aes(x = x, xend = xend), y = -0.5, yend=-5.5, colour="grey30", data = data.frame(x = 0:5+.5, xend= 0:5+.5)) +
  geom_segment(aes(y=-y, yend=-yend), x = 0.5, xend=5.5, colour="grey30", data = data.frame(y = -0:5+.5, yend= -0:5+.5)) +
  scale_x_continuous(breaks = 1:5, labels = 1:5, limits = c(0.5,5.5), position = "top") +
  scale_y_continuous(breaks = (-5):(-1), labels = 5:1, limits = c(-5.5,-.5)) +
  ThemeNoNet +
  theme(panel.grid = element_blank(), aspect.ratio = 1,
        plot.title = element_text(size = 20),
        axis.title.x = element_text(size = 18),
        axis.title.y = element_text(size = 18),
        axis.text.x  = element_text(size = 18),
        axis.text.y  = element_text(size = 18)) +
  labs(title = "Adjacency Matrix Visualization", x = "\nto (alter)", y = "from (ego)\n")
@
%\hh{XXX Explain each of these visualizations and either comment on the differences in the visualizations (colors) or don't show them. }

The lack of a native coordinate system has given rise two very different primary net-vis methods: the node-link diagram and the adjacency matrix visualization \citep{knuth2000, adjmatpro}. We demonstrate these with a simpe example. Consider the network with five nodes, $\{1,2,3,4,5\}$, connected by five directed edges:  $\{2 \to 4, 3 \to 4, 1 \to 5, 3 \to 5, 5 \to 4\}$ shown in Figure~\ref{fig:toyplots}.

The first net-vis method, the node-link diagram, represents nodes with points in two dimensions and then represents edges by connecting the points with lines. These lines can also have arrows on them indicating the direction of the edge for directed networks, as show in Figure~\ref{fig:toyplots}. Because there is often no natural placement of the nodes, they are placed in 2D at random, then adjusted with one of the many layout algorithms available \citep{drawingalgs}. Some commonly used algorithms, such as the Kamada-Kawai (KK) layout \citep{kamadakawai} and the Fruchterman-Reingold (FR) layout \citep{fruchterman-reingold}, are designed to mimic physical systems, drawing the graphs based on the ``forces" connecting them. %Other algorithms use multi-dimensional scaling or properties of the adjacency matrix (e.g. its eigenstructure), to place the nodes \citep{drawingalgs}. 
In Figure~\ref{fig:toyplots}, we use the KK algorithm, and unless otherwise stated, all other node-link diagrams in this paper were drawn with the KK layout algorithm. 

\par The second method for net-vis uses the adjacency matrix of the network. The adjaceny matrix of a network, $\mathbf{A}$ is defined as:
\begin{equation}\label{eq:adjmat}
  A_{ij} =
  \begin{cases}
                                   1 &  \text{if an edge exists } i\to j, \quad i \neq j \\
                                   0 & \text{otherwise}
  \end{cases}
\end{equation}
%In this paper, we only consider the presence or absence of an edge, but if the network has weighted edges, $A_{ij}$ is the weight of the edge from $i \to j$. 
The adjacency matrix visualization for our toy example is also shown in Figure~\ref{fig:toyplots}. This visualization shows the network as a grid, with each row representing the ``from" node and each column representing the ``to" node of an edge. The grid space for edge $i\rightarrow j$ is filled in if $A_{ij} = 1$ and is empty if $A_{ij} = 0$. For undirected networks, $\mathbf{A}$ and the grid visualization are symmetric, with each edge represented twice. 

\par Each net-vis method has its own advantages and disadvantages. For instance, \emph{paths} between two nodes in a network are easier to determine with node-link diagrams than with adjacency matrix visualizations \citep{adjmatviz}. In node-link diagrams, node information can be incorporated into the visualization by coloring or changing the shape of the points, and edge-level information can be incorporated by coloring the lines, or changing their thickness. Visualizing a node variable in an adjacency matrix visualization is not as simple, because this type of net-vis features the edges. An adjacency matrix visualization can, however, be useful when the network is very complex, dense, or large \citep{adjmatviz}. % Experimental studies have shown adjacency matrix visualization to be superior to node-link diagrams for large networks. For example, for basic perceptual tasks on networks, including node and edge count, adjacency matrix visualizations outperform node-link diagrams as the size and density of the network increases \citep{adjmatviz}. One drawback of the adjacency matrix visualization that \citeauthor{adjmatviz} found was that edges are overrepresented for undirected graphs, due to the symmetry of $\mathbf{A}$: the edge $i \rightarrow j$ for $i \neq j$ appears in $\mathbf{A}$ twice: in $A_{ij}$ and $A_{ji}$, and so it also appears twice in the adjacency matrix visualization. This is an advantage, however, for \emph{directed} graphs, where exactly the correct number of edges is represented in a matrix visualization. A node-link diagram, however, may underrepresent the edge count if the edges $x_{ij}$ and $x_{ji}$ both exist and are drawn on top of one another. 
Ultimately, there is no one ``correct" way to visualize network information, and we will be using both the node-link and adjacency matrix net-vis methods throughout this paper to explore our example data and SAOMs.

\subsection{Defining SAOMs} \label{sec:saoms}

Stochastic Actor-Oriented Models (SAOMs) model \textit{dynamic} networks, so we can use SAOMs to study the change of a network in time while also examining the structural and node covariate efects. The two titular properties of SAOMs, stochasticity and actor-orientation, are key to understanding social networks as they exist naturally. Most social networks, even holding constant the set of actors over time, are ever-changing as relationships form or disappear. Actors in social networks also have inherent properties that could affect how they change their role within the network, and existing ties may affect formation or dissolution of other ties.

\subsubsection{Definitions, Terminology, and Notation}

A \textit{dynamic network} is consists of a fixed set of $n$ nodes, that is changing over time, and is observed at $M$ discrete time points, $t_1, \dots, t_M$ with $t_1 < t_2 < \dots < t_M$. We denote the network observation at timepoint $t_m$ by $x(t_m)$ for $m = 1, \dots M$. The SAOM assumes that this longitudinal network of discrete observations is embedded within a continuous time Markov chain, (CTMC), which we denote by $X(T)$. The CTMC is almost entirely unobserved: we assume the first and last network observations are the starting and ending points of the CTMC, i.e. $x(t_1) \equiv X(0)$ and $x(t_M) \equiv X(\infty)$. Nearly all other steps in the chain are unobserved, with the exception of $x(t_2), \dots, x(t_{M-1})$. Unlike the first and last observations of the network, these observations do not have direct correspondence with steps in the continuous time Markov chain. Thus, the observations $x(t_2), \dots, x(t_{M-1})$ are considered to be ``snapshots" of the network at some point between two steps in the CTMC. 

The CTMC process $X(T)$ is a series of single tie changes that happen according to the SAOM's \textit{rate function}, where one actor at a time is ``selected" to make a change. Once selected, the actor changes its ties in order to maximize its \textit{objective function} based on the current state and possible future states of the network. 

\noindent \textbf{The Rate Function} In general, a SAOM's \emph{rate function}, $\rho(x, \mathbf{z}, \boldsymbol{\alpha})$, is a function of the network at its current state $x$, covariates of interest, $\mathbf{z}$, and some parameters,   $\boldsymbol{\alpha}$. With this general formulation, each node can have a different rate of change. Throughout this paper, we assume a simple rate function, $\rho(x, \mathbf{z}, \boldsymbol{\alpha}) \equiv \alpha_m$ that is constant across all actors between time $t_m$ and $t_{m+1}$. With this simple rate function, we have $M-1$ rate parameters, one for each transition from $x(t_{m})$ to $x(t_{m+1})$ for $m = 1, \dots, M-1$. The rate parameter, $\alpha_m$ dictates how often an actor $i$ ``gets an opportunity" to change one of its ties, $x_{ij}$, for $j \in \{1, \dots, n\}$ in the time period from $t_{m}$ to $t_{m+1}$ for $m = 1, \dots, M-1$. A SAOM also assumes that the actors $i$ are conditionally independent given their ties, $x_{i1}, \dots, x_{in}$ at the current network state. Let $\tau(i|x,m) $ be the wait time until actor $i$ gets to the opportunity to change its ties. For any time point $T$, where $t_{m} \leq T < t_{m+1}$, the waiting time to actor $i$'s next change follows an exponential distribution:
\begin{equation} \label{eq:c1}
 \tau(i|x,m) | x_{i1}(m), \dots, x_{in}(m) \stackrel{\text{iid}}{\sim} Exp(\alpha_m)
\end{equation}
\noindent with expected value $\alpha_m^{-1}$. From Equation~\ref{eq:c1}, we can derive the waiting time to the next change opportunity by \textit{any} actor in the network, $\tau(x)$. The value $\tau(x)$ is also exponentially distributed, and has expected value $(n\alpha_m)^{-1}$. The estimation of $\alpha_m$ in \texttt{RSiena} is simple: method of moments (MoM) is used with the statistic 
\begin{equation}\label{eq:cdiff}
C_m = \sum_i\sum_j |x_{ij}(t_{m+1}) - x_{ij}(t_m)|
\end{equation}
which is the total number of changes from $x(t_m)$ to $x(t_{m+1})$. 

\noindent \textbf{The Objective Function} Because of the conditional independence assumptions given in Equation~\ref{eq:c1}, the objective function is written separately for each node. Suppose node $i$ is selected to change at the next step in the CTMC. This node, called the \textit{ego} node, has the potential to interact with all other nodes in the network, $j \neq i$. These nodes $j$, are referred to as \textit{alter} nodes, and will not change any of their ties at the next step in the CTMC. The ego node $i$ in the current network state $x$, wants to make a change that maximizes its \emph{objective function}:
\begin{equation} \label{eq:of1}
f_i(\boldsymbol{\beta}, x, \mathbf{Z}) = \sum_{k=1}^K \beta_k s_{ik}(x, \mathbf{Z}),
\end{equation}
where $x$ is the current network state, $\mathbf{Z}$ are node covariates, and $K$ is the total number of parameters in the objective function. The parameters of the model, $\beta_1, \dots, \beta_K$ correspond to network statistics, $s_{ik}(x, \mathbf{Z})$, for $k = 1,\dots, K$. To maximize $f_i$, $i$ will either destroy a tie where $x_{ij} = 1$, create a tie where $x_{ij}=0$, or make no change if all changes will result in lower $f_i$. 

The parameters of $f_i(\boldsymbol{\beta}, x, \mathbf{Z})$ are either structural or covariate parameters. The structural parameters correspond to statistics that are \emph{only} functions of the current network state, $x$, while the covariate parameters are also functions of the covariates, $\mathbf{Z}$. According to \citet[p. 371]{snijders01}, there should be at least two parameters included in the model: $\beta_1$ for the outdegree of nodes, and $\beta_2$ for the reciprocity of nodes. The former measures the propensity of nodes with a lot of outgoing ties to form more outgoing ties (the ``rich get richer" effect), while the latter measures the tendency of outgoing ties to be returned within a network. The statistics corresponding to these two parameters, $s_{i1}(x)$ and $s_{i2}(x)$ are given in Table~\ref{tab:effects}. In the \texttt{RSiena} software, there are over 80 possible $\boldsymbol{\beta}$ parameters with corresponding statistics $s_{ik}(x, \mathbf{Z})$ to add to the model. The definition of the statistics for all possible parameters are provided in \citet{RSienamanual}, and two additional parameters and their corresponding statistics are discussed in Section~\ref{sec:exdatamods} and shown in Figure~\ref{fig:structures}.

\begin{table}
\centering
\begin{tabular}{m{4.5cm}m{4.5cm}m{3cm}}
\multicolumn{3}{l}{\textbf{Structural Effects}} \\
Name & Statistic & Figure \\
\hline\hline
outdegree  & $s_{i1}(x) = \sum_j x_{ij}$ & \includegraphics[width=2cm]{img/outdegree}\\ \hline
reciprocity  & $s_{i2}(x) = \sum_j x_{ij}x_{ji}$ & \includegraphics[width=2cm]{img/reciprocity}\\ 
%\hline
%transitive triplets  & $s_{i3}(x) = \sum_{j,h} x_{ij}x_{jh}x_{ih}$ & \includegraphics[width=1.5cm]{img/transtrip}  \\
%\\
% \multicolumn{3}{l}{\textbf{Covariate Effects}}\\  
% Name & Statistic & Figure \\
% \hline \hline
% covariate-alter  & $s_{i4}(x) = \sum_j x_{ij}z_j$ & \includegraphics[width=3cm]{img/covaralter} \\  \hline
% covariate-ego  & $s_{i5}(x) = z_i\sum_j x_{ij}$ & \includegraphics[width=3cm]{img/covarego} \\ % \hline
%same covariate & $s_{i6}(x) = \sum_j x_{ij} \mathbb{I}(z_i = z_j)$ & \includegraphics[width=3cm]{img/covarsim}%\\
%jumping transitive triplets  & $s_{i7}(x) = \sum_{j \neq h} x_{ij}x_{ih}x_{hj} \mathbb{I}(z_i = z_h \neq z_j)$
\end{tabular}
\caption{\label{tab:effects} Examples of structural effects included in a SAOM. The darker nodes in the Figure column are the ego nodes ($i$), and all others are alters ($j$). The dotted lines are ties the ego node is en(dis)couraged to make when the parameter corresponding to each effect is positive (negative).}
\end{table}

In the CTMC, when actor $i$ can make a change, it chooses which tie $x_{i1}, \dots, x_{in}$ to change at random according to the probabilities $p_{ij}(\boldsymbol{\beta}, x, \mathbf{Z})$. These probabilities are defined via the objective function. Let $x(i \leadsto j)$ be the network identical to network $x$ with the exception of tie $x_{ij}$, which is equal to $1-x_{ij}$; i.e.\ the presence or absence of node $x_{ij}$ is flipped in the new network. The probability that the tie $x_{ij}$ changes is defined as:
\begin{equation}\label{eq:pij}
p_{ij}(\boldsymbol{\beta}, x, \mathbf{Z}) = \dfrac{\exp\left\{f_i(\boldsymbol{\beta}, x(i\leadsto j), \mathbf{Z})\right\}}{\sum_{h \neq i} \exp\left\{f_i(\boldsymbol{\beta}, x(i \leadsto h), \mathbf{Z})\right\}}
\end{equation}%When $i = j$, the numerator represents the exponential of the value of the objective function when evaluated at the current network state. 
%When the value of the objective function is high at the current state, the probability of not making a change is also high.
We explore these probabilities further in Section~\ref{sec:algorithms}. The probabilities $p_{ij}$ combined with the rate parameters $\alpha_m$ fully characterize the underlying CTMC that models network change. For more details on CTMCs, see e.g. \citet{ctmc}. 
% \subsubsection{Continuous Time Markov Chain (CTMC)}
% \st{
% In the continuous-time Markov chain literature, see for instance \citet{ctmc}, chains are characterized by their \textit{generator} or \textit{intensity} matrix $\mathbf{Q}$. This matrix describes the rate of change between two states of the CTMC process, and the rows of this matrix always add to zero. For directed networks with binary edge variables like the ones we will be working with, there are a very large number of possible states for a directed network with $n$ nodes. We denote the state space as $\mathcal{X}$, a set which contains $2^{n(n-1)}$ states: there are two possible states for an edge, $\{0,1\}$, and there are $n(n-1)$ edge relationships because the network is directed and we exclude self-ties.  The intensity matrix for a CTMC in a SAOM is then a square matrix of dimension $2^{n(n-1)} \times 2^{n(n-1)}$. Only one tie changes at a time in the CTMC, resulting in $n(n-1)$ reachable states from the current network state. Thus, the intensity matrix $\mathbf{Q}$ is very sparse, with only $n(n-1) + 1$ non-zero entries in each row. Note that $n(n-1)$ of these entries represent the possible states that are one edge different from a given state, while the additional non-zero entry is for the state to remain unchanged. All other entries in a row are structural zeroes because those network states cannot be reached from the current state in a single change.
% 
% The two pieces of a SAOM, the rate function/parameter and the objective function, each contribute to the entries of the intensity matrix to describe the rate of change between two network states. The entries of $\mathbf{Q}$ are defined as follows: let $b \neq c \in \{1, 2, \dots, 2^{n(n-1)} \}$ be indices of two different possible states of the network, $x^b, x^c \in \mathcal{X}$.  Then the $bc^{th}$ entry of $Q$ is:
% \[ q_{bc} = \begin{cases}
%      q_{ij}= \alpha_m p_{ij}(\boldsymbol{\beta}, x^b) & \text{if } x^c \in \{x^b(i \leadsto j) | \text{ any } i \neq j \in \{1, \dots, n\}\} \\
%       0 & \text{if } \sum_i \sum_j |x_{ij}^c -x_{ij}^b | > 1 \\
%       -\sum_{i\neq j} q_{ij} & \text{if } x^b = x^c
%    \end{cases}
% \]
% 
% Thus, the rate of change between any two states, $x^b$ and $x^c$, that differ by only one tie $x_{ij}$, is the product of the rate at which actor $i$ gets to change a tie and the probability that the tie that will change is the tie to node $j$. This matrix $\mathbf{Q}$ is the foundation for estimation of a SAOM. %Estimating the parameters in these models is difficult, but thanks to the SIENA software \citep{RSiena}, we have an accessible way to fit SAOMs to network observations.
% 
% %XXX HH can you help? I need a way better transition than that XXX
% }

%### A SAOM as a CTMC {#saomctmc}

%In order to fit this model definition back into the original context of the CTMC described in Section \@ref(dynamicnets), it must be written in terms of its intensity matrix, $\mathbf{Q}$.  This matrix describes the rate of change between states of the process. For networks, there are a very large number of possible states, $2^{n(n-1)}$, so the intensity matrix is a square matrix of that dimension. But, thanks to the property of SAOMs that the states are allowed to change only one tie at a time, there are only $n$ possible states given the current state, $n-1$ of which are uniquely determined by the node $i$ that is given the opportunity to change. Thus, the intensity matrix $\mathbf{Q}$ is very sparse, with only $n(n-1) + 1$ non-zero entries in each row. Note that $n(n-1)$ of these represent the possible states that are one edge different from a given state, and the additional non-zero entry is for the state to remain the same. All other entries in a row are zero because those column states cannot be reached from the row state by just one change as dictated by the SAOM. The entries of $\mathbf{Q}$ are defined as follows: let $b \neq c \in \{1, 2, \dots, 2^{n(n-1)} \}$ be indices of two different possible states of the network, $x^b, x^c \in \mathcal{X}$.  Then the $bc^{th}$ entry of $Q$ is:
% \[ q(x^b, x^c) = \begin{cases}
%       q_{ij}(\alpha, \rho, \boldsymbol{\beta}, x^b) = \lambda_i(\alpha, \rho, x^b, m)p_{ij}(\boldsymbol{\beta}, x^b) & \text{if } x^c \in \{x^b(i \leadsto j) | \text{ any } i \neq j \in \mathcal{N}\} \\
%       0 & \text{if } x^c \text{ differs from } x^b \text{ by more than 1 tie} \\
%       -\sum_{i\neq j} q_{ij}(\alpha, \rho, \boldsymbol{\beta}, x^b)  & \text{if } x^b = x^c
%    \end{cases}
% \]
%
% Thus, the rate of change between any two states that differ by only one tie, $x_{ij}$, is the product of the rate at which actor $i$ gets to change a tie and the probability that the tie that will change is the tie to node $j$.^[Just to be clear, the change is from $x^b_{ij}$ to $x^c_{ij} = 1 - x^b_{ij}$.] Furthermore, the theory of continuous time Markov chains gives that the matrix of transition probabilities between observation times $t_{m-1}$ and $t_{m}$ is dependent only on the difference between timepoints, $t_m - t_{m-1}$. Following the same definition for transition probabilities in Section \@ref(dynamicnets), the matrix of transition probabilities is
% $$e^{(t_m - t_{m-1})\mathbf{Q}},$$
% where $\mathbf{Q}$ is the matrix defined above and $e^X$ for a real or complex square matrix $X$ is equal to $\sum_{k=0}^{\infty} \frac{1}{k!} X^k$.

\subsubsection{Fitting Models to Data}

To fit a SAOM to dynamic network data, we apply the \texttt{RSiena} software, which uses simulation methods to estimate parameter values using either MoM or maximum likelihood (ML) estimation \citep{RSiena}. We chose to use MoM estimation, so we describe that process here, while more information on ML estimation can be found in \citet{SienaAlgs}. %because the theory behind it was established in \citet{saompaper}, while the maximum likelihood estimation methods were not fully established until \citet{saomsml2010}, though \texttt{RSiena} contains capabilities to use maximum likelihood estimation. We also use the score function method for estimating the derivatives of the expected values, as opposed to the finite differences method, both of which are outlined in detail in .
The underlying SIENA software uses a Robbins-Monro algorithm (see \citet{robbinsmonro}) to estimate the solution of the moment equation
\begin{equation}\label{eq:mom}
E_{\theta}S = s_{obs}
\end{equation}
where $\theta = (\boldsymbol{\alpha}, \boldsymbol{\beta})$ is the vector of rate and objective function parameters, and $s_{obs}$ is the observed vector of the corresponding statistics summed over all network observations. The full SIENA MoM estimation algorithm operates in three phases, as described in \citet{RSienamanual} and \citet{SienaAlgs}, and we briefly summarize these phases here. The first phase obtains initial estimates of the score functions for use in the second phase. The second phase is the Robbins-Monro algorithm, and results in parameter estimates through iterative updates and simulation from the CTMC at the iterations of the parameter values. The third phase uses the parameters from phase two to estimate the score functions and covariance matrix of the parameter estimates and carries out convergence checks. In Section~\ref{sec:algorithms}, we further explore these phases in the SIENA MoM algorithm through visualization, bringing them out of their ``black box" and into the light.

The rest of the paper is strucured as follows. In Section \ref{sec:exdatamods}, we introduce the example data and models we use for model-vis. Then, in Section~\ref{sec:minds}, we explore the first step in model-vis by placing the models in the data space. Section~\ref{sec:collectmodels} follows, in which we explore collections of models. Finally, in Section~\ref{sec:algorithms}, we visualize the underlying algorithms in the SAOM fitting process. We conclude with a discussion in Section~\ref{sec:discussion}.

%\subsection{Model Goodness-of-Fit}\label{sec:modgof}

%The \texttt{RSiena} software that fits the models to data also includes a goodnes-of-fit function for examining model fit, \texttt{sienaGOF()}. This function ``assess[es] the fit of the model with respect to auxiliary statistics of networks" \citep[p.~53]{RSienamanual}. Examples of auxiliary statistics include the out- or indegree distribution on the nodes, with the option for users to input their own statistics to examine. The goodness-of-fit is evaluated as follows:
% \begin{enumerate}
% \item The auxiliary statistics are computed on the observed data and $N$ simulated observations from the fitted model. Typically, $N=1000$.
% \item The mean vector and covariance matrix of the statistics on the simulations from the model are computed.
% \item The Mahalanobis distance from the observed statistics to the distribution of the simulated statistics is computed using the mean and covariance found in step 2.
% \item The Mahalanobis distance from each of the $N$ simulations to the same distribution is computed, and the Mahalanobis distance of the observed data is compared to this distribution of distances.
% \item An empirical $p$-value is found by computing the proportion of simulated distances found in step 4 that are as large or larger than the Mahalanobis distance from the data. A SAOM is thus considered a good fit if $p$ is large.
% \end{enumerate}

% <<sienagof, fig.cap="Goodness of fit measures for two network statistics computed on the first simulated wave of a SAOM: indegree and outdegree cumulative distributions. On the x-axis is the degree value, and on the y-axis is the number of times that degree value occurs in the network. \\st{The p-value shown on the x-axis is the proportion of observations simulated from the model that have observed statistic values as large or larger than the values observed in the data.}", cache = TRUE, message = FALSE, warning = FALSE, fig.height=5>>=
% load("data/ansnullpaper.rda")
% library(lattice)
% gof1 <- sienaGOF(ansnull, OutdegreeDistribution, varName ="friendship", join = FALSE)
% gof2 <- sienaGOF(ansnull, IndegreeDistribution, varName ="friendship", join = FALSE)
%
% sims1 <- data.frame(gof1$`Period 1`$Simulations)
% dat1 <- gof1$`Period 1`$Observations
% dat1 %>% data.frame() %>% gather(outdegree, val) %>%
%   mutate(outdegree = parse_number(outdegree)-1) -> dat1
% p1 <- sims1 %>%
%   mutate(sim = row_number()) %>%
%   gather(outdegree, val, X1:X9) %>%
%   mutate(outdegree = parse_number(outdegree) - 1) %>%
%   filter(outdegree <= 6) %>%
%   ggplot(aes(x = outdegree, y = val)) +
%   geom_boxplot(aes(group = outdegree), size = .5, outlier.shape = "x", outlier.size = 5) +
%   geom_violin(aes(group = outdegree), bw = 2, fill = NA) +
%   geom_point(data = dat1, color = 'red') +
%   geom_line(data= dat1, color = 'red') +
%   geom_text(data = dat1, aes(label = val), hjust = -.5) +
%   labs(x = "Outdegree (p = 0.154)", y = "Statistic",
%        title = "Goodness-of-Fit: Outdegree distribution period 1") +
%   ThemeNoNet
%
% sims2 <- data.frame(gof2$`Period 1`$Simulations)
% dat2 <- gof2$`Period 1`$Observations
% dat2 %>% data.frame() %>% gather(indegree, val) %>%
%   mutate(indegree = parse_number(indegree)-1) -> dat2
% p2 <- sims2 %>%
%   mutate(sim = row_number()) %>%
%   gather(indegree, val, X1:X9) %>%
%   mutate(indegree = parse_number(indegree) - 1) %>%
%   filter(indegree <= 6) %>%
%   ggplot(aes(x = indegree, y = val)) +
%   geom_boxplot(aes(group = indegree), outlier.shape = "x", outlier.size = 5) +
%   geom_violin(aes(group = indegree), bw = 2, fill = NA) +
%   geom_point(data = dat2, color = 'red') +
%   geom_line(data= dat2, color = 'red') +
%   geom_text(data = dat2, aes(label = val), hjust = -.5) +
%   labs(x = "Indegree (p = 0.063)", y = "Statistic",
%        title = "Goodness-of-Fit: Indegree distribution period 1") +
%   ThemeNoNet
%
%
% gridExtra::grid.arrange(p1, p2)
% # sum(gof1$`Period 1`$SimulatedTestStat >= gof1$`Period 1`$ObservedTestStat)/1000
% # [1] 0.154
% # > sum(gof2$`Period 1`$SimulatedTestStat >= gof2$`Period 1`$ObservedTestStat)/1000
% # [1] 0.063
%
% @

%The \texttt{plot.sienaGOF()} function allows us to visualize this fit. %In Figure~\ref{fig:sienagof}, we provide an example of what the goodness-of-fit plot output looks like for indegree and outdegree statistics for a small model.
%This function draws a box plot and a violin plot at each value of the statistic of interest observed in the simulations: \st{on the $x$-axis, the out(in)degrees observed in the data ($0, 1, 2, 3, \dots$), and on the $y$-axis, the cumulative number of times that out(in)degree value appears in the simulations and the data.} In order to compare the distribution of the counts of nodes with the specified degree as calculated on the simulated networks to the counts observed in the true data, red points connected by red lines representing the observed values are superimposed on the boxplots. If the red points lie "well within" the simulated values, the model is a good fit to the data. \st{This plot is not shown because it is not intuitive for understanding network data. Boxplots separated by the many outdegree values observed do not communicate \textit{how} the nodes are connected, just that some have more connections than others. In order to understand the fit of the model, we should try to understand how well the model captures the\textit{overall} structure of the network, not just one or two summary measurements of that structure.} %The model examined in Figure~\ref{fig:sienagof} appears to do a better job of capturing the outdegree distribution than the indegree distribution of the data.
%In Section~\ref{sec:minds}, we propose a new way of visualizing goodness-of-fit that uses the traditional node-link diagram to visualize the entire network instead of numerical summaries of the the network.

\section{Example Data and Models}\label{sec:exdatamods}

% removed the following plot
<<adjmatviz, fig.align='center', fig.cap="A visualization of the adjacency matrices of the three waves of network observations in the ``Teenage Friends and Lifestyle Study'' data. The subset we will be using is outlined in red.", results='hide',fig.show='asis', fig.width=8, fig.height=3, eval = FALSE>>=
source("code/00e_small_friends.R")
view_rs2$wave <- factor(view_rs2$wave)
levels(view_rs2$wave) <- c("Wave 1", "Wave 2", "Wave 3")
ggplot(data= view_rs2, aes(x = y, y=-x)) +
  geom_tile(aes(fill = as.factor(value))) +
  scale_fill_manual("i likes j", labels=c("no", "yes"),
                    values = c('white', 'black')) +
  geom_rect(data= NULL, inherit.aes = FALSE, color = 'red',
            aes(xmin = 19.5, xmax = 35.5, ymax = -19.5, ymin = -35.5, fill =NA)) +
  facet_wrap(~wave) + theme_bw() +
  labs(x = "node j (alter)", y = "node i (ego)") +
  ThemeNoNet +
  theme(aspect.ratio=1, axis.text = element_blank(), axis.ticks = element_blank(), panel.grid = element_blank())
@

<<alldataviz, fig.cap="The small friendship network data we model throughout this paper. The teenagers' drinking behavior changes and the network becomes less densely connected over time. These are the mechanisms we hope to model with SAOMs.", out.width='.99\\textwidth', fig.height=3, results='hide'>>=
cols <- RColorBrewer::brewer.pal(8, "Greys")

source("code/00e_small_friends.R")
view_rs2$wave <- factor(view_rs2$wave)
levels(view_rs2$wave) <- c("Wave 1", "Wave 2", "Wave 3")
actual1 <- merge(data.frame(as.edgelist(as.network(fd2.w1))),
                 data.frame(id = 1:16, drink = drink2[,1]),
                 by.x = "X1", by.y = "id", all = T)
actual1$Wave <- 1
actual2 <- merge(data.frame(as.edgelist(as.network(fd2.w2))),
                 data.frame(id = 1:16,drink = drink2[,2]),
                 by.x = "X1", by.y = "id", all = T)
actual2$Wave <- 2
actual3 <- merge(data.frame(as.edgelist(as.network(fd2.w3))),
                 data.frame(id = 1:16,drink = drink2[,3]),
                 by.x = "X1", by.y = "id", all = T)
actual3$Wave <- 3
alldat <- rbind(actual1, actual2, actual3)
#alldat$X1 <- paste0("V", alldat$X1)
alldat$X1 <- as.factor(alldat$X1)
#alldat$X2 <- ifelse(!is.na(alldat$X2), paste0("V", alldat$X2), NA)
alldat$X2 <- as.factor(alldat$X2)
#write_csv(alldat, "smallfriends4Geomnet.csv")
alldataviz <- ggplot(data = alldat,
       aes(from_id = X1, to_id = X2, color = as.ordered(drink), group = as.ordered(drink))) +
  geom_net(fiteach = FALSE, directed=TRUE, layout.alg = 'fruchtermanreingold', size = 4, arrowsize = .25, arrowgap = .05, ecolour = 'black', labelon=T, labelcolour = 'white', fontsize = 2, vjust = .5, linewidth=0.25) +
  scale_color_manual(values = cols[8:5], name = "Drinking\nBehavior", labels = c("never", "1-2 times per year", "once per month", "once per week")) +
  ThemeNet +
  facet_wrap(~Wave, labeller = "label_both") +
  theme(legend.position = 'bottom',
        panel.background = element_rect(fill = NA, color = "grey30"),
        legend.text = element_text(size = 10, family = "Times New Roman"),
        legend.title = element_text(size = 10, family = "Times New Roman"))
alldataviz
@

\noindent \textbf{Example Data:} To guide our visual exploration of SAOMs, we use two example data sources. The first is a subset of the 50 actor dataset from the ``Teenage Friends and Lifestyle Study" that is provided on the \texttt{RSiena} webpage. These data come from \citet{friendsdata}, and we chose to only work with a subset of the data to make node-link diagrams easier to see and to make any changes in the network more noticeable. 
We visualize this data with a node-link diagram in Figure~\ref{fig:alldataviz}. %For model fitting, we condition on wave 1 and estimate the parameters of the models for the second and third waves. 
The actor covariate of interest, drinking behavior, has four values: (1) does not drink, (2) drinks once or twice a year, (3) drinks once a month, and (4) drinks once a week. In Figure~\ref{fig:alldataviz}, the nodes are colored according to the drinking behavior of that student. We can see that, as time passes, the students seem to drink more and become increasingly isolated into smaller groups. An analysis of this type of data with a SAOM should capture these dynamics in a way that allows the researcher to draw conclusions about the nature of the network and behavioral forces at play.

The second data example we use is a much more recent dataset on collaboration in the United States Senate during the $111^{th}$ through $114^{th}$ Congresses, shown in Figure~\ref{fig:senateplot}. These sessions of congress correspond to the years of Barack Obama's presidency, from 2009-2016. (Details of how this data can be downloaded are provided by FranÃ§ois Briatte at \url{https://github.com/briatte/congress}.) In this network, ties are directed from senator $i$ to senator $j$ when senator $i$ cosponsors the bill that senator $j$ authored. There are hundreds of ties between senators when they are connected in this way, so we simplify the network by computing a single value for each pair of senators called the \textit{weighted propensity to cosponsor} (WPC). This value is defined in \citet{senate} as the total number of times senator $i$ cosponsors senator $j$'s bill, weighted by the number of total cosponsors:  
\begin{equation}\label{eq:sen1}
WPC_{ij} = \dfrac{\sum\limits_{k=1}^{n_j} \frac{Y_{ij(k)}}{c_{j(k)}}}{\sum\limits_{k=1}^{n_j} \frac{1}{c_{j(k)}}}
\end{equation}
where $n_j$ is the number of bills in a congressional session authored by senator $j$, $c_{j(k)}$ is the number of cosponsors on senator $j$'s $k^{th}$ bill, where $k \in \{1,\dots, n_j\}$, and $Y_{ij(k)}$ is a binary variable that is 1 if senator $i$ cosponsored senator $j$'s $k^{th}$ bill, and is 0 otherwise. This measure ranges in value from 0 to 1, where $WPC_{ij} = 1$ if senator $i$ is a cosponsor on every one of senator $j$'s bills and $WPC_{ij} = 0$ if senator $i$ is never a cosponsor any of senator $j$'s bills.

<<senateplot, fig.cap='The collaboration network in the four senates during the Obama years, 2009-2016. Edges are shown only if the WPC between two senators is greater than 0.25. We use the FR layout algorithm here.', fig.width=8, fig.height=8, out.width='\\textwidth'>>=
seobama <- read_csv("data/congress/senateobamapres_gsw_25.csv")
to_label <- c("Joseph R. Biden Jr.", "John S. McCain", "Ted Cruz","Marco Rubio",
  "Lindsey O. Graham","Rand Paul", "Bernard Sanders", "Jim Webb", "Mitch McConnell",
  "Harry M. Reid", "Hillary Rodham Clinton", "Amy Jean Klobuchar")
seobama$label <- as.factor(ifelse(seobama$source %in% to_label, seobama$source, ""))
levels(seobama$label) <- c("", "Amy Klobuchar", "Bernie Sanders", "Harry Reid",
                           "Hillary Clinton", "Jim Webb", "John McCain", "Joe Biden",
                           "Lindsey Graham", "Marco Rubio", "Mitch McConnell",
                           "Rand Paul", "Ted Cruz")
set.seed(56049382)
ggplot(data = seobama) +
  geom_net(directed = T, labelon=T, arrowsize = .25, singletons= T, fiteach = T, linewidth = .25, layout.alg = 'fruchtermanreingold', fontsize = 3, repel=T,
           aes(from_id = source, to_id = target, color = party, label = label),
           size = 1) +
  ThemeNet +
  scale_color_manual(values = c("black", "grey30","#ef3b2c")) +
  theme(legend.position = 'bottom', strip.text = element_text(size = 20)) +
  facet_wrap(~senate, nrow = 2, labeller = "label_both") +
  xlim(c(0,1.05)) +
  ylim(c(0,1.05))
@

Because we require binary edges for SAOMs, we focus only on strong collaborations, where $WPC$ is high. For a senate collaboration network $x$, edges are defined as
\begin{equation}\label{eq:wpc2}
  x_{ij} =
  \begin{cases}
                                   1 &  \text{if } WPC_{ij} > 0.25 \\
                                   0 & \text{if } WPC_{ij} \leq 0.25.
  \end{cases}
\end{equation}
The node-link diagram for the four senates during the Obama administration are shown in Figure~\ref{fig:senateplot}. We can see that Senate 111 is much more densely connected than the other three, which have one large, sparsely connected component with a few smaller connected groups throughout. Some prominent names are shown to demonstrate their place in the network.  As with the friendship example, analyzing the senate data with a SAOM should capture the underlying dynamics, giving insight into the impact that the actor covairates have on the senate's collaboration structure. By using data sets from very different contexts, we hope to assess some differences in model performance. 


\noindent \textbf{Models:} To our example data, we fit three different SAOMs. Each SAOM uses a simple rate function, $\alpha_m$, and an objective function with two or three parameters. The first model, M1, contains the absolute minimum number of parameters in the objective function $f_i(x)$: 
\begin{equation}\label{eq:m1}
M1: \quad f_i(x) = \beta_1s_{i1}(x) + \beta_2s_{i2}(x),
\end{equation}
where $s_{i1}$ is the density network statistic and $s_{i2}$ is the reciprocity network statistic for actor $i$ at the current network state $x$ (see Table~\ref{tab:effects}). The second and third models, M2 and M3, contain one additional parameter each in the objective function. We chose the additional parameters because they were significant ($p$-value $<$ 0.05) according to a Wald-type test provided in the \texttt{RSiena} software \citep{RSienaTest}. The M2 model contains an actor-level covariate parameter, and the M3 model contains an additional strutural effect in the objective function.
\begin{align}\label{eq:m2m3}
M2: \quad f_i(x,z) & = \beta_1s_{i1}(x) + \beta_2s_{i2}(x) + \beta_3s_{i3}(x, z) \\
M3: \quad f_i(x) & = \beta_1s_{i1}(x) + \beta_2s_{i2}(x) + \beta_4s_{i4}(x),
\end{align}

\begin{figure}
\begin{subfigure}[t]{.45\textwidth}
<<jtt, echo=FALSE, fig.width=2, fig.height=2, message = FALSE, warning = FALSE, fig.align='center',out.width='.49\\textwidth'>>=
jTTe <- data.frame(from = c('i', 'i', 'h'), to = c('h', 'j', 'j'))
jTTn <- data.frame(id = letters[8:10], group = c(1,1,2))

jTT <- merge(jTTe, jTTn, by.x = 'from', by.y = "id", all = T)

set.seed(12345)
ggplot(data = jTT, aes(from_id = from, to_id = to)) +
  geom_net(aes(shape = as.factor(group)), directed = T, labelon = T,
           labelcolour='white',vjust = 0.5, hjust =0.5, arrowgap = .15,
           size=10, fontsize = 5, ecolour="black",
           linetype = c("solid", "solid","dotted", "solid")) +
  expand_limits(x=c(-0.2,1.2), y=c(-0.1,1.1)) +
  ThemeNet +
  theme(legend.position = "none") + 
  labs(title = expression(paste(s[i3], "(x,z)")))
  # labs(title = expression(paste(s[i3], "(x,z)", "=",Sigma[j != h], x[ij], x[ih], x[hj] %*% 
  #                               I, "(", z[i], "=", z[h]!= z[j], ")")
  #                         )
  #      )
@
\caption{\label{fig:jtt}Realization of a JTT, where the covariate is represented by the shape of the nodes. The dotted tie is encouraged if $\beta_3$ is positive. $s_{i3}(x,z) = \sum_{j \neq h} x_{ij}x_{ih}x_{hj} \mathbb{I}(z_i = z_h \neq z_j)$}
\end{subfigure}\hfill
\begin{subfigure}[t]{.45\textwidth}
<<dab, echo=FALSE, fig.width=2, fig.height=2, message = FALSE, warning = FALSE, fig.align='center', out.width='.49\\textwidth'>>=
dade <- data.frame(from = c('i', 'i', 'h', 'k', 'i'), to = c('h', 'k', 'j', 'j','j'))
dadn <- data.frame(id = letters[8:11], group = c(1,1,1,1))

dad <- merge(dade, dadn, by.x = 'from', by.y = "id", all = T)

set.seed(12345)
ggplot(data = dad, aes(from_id = from, to_id = to)) +
  geom_net(directed = T, labelon = T, ecolour="black",
           linetype = c("solid", "solid", "dotted", "solid","solid" ,"solid"),
           labelcolour='white',vjust = 0.5, hjust =0.5, arrowgap = .15, size=10, fontsize = 5) +
  expand_limits(x=c(-0.1,1.1), y=c(-0.1,1.1)) +
  ThemeNet +
  theme(legend.position = "none") + 
  labs(title = expression(paste(s[i4], "(x)")))
  # labs(title = expression(paste(s[i4], "(x)", "=", "|{j:", x[ij],"=0, ", Sigma[h], x[ih], x[hj] >= 2, "}|" )
  #                         )
  #      )
@
\caption{\label{fig:dad}Realization of a N22 between actors $i$ and $j$. The dotted tie is encouraged if $\beta_4$ has a positive value. $s_{i4}(x) =  |\{j : x_{ij} = 0, \sum\limits_h x_{ih}x_{hj} \geq 2\}|$}
\end{subfigure}
\caption{\label{fig:structures} The additional network effects included in our models fit to the friends data. On the left, a JTT between $i$ and $j$, who have different covariate values, and on the right, a N22 between $i$ and $j$, who are indirectly connected by nodes $k$, $h$.}
\end{figure}

\noindent where $s_{i3}(x,z) = \sum_{j \neq h} x_{ij}x_{ih}x_{hj} \mathbb{I}(z_i = z_h \neq z_j)$, and $s_{i4}(x) =  |\{j : x_{ij} = 0, \sum\limits_h x_{ih}x_{hj} \geq 2\}|$. These statistics are known as the \emph{number of jumping transitive triplets} (JTT) and the \emph{number of doubly achieved distances two effect} (N22), respectively. The JTT effect emphasizes triads formed between actors with \emph{different} covariate values, while the N22 effect emphasizes \emph{indirect} ties between actors. The covariate for the friendship data is the drinking behavior, and is the number of bills authored for the senate data. These additional effects are visualized in Figure~\ref{fig:jtt} and Figure~\ref{fig:dad}, where the ties affected by the parameter value are represented by dotted lines. If the estimates of these parameters $\hat{\beta}_1, \dots, \hat{\beta}_4$ are positive, the model encourages the tie represented by the dotted line to form, while the opposite is true if the parameter estimate is negative. 


\section{Model in the data space}\label{sec:minds}

%Every good \emph{data} analysis includes both numerical and visual summaries of the data, so why restrict \emph{model} analysis, description, and diagnostics to numerical summaries? 
\emph{Model visualization} complements traditional model diagnostic tools such as $R^2$ values and residual plots by enriching our understanding and interpretation of statistical models. Each of the three different meanings of the word ``model", the model \emph{family}, the model \emph{form}, and the \emph{fitted} model \citep[see][pg. 2]{modelvis}, can benefit from model visualization (model-vis). We apply the three principles of model-vis: view the model in the data space, visualize collections of models, and explore the process of fitting the model, to the models and data introduced in Section~\ref{sec:exdatamods}. Since we have already decided on our model family (SAOMs), we shift our focus to the fitted model and the model form. Specifically, we use model visualization to learn more about how the \emph{model form} affects the \emph{fitted model}. 

First, we put models in the data space to assess the model fit. %In \citeauthor{modelvis}, they chose define the \textit{data space} as ``the region over which we can reliably make inferences, usually a hypercube containing the data" \citep[p.~206]{modelvis}. But what does this definition mean for dynamic network models?
We define the data space for SAOMs as the combined data structures of the network: the node, edge, and time information. We use the \texttt{R} package \texttt{geomnet} \citep{geomnet} to visualize the three data spaces together. The package lays out the network using node-link diagrams, and the network's node or edge data are then mapped to different visual features.  The color, size, and shape of the points can be used to represent variables in the node data, while the color, linewidth, and linetype of the lines between points can be used to represent the edge variables. To view temporal changes, we place the network at different timepoints side-by-side to see the evolution. By pulling all of this information together using \texttt{geomnet}, we can show the entire data space at once.

<<funsenatevis, fig.height=4, fig.cap='The 111th Senate at two times: while Clinton was in the senate in 2009 (on the left) and after she left (on the right). We map sex, party, and bills authored to the shape, color, and size of the nodes, respectively. We also map the $WPC$, to the edge linewidth.'>>=
se111clint <- filter(seobama, senate == 111)
se111noclint <- filter(seobama, senate == 111)
se111noclint$target[which(se111noclint$target == "Hillary Rodham Clinton")] <- NA
se111clint$Clinton <- 'Yes'
se111noclint$Clinton <- 'No'
clintonSenate <- rbind(se111clint, se111noclint)
clintonSenate$Clinton <- as.factor(clintonSenate$Clinton)
clintonSenate$Clinton <- ordered(clintonSenate$Clinton, levels = c("Yes", "No"))

set.seed(12981120)
clintonSenate %>% filter(!(source %in% c("Roland Burris", "Bernard Sanders", "Frank R. Lautenberg", "Mary L. Landrieu") & is.na(target))) %>%
ggplot() +
  geom_net(directed = T, labelon=F, arrowsize = .3, singletons= F, fiteach = T, arrowgap = .01, layout.alg = 'fruchtermanreingold', ecolour = "grey70",
           aes(from_id = source, to_id = target, color = party, linewidth = gsw, shape = sex,size = n_au/30)) +
  scale_size_continuous(name="# bills\nauthored" , guide = guide_legend(nrow= 2), breaks = c(0,1,2,3), labels = c(0,30, 60, 90)) +
  #theme_net() +
  ThemeNet +
  scale_shape_manual(values = c(17,16), guide = guide_legend(nrow= 2)) +
  scale_color_manual(values = c("black", "grey80","#ef3b2c"), guide = guide_legend(nrow=2)) +
  facet_wrap(~Clinton, nrow = 1, labeller = 'label_both')  +
  theme(legend.position = "bottom")
@

To demonstrate the functionality of \texttt{geomnet}, we visualize the connections in the $111^{th}$ United States Senate at two different timepoints: when Hillary Clinton was in the senate, and after she left to become Secretary of State. Clinton was only in the $111^{th}$ senate from January 3-20, 2009, in the middle of her second term as the junior senator from New York. In that time, she authored two bills and was a cosponsor on 17. There are many senators who cosponsored both of her bills, giving their edges to her a $WPC$ of 1. So with Clinton included in the node-link diagram in Figure~\ref{fig:funsenatevis}, the senate looks much more collaborative than it does without her. We then map our potential covariates to visual features: number of bills authored  maps to the size of the node, gender maps to the shape, and party to the color. In addition, we visualize the strength of the tie by mapping the $WPC$ value between the two senators to the linewidth of the edge. By viewing node information, edge information, and time simultaneously, we have learned that Senator Clinton greatly affected the structure of collaboration within the senate.

<<summnetM1, message=FALSE, fig.cap='On the left, the first wave of observed data that is conditioned on in the model. On the right, the second wave of observed data. In the middle, a summary network from the first model fit to the data. This summary network represents 1,000 simulations of wave 2 using the values from the simple fitted model M1.',results='hide', fig.height=2.5>>=
friendshipData <- array(c(fd2.w1, fd2.w2, fd2.w3), dim = c(16, 16, 3))
friendshipData <- array(c(fd2.w1, fd2.w2, fd2.w3), dim = c(16, 16, 3))
drink1 <- drink[20:35,1]
drink2 <- drink[20:35,2]
drink3 <- drink[20:35,3]
friendship <- sienaDependent(friendshipData)
alcohol <- varCovar(cbind(drink1,drink2, drink3))
mydata <- sienaDataCreate(friendship, alcohol)
M1eff <- getEffects(mydata)
M2eff <- includeEffects(M1eff, jumpXTransTrip, interaction1 = "alcohol")
M3eff <- includeEffects(M1eff, nbrDist2twice)
simu2 <- read.csv("data/simulation-1000-M1-M2-M3.csv")
means <- simu2 %>% group_by(Model, parameter) %>% summarize(
  means = mean(estimate)
)
M1parms <- (means %>% filter(Model == "M1"))$means
M2parms <- (means %>% filter(Model == "M2"))$means
M3parms <- (means %>% filter(Model == "M3"))$means
set.seed(4231352)
#M1sims1000 <- netvizinf::saom_simulate(dat = mydata,
#                            struct = M1eff,
#                       parms = as.numeric(M1parms),
#                       N = 1000)
#save(M1sims1000, file = "data/M1sims1000.RData")

# M2sims1000 <- netvizinf::saom_simulate(dat = mydata,
#                             struct = M2eff,
#                        parms = as.numeric(M2parms),
#                        N = 1000)
# save(M2sims1000, file = "data/M2sims1000.RData")

# M3sims1000 <- netvizinf::saom_simulate(dat = mydata,
#                             struct = M3eff,
#                        parms = as.numeric(M3parms),
#                        N = 1000)
# save(M3sims1000, file = "data/M3sims1000.RData")


#M1simsdf <- netvizinf::sims_to_df(M1sims1000)
# write_csv(M1simsdf, "data/M1simsdf.csv")
M1simsdf <- read_csv("data/M1simsdf.csv")
#M2simsdf <- netvizinf::sims_to_df(M2sims1000)
#write_csv(M2simsdf, "data/M2simsdf.csv")
M2simsdf <- read_csv("data/M2simsdf.csv")
# M3simsdf <- netvizinf::sims_to_df(M3sims1000)
# write_csv(M3simsdf, "data/M3simsdf.csv")
M3simsdf <- read_csv("data/M3simsdf.csv")

M1avgW2 <- M1simsdf %>% filter(!is.na(to) & wave == 1) %>%
  group_by(from, to) %>%
  summarise(count = n()) %>%
  mutate(weight = ifelse(from == to, 0, count))
M2avgW2 <- M2simsdf %>% filter(!is.na(to) & wave == 1) %>%
  group_by(from, to) %>%
  summarise(count = n()) %>%
  mutate(weight = ifelse(from == to, 0, count))
M3avgW2 <- M3simsdf %>% filter(!is.na(to) & wave == 1) %>%
  group_by(from, to) %>%
  summarise(count = n()) %>%
  mutate(weight = ifelse(from == to, 0, count))

# make a df of wave 2, wave 1, and the three averages and facet.
names(actual1)[1:2] <- c("from", "to")
names(actual2)[1:2] <- c("from", "to")
actual1$count <- 1
actual1$weight <- 1
actual2$count <- 1
actual2$weight <- 1
actual1 <- actual1 %>% dplyr::select(from,to, count, weight)
actual2 <- actual2 %>% dplyr::select(from,to, count, weight)
actual1$cat <- "1st Wave"
actual2$cat <- "2nd Wave"

avgW2M1 <- M1avgW2 %>% ungroup() %>%
  filter(weight > 50) %>%
  mutate(from = as.factor(from), to = as.factor(to),
         cat = "Model1")
add1 <- setdiff(as.character(1:16), unique(c(as.character(avgW2M1$from), as.character(avgW2M1$to))))
avgW2M1 %>% add_row(from = add1, to = NA, count = NA, weight = NA, cat = "Model1") -> avgW2M1
avgW2M2 <- M2avgW2 %>% ungroup() %>%
  filter(weight > 50) %>%
  mutate(from = as.factor(from), to = as.factor(to),
         cat = "Model2")
add2 <- setdiff(as.character(1:16), unique(c(as.character(avgW2M2$from), as.character(avgW2M2$to))))
avgW2M2 %>% add_row(from = add2, to = NA, count = NA, weight = NA, cat = "Model2") -> avgW2M2
avgW2M3 <- M3avgW2 %>% ungroup() %>%
  filter(weight > 50) %>%
  mutate(from = as.factor(from), to = as.factor(to),
         cat = "Model3")
add3 <- setdiff(as.character(1:16), unique(c(as.character(avgW2M3$from), as.character(avgW2M3$to))))
avgW2M3 %>% add_row(from = add3, to = NA, count = NA, weight = NA, cat = "Model3") -> avgW2M3

combinedavgs <- rbind(actual1, actual2, avgW2M1, avgW2M2, avgW2M3)
combinedavgs %>% group_by(cat) %>%
  mutate(linewidth = weight / max(weight,na.rm = T)) -> combinedavgs

combinedavgs %>% filter(cat == "Model1") %>% mutate(logweight = log(weight)) -> t1
colors <- tweenr::tween_color(data = c("#969696", "#d73027"), n = table(t1$cat) %>% as.numeric, ease = 'linear')
t1 %>% arrange(logweight) -> t1
t1$color <- NA
t1$color <- colors[[1]]

p1 <- ggplot(data = t1) +
  geom_net(aes(from_id = from, to_id = to, linewidth= linewidth),
           directed = T, curvature = .2, labelon=T, size = 3,
           arrow = arrow(angle = 20, type = "open", length = unit(.2, "cm")), ecolour = "black",
           color = "#252525", hjust = .5, vjust = .5, labelcolour = 'white', fontsize = 2, arrowgap = .02,
           singletons = T) +
  labs(title = "M1 Expected Network") + ThemeNet + 
  theme(plot.background = element_rect(color = 'black'))

p2 <- ggplot(data = filter(combinedavgs, cat == "1st Wave")) +
  geom_net(aes(from_id = from, to_id = to, linewidth= linewidth/3),
           directed = T, curvature = .2, labelon=T, size = 3,
           arrow = arrow(angle = 20, type = "open", length = unit(.2, "cm")), ecolour = "#969696",
           color = "#252525", hjust = .5, vjust = .5, labelcolour = 'white', fontsize = 2, arrowgap = .03,
           singletons = T) +
   labs(title = "Wave 1") + ThemeNet +
    theme(plot.background = element_rect(color = 'black'))
p3 <- ggplot(data = filter(combinedavgs, cat == "2nd Wave")) +
  geom_net(aes(from_id = from, to_id = to, linewidth= linewidth/3),
           directed = T, curvature = .2, labelon=T, size = 3,
           arrow = arrow(angle = 20, type = "open", length = unit(.2, "cm")), ecolour = "#969696",
           color = "#252525", hjust = .5, vjust = .5, labelcolour = 'white', fontsize = 2, arrowgap = .03,
           singletons = T) +
  labs(title = "Wave 2")  +  ThemeNet +
  theme(plot.background = element_rect(color = 'black'))
grid.arrange(p2, p1, p3, nrow = 1)
@

To view a SAOM in the data space, we simulate from the fitted model. No single network instance simulated from a SAOM will fully represent the model, so instead we look at an \textit{expectation} from the network model. We frequently rely on expected values in statistics, but network models, especially those as complex as SAOMs, lack an expected network measure. We could talk about expected values of parameters, but the parameters can be hard to interpret and cannot teach us much about the overall structure of the network. How then, can we arrive at an ``expected" network? We answer this question with visualization.
%Expected values of parameters can be computed, but if they cannot directly tell us anything about our dependent variable, they lose value. 
%There is no defined expected value of a network model, though expectations exist for most other statistical models. 
%For network data, we view an ``average" network through a sort of summary network drawn using the traditional node-link diagram. 
%In Figure~\ref{fig:summnetM1}, we show the \textit{expected network} of wave 2 from model M1, created with 1,000 simulations from the model. 
To construct an expected network, we: 
\begin{enumerate}
\item simulate 1,000 wave 2 and wave 3 instances of our small friendship network from model M1 using parameters that have previously been estimated. Then, we 
\item count the number of times each edge $i \rightarrow j$ appeared in one of the 1,000 network simulations. Then, we
\item construct one weighted summary network for each time point, which has edge weights equal to the proportion of the simulated networks in which the edge appeared. Finally, we 
\item create the \textit{expected network} by including only the edges that appeared in more than 5\% of the simulations. 
\end{enumerate}
The resulting expected network is in the middle panel of Figure~\ref{fig:summnetM1}. Edges with higher weights are darker, thicker lines. On either side of the average network in Figure~\ref{fig:summnetM1}, we show the data for comparison: wave 1 on the left, and wave 2 on the right. We can see that the structure of the average network is much more similar to wave 1 than to wave 2. In both wave 1 and the expected network, the edges form two distinct clusters of nodes: $\{2,3,4,5,6,12,13,15\}$ and $\{7,8,9,10,11,14\}$. The expected network, however, is supposed to represent the second wave of data, on the right in Figure~\ref{fig:summnetM1}. The expected network does not resemble wave 2, which has one small node cluster and on large node cluster, with four unconnected nodes. Because the expected network does not resemble wave 2, we take this as evidence that the simple model, M1, does not capture the change in network structure from the first to the second wave of data. Visualizing the expected network in this way places the model M1 in the data space, and helps us assess model fit. % We can use the average network to help determine model goodness-of-fit. Because the the expected  network looks more like the first wave than the second wave, we can use the visualization in Figure~\ref{fig:summnetM1} as evidence of poor model fit.  

<<summary2, fig.height=2.5, fig.cap="Network differences between the expected network from M1 and Wave 1 and Wave 2 of the data. Some edges appear in both the data and the expected Wave 2, while others only appear in the data or only in the exectation. Width of edges represent the proportion of appearances in the 1,000 simulations. The FR layout is used.">>=

red <- "#fd8d3c"
green <- "#41ab5d"

#762a83
#af8dc3
#e7d4e8
#d9f0d3
#7fbf7b
#1b7837

am_t1 <- as.matrix(igraph::as_adj(igraph::graph_from_data_frame(arrange(t1, parse_number(from), parse_number(to)))))[-17,-17]

diff1 <- am_t1 - fd2.w1
diff1[which(diff1==1)] <- 2
diff1[which(fd2.w1 == 1 & am_t1 == 1)]  <- 1
diff1 <- diff1 %>% dplyr::as_data_frame() 
names(diff1) <- paste0("V", 1:16)
diff1 %>% mutate(from =paste0("V", 1:16)) %>% gather(to, value, -from) %>%
  mutate(from = as.character(parse_number(from)), to = as.character(parse_number(to)), 
         var = recode(value, `0` = "Neither", `2` = "InAvgNotW1",  `1`= "InBoth"),
         color2 = recode(value, `0`="white", `1`= "black", `2` = green)) %>%
  filter(value !=0) %>% 
  full_join(t1, c("from","to")) -> t1_1 
t1_1 <- arrange(t1_1, from, to)
p1 <- ggplot(data = t1_1) +
  geom_net(aes(from_id = from, to_id = to, linewidth= linewidth, linetype = color2),
           directed = T, curvature = .2, labelon=T, size = 5,
           arrow = arrow(angle = 20, type = "open", length = unit(.2, "cm")), # ecolour = t1_1$color2,
           color = "#252525", hjust = .5, vjust = .5, labelcolour = 'white', fontsize = 4, arrowgap = .02, layout.alg = "fruchtermanreingold",
           singletons = T) +
  scale_linetype_manual(values =c("longdash", "dotted") ) + 
  labs(title = "Difference between Wave 1 &\nWave 2 expectation from M1") + ThemeNet +
  theme(plot.background = element_rect(color = 'black'), legend.position = "none")

pforleg <- ggplot(data = mtcars) + 
  geom_line(aes(x = mpg, drat, group = as.factor(cyl), linetype = as.factor(cyl))) + 
  scale_linetype_manual(values = c("dotted", "solid", "longdash"), name = "Tie is...", labels = c("in data, in\nexpectation", "in data,not in\nexpectation", "not in data, in\nexpectation")) + 
  theme_grey() + 
  theme(legend.key = element_rect(color = "black", fill = "white"), 
        legend.key.height = unit(25, "pt"))


diff2 <- -am_t1 + fd2.w2 
diff2[which(diff2==1)] <- 2
diff2[which(fd2.w2 == 1 & am_t1 == 1)]  <- 1
diff2 <- dplyr::as_data_frame(diff2)
names(diff2) <- paste0("V", 1:16)
diff2 %>% mutate(from =paste0("V", 1:16)) %>% gather(to, value, -from) %>% 
  mutate(from = as.character(parse_number(from)), to = as.character(parse_number(to)), 
         var = recode(value, `0` = "Neither", `-1` = "InAvgNotW2",  `1`= "InBoth", `2` = "InW2notAvg"),
         color2 = recode(value, `0`="white", `1`= "black", `2` = red, `-1` = green)) %>% 
  filter(value !=0) %>% 
  full_join(t1, c("from","to")) -> t1_2 
t1_2$linewidth[is.na(t1_2$linewidth) & !is.na(t1_2$to)] <- 1
t1_2 <- arrange(t1_2, from, to)
t1_2 <- t1_2 %>% filter(!(from == 16 & is.na(to)))
p2 <- ggplot(data = t1_2) +
  geom_net(aes(from_id = from, to_id = to, linewidth= linewidth, linetype = color2),
           directed = T, curvature = .2, labelon=T, size = 5,
           arrow = arrow(angle = 20, type = "open", length = unit(.2, "cm")), #ecolour = t1_2$color2,
           color = "#252525", hjust = .5, vjust = .5, labelcolour = 'white', fontsize = 4, arrowgap = .02, layout.alg = "fruchtermanreingold",
           singletons = T) +
  scale_linetype_manual(values = c("longdash", "solid", "dotted")) + 
  labs(title = "Difference between Wave 2 &\nWave 2 expectation from M1") + ThemeNet +
  theme(plot.background = element_rect(color = 'black'), legend.position = "none")
legendsumm <- cowplot::get_legend(pforleg)
ggdraw() + draw_plot(p1, width = .4) +  draw_plot(p2, width = .4, x=.4) + 
  draw_grob(legendsumm,x=.8, y=.25, width = .2, height = .4)
@

Another way to view a SAOM in the data space is with \textit{difference networks} between the expected network and the data, as shown in Figure~\ref{fig:summary2}. We created these networks by computing the difference between the adjacency matrices for the expected network and wave 1, and for wave 2 and the expected network. The lack of model fit becomes even more clear in Figure~\ref{fig:summary2}, with the broken edges representing ties that only show up in the expected network, and the solid edges representing ties in the data that the model misses completely.  Dotted edges are in both the expectation and the data and represent where the model fit the data well. Again, the widths of the edges demonstrate the frequency of the ties in the simulations. On the left in Figure~\ref{fig:summary2}, we see the difference between the first wave of data and the expected network. We only see dotted and broken edges, meaning that the simulated networks consist primarily of edges in wave 1, plus some new edges that are not in the first wave of data. The added ties are all reciprocating edges in the data, meaning that the model predicts more reciprocity between actors than there actually is. On the right in Figure~\ref{fig:summary2}, we see the difference between the second wave of data and the expected network. Because the model is simulating the second wave of data, we would hope to see very few differences between the expected network and wave 2. However, there are six solid edges, which are edges in the data that do not appear at all in the expected network, and there are many broken edges that appear in the expected network but not in the data. Although there are some edges that appear in both the expected network and wave 2, the majority of edges in the expected network are not in wave 2, and there are several edges missing from the expectation. We conclude that the model M1 is a poor fit to our example data, which we have seen very clearly by viewing the model in the data space. 

% No longer using this plot
<<gof2, fig.cap="An example of viewing the distribution of a statistic of interest from a SAOM. Here, the Mahalanobis distance of the cumulative outdegree values computed on a simulated network to the mean value simulated is shown. The distance computed from the data is shown as a vertical line.", fig.height=3, out.width='.6\\textwidth', eval = FALSE>>=

library(lattice)
gof1 <- sienaGOF(ansnull, OutdegreeDistribution, varName ="friendship", join = FALSE)
histdat <- data.frame(x = log(gof1$`Period 1`$SimulatedTestStat))
ggplot(histdat) + geom_histogram(aes(x = x), binwidth = .1, fill = 'white', color = "black") +
  #geom_density(aes(x = x, y = ..density..*250)) +
  geom_vline(xintercept = log(gof1$`Period 1`$ObservedTestStat), color = 'red') +
  ThemeNoNet +
  labs(x = "log(Statistic Value)", title = "Distribution of log Mahalanobis Distance")

@

%\st{If, as discussed in Section~\ref{sec:modgof},
%Figure~\ref{fig:sienagof},
%summary statistics such as outdegree and indegree distribution computed on the network are of interest, we can look at the computed distances between the data and the model simulations. distributions of these statistics computed on networks simulated from the model show the model in the edge data space. For an example, see Figure~\ref{fig:gof2}, where the distribution of the log of the Mahalanobis distance is shown for the outdegree distribution auxiliary statistics, and the log Mahalanobis distance for the observed data is shown as a vertical line. }


% \begin{itemize}
% \item WHat is the model? the network model (SAOM model is just one of many)
% \item What is the data? Edge data, node data, observed time points, unobserved timepoints (CTMC)
% \item Note: specifically dealing with dynamic networks observed at discrete time points.
% \end{itemize}
% What are some things that could go in here? need more data examples; not just the friends data.
% Harry Potter data??? I want to find my own data, too.}

\section{Collections of models}\label{sec:collectmodels}

Now that we have assessed model fit, we next want to learn about model behavior in order to choose better fitting models in the future. This is the second principle of model visualization: "collections [of models] are more informative" \citep{modelvis}. We examine the behavior of SAOMs by visualizing collections of models. We build collections of models in four ways. First, we explore the space of all possible models to determine the most significant model parameters.  Second, we vary model settings to uncover relationships between parameters and their effects on the fitted model. Next, we fit the same model to the same data many times to assess the behavior of the underlying simulations. Finally, we fit the same model to different data to learn about fitted parameter values under different conditions. We chose these four collections because they each explore something different about SAOM behavior: the choice of parameters in the objective function, the effects of underlying simulations, and the resulting fitted values under different circumstances. %Using the tools in our visualization toolbox, we view these collections to learn about model behavior.%The first considers the many parameters to include in a SAOM, while the second results in different parameter estimates, so we can see how parameter estimates vary. The third collection shows how those many possible parameters affect the values of parameters in the fitted models. Finally, we explore how the same model behaves under two completely different circumstances. 

%\subsection{Exploring the space of all possible models}\label{sec:allpossible}

\noindent \textbf{Exploring the space of all possible models:} The \texttt{RSiena} manual contains over eighty effects to include in the SAOM objective function. To select objective function parameters for fitting models to our example data, we searched through all possible effects available to find \textit{significant} effects. The \texttt{RSiena} package includes the function \texttt{Wald.Rsiena} for Wald-type tests of parameter significance. We start by including the outdegree and reciprocity parameters in the model then add one effect to the objective function, fit the model to the example data, test the added effect for significance, and repeat for all possible model parameters. We performed this procedure, which we used to pick M2 and M3, for both the small friendship and the senate collaboration data. A visualization of the significant effects for the two example data sets are shown in Figure~\ref{fig:sigtesting} in the Appendix. Using this one-at-a-time testing method results in many possible significant parameters of different type and magnitude, demonstrating the flexibility of these models.  %We see in both the friendship data and the senate data results that most of the significant effects have absolute value less than ten. In addition, the $p$-values for the effects from the friendship data are more spread out than the $p$-values for the senate data, which are concentrated at about 0.02 or less. This may suggest that larger data sets tend to result generally in smaller $p$-values, just like a larger sample size results in smaller $p$-values in a $t$-test.

%\subsection{Varying model settings}

\noindent \textbf{Varying model settings:} The second method for viewing collections of models is varying model settings. We have done this already by choosing three different models M1, M2, M3 to fit to our example data sets. To explore the behavior of these three models, we simulate from them. Using the parameter values given in Table~\ref{tab:meanests} for the friendship data, we simulated 1,000 observations from each of the three models. From the simulations, we created an expected network for wave 2 from models M1, M2, and M3 using the same method described in Section~\ref{sec:minds}. These expected networks are shown in Figure~\ref{fig:averagenet} in the Appendix. We see that all models are failing to capture the structure of the second wave, even though the individual parameters were all significant. %In Section~\ref{sec:samemodsamedat}, we fit these three models to the data 1,000 times, and examine the parameter densities. Here, . For simulation, we used the mean parameter values from the 1,000 fits of the corresponding model. 

%\subsection{Fitting the same model to the same data}\label{sec:samemodsamedat}

<<distests, fig.cap="Distribution of fitted parameter values for our three SAOMs. The inclusion of $\\beta_3$ or $\\beta_4$ clearly has an effect on the distribution of the rate parameters, $\\alpha_1$ and $\\alpha_2$, and on the other objective function parameters $\\beta_1, \\beta_2$.", fig.height=3>>=

simu2 <- read.csv("data/simulation-1000-M1-M2-M3.csv")
levels(simu2$parameter) <- c("alpha[1]","alpha[2]","beta[1]","beta[2]","beta[3]","beta[4]")
ggplot(data = simu2) +
  geom_density(aes(x = estimate, fill = Model), alpha = .5) +
  facet_wrap(~parameter, scales = 'free', labeller= label_parsed) +
  ThemeNoNet +
  scale_fill_brewer(palette = "Greys") + 
  xlab("estimate value") + 
  theme(legend.position = 'bottom')
@

\noindent \textbf{Fitting the same model to the same data:} The simulations performed to fit a SAOM lead to inherent variability in the fitted model. To examine this variability, we fit models M1, M2, and M3 to the friendship network 1,000 times each. The distributions of the fitted model parameters are shown in Figure~\ref{fig:distests}. Clearly, the inclusion of the JTT parameter, $\beta_3$, results in very different estimates the of the other four parameters in all models, $\alpha_1$, $\alpha_2$, $\beta_1$, and $\beta_2$. On the other hand, the inclusion of the indirect tie parameter, $\beta_4$, does not lead to different estimates of other model parameters. When $\beta_3$ is included, its estimate is postitive, meaning that friendships between two girls with different drinking behaviors tend to form when there is an intermediary who is already friends with the two girls. The inclusion of this parameter leads to increases in the rate parameters' estimates. Thus, when modeling the transitive triplet behavior in this way, we conclude that girls change friends more frequently. It is not clear, however, that the addition of a parameter to the objective function \textit{should} effect the estimates of the rate parameters. The strong relationship between rate parameter estimates and objective function parameters is further shown in Figure~\ref{fig:corplots} in the Appendix. After visualizing the results of the same models fit many times to the same data, we were able to see potential model behavior problems right away.   %In comparison with models M1 and M3, model M2 typically has higher estimates of the rate parameter, meaning that the inclusion of the covariate statistic in the model leads to higher estimates of the number of times, on average, a node gets to change its ties. 

%\subsection{Fitting the same model to different data}\label{sec:samemoddiffdat}

<<tableofmeans>>=
simu2 <- read.csv("data/simulation-1000-M1-M2-M3.csv")
senM1est <- read.csv("data/congress/senateM1ests.csv")
senM2est <- read.csv("data/congress/senateM2ests.csv")
senM3est <- read.csv("data/congress/senateM3ests.csv")
senM1est$Model <- "M1"
senM2est$Model <- "M2"
senM3est$Model <- "M3"
senM1est$Sim <- 1:nrow(senM1est)
senM2est$Sim <- 1:nrow(senM2est)
senM3est$Sim <- 1:nrow(senM3est)
senM1est %>% gather(parameter, estimate, Rate:recip) %>% dplyr::select(Sim, Model, parameter, estimate) -> senM1est2
senM2est %>% gather(parameter, estimate, Rate:jumpXTransTrip) %>% dplyr::select(Sim, Model, parameter, estimate) -> senM2est2
senM3est %>% gather(parameter, estimate, Rate:nbrDist2twice) %>% dplyr::select(Sim, Model, parameter, estimate) -> senM3est2
senateEsts <- rbind(senM1est2, senM2est2, senM3est2)
senateEsts$parameter <- as.factor(senateEsts$parameter)
levels(senateEsts$parameter) <- c("beta1", "beta3", "beta4", "alpha1", "alpha2", "alpha3", "beta2")
SSE <-
  senateEsts %>% group_by(Model, parameter) %>%
  summarize(meanEst = mean(estimate), sdEst = sd(estimate)) %>%
  mutate(val = paste0(sprintf("%.3f",meanEst), " ", "(", sprintf("%.3f", sdEst), ")")) %>%
  dplyr::select(Model, parameter, val) %>%
  spread(Model, val) %>% arrange(as.character(parameter))
SFE <- simu2 %>% group_by(Model, parameter) %>%
  summarize(meanEst = mean(estimate), sdEst = sd(estimate)) %>%
   mutate(val = paste0(sprintf("%.3f",meanEst), " ", "(", sprintf("%.3f", sdEst), ")")) %>%
  select(Model, parameter, val) %>%
  dplyr::select(Model, parameter, val) %>%
  spread(Model, val) %>% arrange(parameter)
@

% table of fitted M1 M2 M3
\begin{table}
\scalebox{0.9}{
\begin{tabular}{lcccccc }

  & \multicolumn{3}{c}{Friendship Data} & \multicolumn{3}{c}{Senate Data} \\
  \hline
  & M1 & M2 & M3 & M1 & M2 & M3 \\
  \hline
  $\alpha_1$ & \Sexpr{SFE$M1[1]} & \Sexpr{SFE$M2[1]} & \Sexpr{SFE$M3[1]} & \Sexpr{SSE$M1[1]} & \Sexpr{SSE$M2[1]} & \Sexpr{SSE$M3[1]} \\
  $\alpha_2$ & \Sexpr{SFE$M1[2]} & \Sexpr{SFE$M2[2]} & \Sexpr{SFE$M3[2]} & \Sexpr{SSE$M1[2]} & \Sexpr{SSE$M2[2]} & \Sexpr{SSE$M3[2]}  \\
  $\alpha_3$ & -- & -- & -- & \Sexpr{SSE$M1[3]} & \Sexpr{SSE$M2[3]} & \Sexpr{SSE$M3[3]} \\
  \hline
  $\beta_1$ & \Sexpr{SFE$M1[3]} & \Sexpr{SFE$M2[3]} & \Sexpr{SFE$M3[3]} & \Sexpr{SSE$M1[4]} & \Sexpr{SSE$M2[4]} & \Sexpr{SSE$M3[4]} \\
  $\beta_2$ & \Sexpr{SFE$M1[4]} & \Sexpr{SFE$M2[4]} & \Sexpr{SFE$M3[4]} & \Sexpr{SSE$M1[5]} & \Sexpr{SSE$M2[5]} & \Sexpr{SSE$M3[5]}\\
$\beta_3$ & -- & \Sexpr{SFE$M2[5]} & -- & -- & \Sexpr{SSE$M2[6]} & -- \\
  $\beta_4$ & -- & --  & \Sexpr{SFE$M3[6]} & -- & -- & \Sexpr{SSE$M3[7]} \\
  \hline
\end{tabular}
}
\caption{\label{tab:meanests} The means (std.\ dev.)\ of parameter values estimated from repeated fittings of $M1, M2, M3$ to the two example data sets.}
\end{table}

\noindent \textbf{Fitting the same model to different data:} Finally, we view collections of models by fitting the same models to different data to understand how the model behaves in different contexts. As we did for the friendship data, we estimated the parameters for M1, M2, and M3, when fit to the senate data (see Figure \ref{fig:senateplot}) 100 times each. The means and standard deviations of the repeated parameter estimates for all six model-data combinations are given in Table~\ref{tab:meanests}, while corresponding density plots of the objective function parameters are given in Figure~\ref{fig:compareModels} in the Appendix. Looking at the estimates for both data sets, a few patterns emerge. First, we can see the same relationship between the outdegree parameter, $\beta_1$, and the reciprocity parameter, $\beta_2$ in both data sets and across all three models. The estimates of $\beta_1$ are all negative and range from about -5 and -3, while the estimates of $\beta_2$ are all positive and range from about four to five. %In addition, the outegree parameter, $\beta_1$, \textit{decreases} when $\beta_3$ is included, while the reciprocity parameter, $\beta_2$ increases. This inverse relationship implies that the girls prefer to form small, closely knit friend groups, as opposed to being popular and having many friends. Tight-knit friend groups are more encouraged by model M2 than by models M1 and M3.
This suggests that in both data sets, nodes prefer outgoing ties that are reciprocated to those that are not. When we consider the context, this seems obvious: the students want to have close mutual friendships, and senators want mutual support for their bills. We are also concerned, however, with the extremely high correlation between the estimates of $\beta_1, \beta_2$. In all six model-data combinations, the absolute correlation between $\hat{\beta}_1, \hat{\beta}_2$ is greater than 0.93. %We explore the relationship between $\beta_1$ and $\beta_2$ further in Section~\ref{sec:samemodsamedat}.
% We have seen how the same model forms behave in different context, with some  differences and some similarities. 
After seeing the high correlation between parameter estimates in very different data contexts, we might question the guideline that both parameters need to be included in the objective function. 

In Figure~\ref{fig:distests}, we saw that the inclusion of $\beta_3$ for the friendship data has a noticeable effect on the other parameters in the model. In Figure~\ref{fig:compareModels}, however, we see that this effect is not present in the senate data. Furthermore, the estimates of $\beta_4$ for the senate data are near zero, suggesting that this effect, which considers indirect ties, is not informative for the senate data. In contrast for the friendship data, the mean estimate of $\beta_4$ is about -7.5, which suggests that indirect ties are \textit{strongly} discouraged from forming. Again, this is fairly intuitive given the context: teenage girls likely want to have direct friendships with each other. Senators, on the other hand, have to wheel and deal to pass their legislation, and indirect relationships may be beneficial at times. By examining the results of models fitted to different data, we better understand the interpretation of the objective function parameters, and we have learned of some potential parameter correlation problems affecting the fit of the models. All in all, we have used four different ways of viewing collections of models, which have given us insight into the behavior of SAOMs.

%Although this indirect relationship variable does not help explain the senate collaboration structure, it does help clarify the teenage friendship structure.

%We look at collections produced ``from exploring the space of all possible models," ``by varying model settings", ``by fitting the same model to different datasets", and ``by treating each iteration as a model" \citep[p.~210-11]{modelvis}. Because fitting a SAOM requires MCMC methods, we also fit the same model many times, resulting in distributions of fitted parameters for one data set.


% taken out.
<<getdatam, eval = FALSE>>=
nulls <- read.csv("data/distribution_null_model.csv")
alt <- read.csv("data/distribution_jumpTT_model.csv")
alt2nd <- read.csv("data/distribution_dblpairs_model.csv")
means <- simu2 %>% group_by(Model, parameter) %>% summarize(
  means = mean(estimate)
)
null_mod_sv <- as.numeric(nulls[,-c(1,6,7)] %>% summarise_all(funs(mean)))
alt_mod_sv <- as.numeric(alt[,-c(1, 7,8)] %>% summarise_all(funs(mean)))
alt_mod2_sv <- as.numeric(alt2nd[,-c(1,7,8)] %>% summarise_all(funs(mean)))
@

% taken out
<<corplotsSenate, eval = FALSE>>=
senM1est <- read.csv("data/congress/senateM1ests.csv")
senM2est <- read.csv("data/congress/senateM2ests.csv")
senM3est <- read.csv("data/congress/senateM3ests.csv")
senM4est <- read.csv("data/congress/senateM4ests.csv")
senM5est <- read.csv("data/congress/senateM5ests.csv")
names(senM1est) <- c("alpha1", "alpha2", "alpha3", "beta1", "beta2")
names(senM2est) <- c("alpha1", "alpha2", "alpha3", "beta1", "beta2", "beta3")
names(senM3est) <- c("alpha1", "alpha2", "alpha3", "beta1", "beta2", "beta4")
names(senM4est) <- c("alpha1", "alpha2", "alpha3", "beta1", "beta2", "beta5")
names(senM5est) <- c("alpha1", "alpha2", "alpha3", "beta1", "beta2", "beta6")
senM1est$Model <- "M1"
senM2est$Model <- "M2"
senM3est$Model <- "M3"
senM4est$Model <- "M4"
senM5est$Model <- "M5"
senM1est$Sim <- 1:nrow(senM1est)
senM2est$Sim <- 1:nrow(senM2est)
senM3est$Sim <- 1:nrow(senM3est)
senM4est$Sim <- 1:nrow(senM4est)
senM5est$Sim <- 1:nrow(senM5est)
senM1est2 <- senM1est %>% gather(parameter, estimate, alpha1:beta2)
senM2est2 <- senM2est %>% gather(parameter, estimate, alpha1:beta3)
senM3est2 <- senM3est %>% gather(parameter, estimate, alpha1:beta4)
senM4est2 <- senM4est %>% gather(parameter, estimate, alpha1:beta5)
senM5est2 <- senM5est %>% gather(parameter, estimate, alpha1:beta6)
senM45est <- rbind(senM4est2, senM5est2)
senM123est <- rbind(senM1est2, senM2est2, senM3est2)
senM123est %>% spread(parameter, estimate)-> senM123est2

senM45est %>% spread(parameter, estimate)-> senM45est2
ggpairs(senM123est2, columns = 3:9, ggplot2::aes(colour=Model, alpha = .5))
@

\section{Explore algorithms, not just end result}\label{sec:algorithms}

The last model visualization principle, "explore the process of model fitting," is crucial for understanding SAOMs \citep{modelvis}. When fitting a SAOM in \texttt{RSiena}, several simulation steps are hidden from the user. One of the hidden steps in the SIENA MoM algorithm is the \textit{microstep} process. At each microstep, an \textit{ego} node is selected to make a change, and the chosen ego node randomly makes one change in its ties to maximize its objective function. Between two network observations $x(t_m)$ and $x(t_{m+1})$, there can be dozens, hundreds, or even thousands of microsteps, depending on the size of the network and the number of differences between two network observations. We want to simulate and view these in-between steps in order to better understand the behavior of the underlying continuous-time Markov chain. Hiding the microsteps is practical and efficient if the goal is to fit a model to data, obtain parameter estimates, and draw conclusions or make predictions. But in order to understand \textit{how} the models are fit, we need to extract and explore the different steps of the process. We use these steps to visually explore the algorithms, so that we are visualizing the process, and not just the main result.  

%A series of microsteps is obtained by simulating from the model in its current state, $x(t_m)$ with current parameter values $\mathbf{\theta}_{0} = \{\alpha_{1_0}, \dots \alpha_{{(m-1)}_0}, \beta_{1_0}, \dots, \beta_{K_0}\}$, to the next state, $x(t_{m+1})$. One tie changes at a time until the simulated network has achieved the same number of differences, $C$, from $x(t_m)$ as $x(t_{m+1})$, where $C$ is definined in Equation~\ref{eq:cdiff}. 

%This simulation process follows the steps of the continuous-time Markov chain. Each tie change in the CTMC is referred to as one ``microstep".  The options for change are removing a current tie, adding a new tie, or making no change at all. 

<<makinggif, fig.cap='A selection of images in the microstep animation. The selected edges and nodes are emphasized by changing the size and the color, then when edges are removed, they fade out, shrinking in size, while the nodes change color and shrink to blend in with the other nodes. Frames 15-28 show a an edge disappearing, while frames 71-84 show an edge appearing.', message = FALSE, warning = FALSE, fig.height=2.5>>=
# Need old version of dplyr to get this to work:
# if (!require("dplyr050",character.only = TRUE)) {
#       devtools::install_github("sctyner/dplyr050")
# }
# library(dplyr050)
friend.data.w1 <- as.matrix(read.table("data/s50_data/s50-network1.dat"))
friend.data.w2 <- as.matrix(read.table("data/s50_data/s50-network2.dat"))
friend.data.w3 <- as.matrix(read.table("data/s50_data/s50-network3.dat"))
drink <- as.matrix(read.table("data/s50_data/s50-alcohol.dat"))
fd2.w1 <- friend.data.w1[20:35,20:35]
fd2.w2 <- friend.data.w2[20:35,20:35]
fd2.w3 <- friend.data.w3[20:35,20:35]
friendshipData <- array(c(fd2.w1, fd2.w2,fd2.w3), dim = c(16, 16, 3))

# running Siena
#friendship <- sienaDependent(friendshipData)
#alcohol <- varCovar(drink[20:35,])
#mydata <- sienaDataCreate(friendship, alcohol)
#myeffnull <- getEffects(mydata)
#myalgorithm <- sienaAlgorithmCreate(projname = 's16_3')

ansnullchains %>%
  dplyr::filter(period == 1) %>%  #only look at chains from wave 1 to wave 2
  #dplyr::group_by(rep) %>%
  dplyr::select(rep, from = ego, to = alter) %>%
  dplyr::mutate(val = as.numeric(!from == to),
         from = paste0("V", parse_number(from)+1), # make the chains
         to = paste0("V", parse_number(to)+1)) -> ansnullchainsw1w2
colnames(fd2.w1) <- paste0("V", 1:16)
rownames(fd2.w1) <- paste0("V", 1:16)
wave1friends <- fortify(as.adjmat(fd2.w1))
ms1 <- netvizinf::listMicrosteps(dat = wave1friends,
                      microsteps = dplyr::filter(ansnullchainsw1w2, rep == 1))
microsteps <- ms1[1:20]

microsteps_df <- getMicrostepsDF(microsteps)
pte <- pretween_edges(microsteps = microsteps)
ptv <- pretween_vertices(pte = pte, layoutparams = list(n = 16))
# step 1 is remove, step 9 is add.
# get tweened data for all microsteps
pall <- tween_microsteps(ptv, pte, microsteps_df)

static_pall <- dplyr::filter(pall, .frame %in% c(15,20,22,24,26:28, 71:73, 75, 79, 81, 84))

static_pall$type <- NA
static_pall$type[which(static_pall$.frame %in% c(15,20,22,24,26:28))] <- "Remove Edge"
static_pall$type[which(static_pall$.frame %in% c(71:73, 75, 79, 81, 84))] <- "Add Edge"

set.seed(34569234)
froms <- static_pall %>% dplyr::select(id = from, microstep, x = from.x,
        y = from.y, ms, .frame, size, color) %>% unique
tos <- static_pall %>% dplyr::select(id = to, microstep, x = to.x, y = to.y,
        ms, .frame) %>% unique
tos <- dplyr::left_join(tos, froms)
nodedata <- rbind(froms, tos) %>% unique
nodedata <- nodedata %>% tidyr::replace_na(replace = list(size = 1,
        color = "#333333"))
to_ani <- ggplot() +
  geom_curve(data = static_pall, aes(x = from.x,
        y = from.y, xend = to.x, yend = to.y, frame = .frame,
        colour = ecolor, size = esize), curvature = 0.1, arrow = arrow(angle = 20,
        length = unit(0.1, "inches"))) +
  geom_point(data = nodedata,
        aes(x = x, y = y, colour = color, size = size, frame = .frame)) +
  scale_colour_identity() + scale_size_identity() + theme_void() +
        coord_fixed()
to_ani +
  facet_wrap(~.frame, nrow=2, labeller = 'label_both') +
  ThemeNet +
  theme(panel.background = element_rect(color = 'black'))
@


To visualize the underlying algorithm, we animate the microsteps that form the transition steps of the CTMC from wave 1 to wave 2 of the small friendship network example shown in Figure~\ref{fig:alldataviz}. The microstep animation, some frames of which are shown in Figure~\ref{fig:makinggif}, represent one possible path from wave 1 to wave 2 according to the model M1. We show each change in the network as follows: we first increase the size and change the color of the ego node to focus attention to its ties. Then, the changing tie either fades in or fades out. If there are no changes at a particular microstep, the microstep does not appear. Some frames of the animations are shown in Figure~\ref{fig:makinggif}, and the full movie, created with \texttt{tweenr} and \texttt{gganimate} can be viewed at \url{https://vimeo.com/240089108} \citep{tweenr, gganimate}. Movies similar to this animation were used to visualize the changes of dynamic networks in \citet{dynnetviz}. Frames 15-28 of Figure~\ref{fig:makinggif} show a deleted tie, while frames 71-84 show an added tie. For both changes, the ego node is emphasized with color and size. When a tie is removed, it is emphasized with the same color and size as the node. As the animation proceeds, the tie shrinks and disappears as the ego node shrinks and changes color back to black. If a tie is being added, the it appears red from nothing, grows large for emphasis, then changes color and size to match the rest of the edges. In this network animation, we see the modeled steps of the \textit{unobserved} CTMC process between two observed networks. We see each part of the SAOM come into play: we first see the selection of nodes to change via the rate function, then we see the result of the actor maximizing its objective function by either deleting or adding a node. Finally, the layout of the nodes changes as edges are removed or added, which demonstrates how the overall network structure changes with each of the individual tie changes. Combining the tie changes in this animation makes the CTMC process the focus and demonstrates the underlying change mechanism of the dynamic network according to the chosen model. 

<<adjmatorder, message = FALSE, warning = FALSE, fig.cap="On the left, the starting friendship network represented in adjacency matrix form, ordered by node ID. On the right, the same adjacency matrix is presented after ordering the vertices by one repetition of the microstep simulation process from wave one to wave two.", fig.width=8, fig.height=3, fig.align='center'>>=
# first present the original wave 1 adj mat, then present the adj mat after being ordered by the 1st pc of the big adjmat A.
fd.w12 <- data.frame(fd2.w1)
fd.w12 <- data.frame(fd2.w1)

numordersf <- fd.w12 %>%
  mutate(from = names(fd.w12)) %>%
  gather(key = to ,value = value, -from)

pcaordersf <- numordersf

numordersf$from <- factor(numordersf$from)
levels(numordersf$from) <-  1:16
numordersf$to <- factor(numordersf$to)
levels(numordersf$to) <- 1:16

p1 <- ggplot(data = numordersf, aes(y = rev(from), x = to)) + geom_tile(color = 'black', aes(fill = as.factor(value))) +
  scale_fill_manual(values = c('white', 'black')) +
  scale_x_discrete(labels = 1:16, position = "top") +
  scale_y_discrete(labels =16:1, name = "from")  +
  ThemeNoNet +
  theme(legend.position = 'none', panel.grid = element_blank(), 
        panel.border = element_blank(), 
        axis.text.y = element_text(margin = margin(r=0))) +
  coord_fixed()

# histogram of number of MS in 1000 reps
# sfmsall %>% group_by(rep) %>% summarize(noMs = max(ms)) %>% ggplot() + geom_histogram(aes(x = noMs), binwidth = 5)

sfmsall <- read_csv("data/smallfriendsmicrosteps.csv")

sfmsall %>%
  group_by(from, to) %>%
  filter(rep == 1) %>%
  summarise(count = n()) -> sfmsall_summ
sfmsall_summ <- sfmsall_summ %>% ungroup %>% mutate(from = parse_number(from), to = parse_number(to))
sfmsall_summ$from <- as.factor(sfmsall_summ$from)
levels(sfmsall_summ$from) <- paste0('V', 1:16)
sfmsall_summ$to <- as.factor(sfmsall_summ$to)
levels(sfmsall_summ$to) <- paste0('V', 1:16)
#sfmsall_summ <- sfmsall_summ %>% ungroup() %>% mutate(from = as.factor(from), to = as.factor(to))
sfmsall_summ %>%
  spread(to, count, fill = 0) %>%
  data.frame -> allmsrep1_adjmat
allmsrep1_adjmat$V16 <- 0
allmsrep1_adjmat <- allmsrep1_adjmat[,-1]
allmsrep1_adjmat <- as.matrix(allmsrep1_adjmat)
rownames(allmsrep1_adjmat) <- paste0("V", 1:16)

pca1 <- prcomp(allmsrep1_adjmat)
order1 <- names(sort(pca1$rotation[,1], decreasing = T))
order1 <- gsub("V", "",order1)
# order2 <- names(sort(abs(pca1$rotation[,1]), decreasing = T))

pcaordersf$from <- as.factor(pcaordersf$from)
levels(pcaordersf$from) <- gsub("V", "", levels(pcaordersf$from))
pcaordersf$from <- factor(pcaordersf$from , levels = levels(pcaordersf$from)[match( order1,levels(pcaordersf$from) )])
pcaordersf$to <- as.factor(pcaordersf$to)
levels(pcaordersf$to) <-gsub("V", "", levels(pcaordersf$to))
pcaordersf$to <- factor(pcaordersf$to , levels = levels(pcaordersf$to)[match( order1,levels(pcaordersf$to) )])

p2 <- ggplot(data = pcaordersf, aes(x = to, y = from)) +
  geom_tile(color = 'black', aes(fill = as.factor(value))) +
  scale_fill_manual(values = c('white', 'black'), name = "Tie is...", labels = c("not present","present")) +
  scale_x_discrete(position = "top") +
  #scale_y_discrete(name = "from",labels = rev(str_replace(order1, "V", ""))) +
  ThemeNoNet +
  coord_fixed()
p2a <- ggplot(data = pcaordersf, aes(x = to, y = from)) +
  geom_tile(color = 'black', aes(fill = as.factor(value))) +
  scale_fill_manual(values = c('white', 'black')) +
  scale_x_discrete(position = "top") +
  scale_y_discrete(name = "from",labels = rev(str_replace(order1, "V", ""))) +
  ThemeNoNet + theme(legend.position = "none") + 
  coord_fixed()
legendp2 <- cowplot::get_legend(p2)
ggdraw() + draw_plot(p1, width = .4) +  draw_plot(p2a, width = .4, x=.4) + 
  draw_grob(legendp2,x=.8, y=.25, width = .2, height = .4)
# cowplot::plot_grid(p1,p2a,legendp2, ncol=3)
@

We continue to use animation to view the changing structure of the network via adjacency matrix visualizations. In the first adjacency matrix visualization in Figure~\ref{fig:adjmatorder}, we use the node ID to order the rows and columns. This arbitrary ordering does not easily convey the underlying structure of the network. We use \textit{matrix seriation} to reorder the rows and columns of the adjacency matrix visualization so that the structure of the network is apparent \citep{seriation}. %This lack of perceived structure would contribute to further confusion as the network chages, so we adjust the ordering so that the viewer can better perceive the structure of the network. This process is known as matrix seriation 
To seriate the matrix, we first construct a cumulative adjaceny matrix, $\mathbf{A}^{cum}$, for the series of microsteps simulating the network from $x(t_m)$ to $x(t_{m+1})$. A single entry in the cumulative adjacency matrix, $\mathbf{A}^{cum}_{ij}$, is the total number of times the edge $i \rightarrow j$ appears in the network from the initial observation, $x(t_m) \equiv X(0)$ to the final result of the last microstep, $X(R)$, where $R$ is the total number of microsteps taken:
\begin{equation}\label{eq:acum}
\mathbf{A}^{cum}_{ij} = \sum_{r = 0}^R X_{ij}(r). 
\end{equation}
We then perform a principal component analysis (PCA) on $\mathbf{A}^{cum}$, and use the values of the first principal component to order the vertices on the $x$ and $y$ axes for the adjacency matrix animation. The resulting visualization for the first wave of data is shown in Figure~\ref{fig:adjmatorder}. %we present the first wave of data ordered by the arbitrary node ID order alongside the seriated adjacency matrix ordered by the first principal component loading on the cumulative adjacency matrix, $\mathbf{A}^{cum}$. 
Using PCA on $\mathbf{A}^{cum}$ to order the rows and columns of the adjacency matrix visualization clearly shows the two distinct connected components in the first wave of the network, which are difficult to find in the visualization ordered by node ID. We continue to use the seriated matrix in another animation of the microstep process, which is available at \url{https://vimeo.com/240092677}. A few frames of the video are shown in Figure~\ref{fig:adjmatani}. In the animation, a square appears or disappears as that edge appears or disappears in the microstep process. %Through this animation, we can see edges appearing, and then later on disappearing. 
This visualization also shows the microsteps where the nodes do not make a change. From start to finish, we see network evolve according to the model: sometimes ties are removed right after they are created, and sometimes several ties are removed or created in a row. The adjacency matrix animation shows the many ways the algorithm explores the data space. By viewing this animation, we can gain a better understanding of the dynamics of this model. 

<<adjmatani, fig.cap='A selection of frames from the adjacency matrix visualization animation for one series of microsteps. The node labels are removed to declutter the figure. Starting at frame 0, there are two clearly connected components: one in the bottom left corner, and one in the top right corner. By the end, the component in the bottom left has spread out and become sparsely connected, while the top right component has shrunk, but remains closely connected.', warning = FALSE, message = FALSE, results='hide', fig.height=3, out.width="\\linewidth">>=
adjmatall <- matrix(0, nrow = 16, ncol = 16)
rownames(adjmatall) <- paste0("V", 1:16)
colnames(adjmatall) <- paste0("V", 1:16)
dat2 <- sfmsall %>% filter(rep == 1)
adjmatall2 <- NULL
for(i in 0:55){
  MS <- dat2[dat2$ms == i,c("from", "to")]
  myadjmat <- adjmatall
  for(j in 1:nrow(MS)){
    myadjmat[MS[j,"from"][[1]],MS[j,"to"][[1]]] <- 1
  }
  adjmatall2 <- rbind(adjmatall2, myadjmat)
}

adjmatall2 <- cbind(adjmatall2, rep(0:55, each = 16))
rownames(adjmatall2) <- NULL
adjmatall2 <- as.data.frame(adjmatall2)
adjmatall2$from <- rep(paste0("V", 1:16), 56)
names(adjmatall2)[17] <- "ms"
adjmatall2 %>% gather(to, val, V1:V16) -> adjmatall3

adjmatall3$to <- as.factor(adjmatall3$to)
levels(adjmatall3$to) <- gsub("V", "", levels(adjmatall3$to))
adjmatall3$from <- as.factor(adjmatall3$from)
levels(adjmatall3$from) <- gsub("V", "", levels(adjmatall3$from))
adjmatall3$to <- factor(adjmatall3$to, levels = order1)
adjmatall3$from <- factor(adjmatall3$from, levels = order1)

adjmatall3$frame <- adjmatall3$ms

dat3 <- adjmatall3 %>% filter(ms %in% c(0,2,4,6,8,10,12,43,45,47,49,51,53,55))

dat4 <- dat3 %>% select(-ms) %>% spread(frame, val)
dat5 <- dat4
for(i in 4:16){
  for(j in 1:nrow(dat5)){
    if(dat4[j,i] != dat4[j,i-1]){
      if(dat4[j,i] > dat4[j,i-1]){
        dat5[j,i] <- 2
      } else { dat5[j,i] <- 3}
    }
  }
}
#Sys.time() - p

dat6 <- dat5 %>% gather(frame, val, 3:16) %>% mutate(frame = parse_number(frame))

ggplot() +
  geom_tile(data = dat6, aes(x = to, y = from, fill = as.factor(val), color=as.factor(val))) +
  scale_fill_manual(values = c("white", "black", "red", "white"), 
                    labels = c("not present", "present", "added", "removed"), 
                    name = "Tie is...") +
  scale_color_manual(values = c("#969696", "#969696", "red", "red"), 
                    labels = c("not present", "present", "added", "removed"), 
                    name = "Tie is...") + 
  geom_tile(data = dat6 %>% filter(val == 3), aes(x = to, y = from), fill = NA, color = 'red', size = .25) +
  scale_x_discrete(position = "bottom") +
  labs(x = "alter node (j)", y = "ego node (i)") +
  ThemeNoNet +
  theme(axis.text.x.bottom = element_blank(),
        axis.text.y = element_blank(), 
        strip.text = element_text(size = rel(.75)),
        strip.text.x = element_text(size = rel(.75)),
        strip.text.y = element_text(size = rel(.75)),
        legend.text = element_text(size = rel(.75)), 
        legend.title = element_text(size = rel(.75)), 
        legend.position = "bottom") +
  coord_fixed() +
  facet_wrap(~frame, labeller = 'label_both', nrow = 2)
#}
#plots
@

% removed
<<histogramalters, fig.width = 7, fig.height=9, fig.cap="Each panel shows the theoretical (as lines) and empirical (as points) probabilities of the chosen ego node changing its tie to each of the other nodes. The color of the line indicates whether the tie from the ego to the alter node is being added, removed, or if there is no change to the network in this step.", eval = FALSE>>=
ansnullchains %>% group_by(period, rep) %>%
  mutate(ms = row_number()) %>%
  filter(ms == 1, period == 1) %>%
  mutate(alterProb = exp(logAlterProb),
         ego = as.numeric(as.character(ego)),
         alter = as.numeric(as.character(alter))) %>%
  ungroup() %>%
  group_by(ego, alter) %>%
  summarize(count = n(), alterProb = unique(alterProb)) %>% ungroup() %>%
  group_by(ego) %>%
  mutate(csum = cumsum(alterProb)) %>%
  arrange(ego, csum) %>%
  mutate(totego = sum(count),
         empprob = count / totego) -> cumprobs
cumprobs$ego <- factor(paste0("V",cumprobs$ego+1), levels = paste0("V", 1:16))
cumprobs$alter <- factor(paste0("V",cumprobs$alter+1), levels = paste0("V", 1:16))

# is the tie that's added new, removed, or does the network stay the same

changes <- cumprobs %>% dplyr::select(ego, alter) %>% unique
wave1friends <- fortify(as.adjmat(fd2.w1))
wave1friends2 <- wave1friends[,-3]
wave1friends2$type <- "remove"
changes2 <- left_join(changes, wave1friends2, c("ego" = "from", "alter" = "to"))

changes2$type[is.na(changes2$type)] <- ifelse(changes2$ego[is.na(changes2$type)] == changes2$alter[is.na(changes2$type)], "noChange", "add")

cumprobs %>% left_join(changes2) -> changes3

changes3$ego <- factor(changes3$ego, levels = paste0("V", 1:16), labels = 1:16)
changes3$alter <- factor(changes3$alter, levels = paste0("V", 1:16), labels = 1:16)

ggplot(data = changes3) +
  geom_segment(aes(x = alter, xend = alter, y = 0, yend = empprob, color = type), size = 2, alpha = .75) +
  geom_point(
    aes(x = alter , y = alterProb), shape = 4, size = 2, alpha = .75) +
  labs(x = "alter node\n", y = "\nprobability of alter node") +
  coord_flip() +
  facet_wrap(~ego, labeller = "label_both") +
  ThemeNoNet
#
# cumprobs %>% ggplot() + geom_histogram(aes(x = alter, weight = count), color = 'black', fill = 'white', binwidth = 1) + scale_x_continuous(labels = paste0("V",1:16), breaks = 0:15) + coord_flip() + facet_wrap(~ego, labeller = "label_both") + theme_gray()

# cprobslist <- split(cumprobs, cumprobs$ego)
# plots <- list()
# for(i in 1:length(cprobslist)){
#   cprobslist[[i]] %>%
#   ggplot() +
#   geom_histogram(aes(x = as.numeric(as.character(alter)),
#                      y = alterProb), stat = 'identity', fill = 'white', color = 'black') +
#   geom_line(aes(x = as.numeric(as.character(alter)),
#                 y = csum)) +
#   geom_point(aes(x = as.numeric(as.character(alter)),
#                 y = csum)) +
#   scale_x_continuous(limits = c(-.5, 15), breaks = 0:15, labels = 0:15) +
#   geom_hline(yintercept = 1, color = 'red', linetype = 'dashed') +
#   labs(x = "alter", y = (p[ij]), title = paste("Ego node:", names(cprobslist)[i])) + # ylab((p[ij])) +
#   theme_gray() +
#     theme(plot.title = element_text(size = 10),
#           axis.title = element_text(size = 8),
#           plot.margin = unit(rep(0,4),"pt")) -> p
#   plots[[i]] <- p
# }
# grid.arrange(plots[[1]], plots[[2]], plots[[3]], plots[[4]],
#              plots[[5]], plots[[6]], plots[[7]], plots[[8]],
#              #plots[[9]], plots[[10]], plots[[11]], plots[[12]],
#              #plots[[13]], plots[[14]], plots[[15]], plots[[16]],
#              ncol = 2, padding = unit(0, 'line'))

@

<<heatmap, fig.align='center', fig.cap='Adjacency matrix visualizations showing the theoretical (left) and empirical (right) transition probabilities for the first microstep observed in 1,000 simulations. The ego node is on the y-axis, and the alter node is on the x-axis. There are many ties with empirical probability zero.', fig.height=2.5>>=

ansnullchains %>%
  filter(period == 1) %>%
  group_by(rep) %>%
  mutate(msno = row_number()) %>%
  filter(msno == 1) %>%
  mutate(ego = as.numeric(as.character(ego))+1,
         alter = as.numeric(as.character(alter))+1,
         prob = exp(logAlterProb)) -> probsDat

ansnullchains %>%
  filter(period == 1) %>%
  group_by(rep) %>%
  mutate(msno = row_number()) %>%
  filter(msno == 1) %>%
  ungroup() %>% group_by(ego, alter) %>% 
  tally() %>% ungroup() %>% mutate(empprob = n/1000) %>% 
  mutate(ego = as.numeric(as.character(ego))+1,
         alter = as.numeric(as.character(alter))+1) ->empprobsDat

# probsDat %>% group_by(ego, alter) %>% summarize(count = n()) %>% dim
# 103, 3
p1 <- ggplot(data = probsDat,
       aes(y = factor(ego, levels = rev(order1)),
             x = factor(alter, levels = order1),
             fill = prob)) +
  geom_tile(color = 'black') +
  scale_fill_gradient(low = 'white', high = 'black', limits = c(0,1), name = "theoretical\nprobability", labels = c("0","",".5","","1")) +
  labs(y = "ego", x = "alter")+
  scale_x_discrete(position = 'top') +
  #scale_y_discrete(labels = rev(str_replace(order1, "V", ""))) +
  ThemeNoNet + 
  theme(panel.grid = element_blank(), 
          axis.text.x.top = element_text(size=rel(.7)),
          axis.text.y.left = element_text(size=rel(.7)),
          legend.position = "bottom", 
          legend.title = element_text(size=rel(.7)), 
          legend.text = element_text(size=rel(.7)), 
          legend.key.height = unit(.5, "cm"), 
          legend.margin=margin(0,0,0,0),
          legend.box.margin=margin(-10,-10,-5,-10)) +
  coord_fixed() + 
  ggtitle("Theoretical Probabilities")
p2 <- ggplot(data = empprobsDat,
       aes(y = factor(ego, levels = rev(order1)),
             x = factor(alter, levels = order1),
             fill = empprob)) +
  geom_tile(color = 'black') +
  scale_fill_gradient(low = 'white', high = 'black', name = "empirical\nprobability", labels = c(".02", ".04", ".06"), breaks = c(.02, .04, .06)) +
  labs(y = "ego", x = "alter")+
  scale_x_discrete(position = 'top') +
  #scale_y_discrete(labels = rev(str_replace(order1, "V", ""))) +
  ThemeNoNet + 
    theme(panel.grid = element_blank(), 
          axis.text.x.top = element_text(size=rel(.7)),
          axis.text.y.left = element_text(size=rel(.7)),
          legend.position = "bottom", 
          legend.title = element_text(size=rel(.7)), 
          legend.text = element_text(size=rel(.7)), 
          legend.key.height = unit(.5, "cm"), 
          legend.margin=margin(0,0,0,0),
          legend.box.margin=margin(-10,-10,-5,-10)) +
  coord_fixed() + 
  ggtitle("Empirical Probabilities")
plot_grid(p1, p2)
#gridExtra::grid.arrange(p1, p2, nrow = 1)
@


We also attempt to better understand the microstep process by visualizing the theoretical and observed transition probabilites for the first microstep in the process. These probabilities are defined in Equation~\ref{eq:pij}. We only do the first step of many because each microstep is dependent on the previous network state. %By ego node state, we mean the current set of incoming and outgoing ties to the ego node, $i$. Put more precisely, let the vector $X_i = (\{x_{ik}\}, \{x_{ki}\})_{k \neq i} \in \{0,1\}^{2(n-1)}$ represent the set of all tie variables involving node $i$ in the current network. For $p_{ij}$ to be comparable for any two network states $x$ and $x'$, the value of the vector $X_i$ must be the same in both states. It is possible to incorporate this information into our visualizations, but for now we look at only the first step, because we know without needing any complicated conditioning steps that the previous node state $X_i$ is identical in all cases. 
There are 1,000 transitions to examine: the first microstep taken in each one of the simulations. %Another way to view these transition probabilities is through the adjacency matrix visualization. 
We use another seriated adjacency matrix visualization to show theoretical and empirical transition probabilities of the first microstep in Figure~\ref{fig:heatmap} for each tie that was the first change in one of the simulations. The figure is noticeably sparse: %showing the lack of coverage in the model.
of the $16\cdot 16 = 256$ possible choices for the CTMC to make, only 103, or about 43\%, are made in the 1,000 simulated chains. This leaves %reinforces what we saw in Figure~\ref{fig:histogramalters}, where there are many paths not taken. This effect could only be exacerbated as more steps are taken in the CTMC, leading to 
a large area of data space unexplored by the first step in the SAOM model fitting process. We present additional evidence for lack of coverage in Figure~\ref{fig:alledgests} the Appendix. The Figures~\ref{fig:heatmap} and \ref{fig:alledgests} show that the model M1 and the the corresponding fitting process do not explore the data space enough to adequately capture the network change mechanism, a finding we were only able to discern thanks to model visualization techniques. 
%Put more precisely, let the vector $X_i = (x_{ij}|_{j \neq i}, x_{ji}|_{j \neq i}$ for $i \neq j \neq k \neq \ell \in \{1, \dots, n\}$, $p_{ij}$ is not necessarily the same \st{ARGH NOT SURE THIS IS TRUE } if the current state of the network achieved from the first microstep, denoted $X(1)$, is $X(1) = x(i \leadsto \ell)$ than is different in the case of $x_{ij}$ changing in the first step from the case of $x_{ij}$ not changing in the first step.
%In Figure~\ref{fig:histogramalters}, we present each ego node and the resulting probabilities of tie changes. The probability shown by the bars is the theoretical probability, according to the objective function, of the ego node changing its tie to the alter node, while the probability shown by the points is the empirical probability of that change being made. The empirical probability is calculated by counting all instances of the ego node first, then computing the proportion of each different alter node change. In most cases, they are almost identical, which demonstrates that the algorithm is performing as expected. There are, however, many steps that are never taken. Some ego nodes, like 1, 4, and 16, really explore the space of all possible outgoing ties. On the other hand, nodes like 13 and 15 explore very few possible outgoing ties.  It is unclear why this would be happening; it may have something to do with the ties in wave 1, where 13 and 15 have multiple connections, while 1 and 16 have none, but there are counterexamples of well connected nodes, such as 12, that explore the space more. In addition, we can see that the ties changed most often are removing ties, not adding ties. This matches the overall pattern of the data: we see the most ties in the first wave of data, and the least ties in the third wave.



% removed 
<<allsteps, fig.cap="Two simulations (out of 1,000) of the microstep process from wave 1 to wave 2. The $x$ axis is the microstep number, with step 0 representing the first wave of data and the final step representing the second wave of data. We can see that many edges are underrepresented in this process: they are in the second wave, but never appear in the microsteps.", fig.height=2.5, eval = FALSE>>=
smallfriends <- read_csv("data/smallfriends4Geomnet.csv")
names(smallfriends)[1:2] <- c("from", "to")
ansnullchains %>%
  dplyr::filter(period == 1) %>%  #only look at chains from wave 1 to wave 2
  #dplyr::group_by(rep) %>%
  dplyr::select(rep, from = ego, to = alter) %>%
  dplyr::mutate(val = as.numeric(!from == to),
         from = paste0("V", parse_number(from)+1), # make the chains
         to = paste0("V", parse_number(to)+1)) -> ansnullchainsw1w2
msall1 <- netvizinf::listMicrosteps(dat = wave1friends,
                      microsteps = filter(ansnullchainsw1w2, rep == 1))
msall1_df <- plyr::rbind.fill(lapply(X = seq_along(msall1), FUN = function(i, x) {
        x[[i]]$ms <- i - 1
        return(x[[i]])
    }, msall1))
msall1_df <- msall1_df %>% dplyr::select(from, to, ms)
wave2 <- smallfriends %>% filter(Wave == 2) %>% na.omit %>% data.frame()
msall1_df <- msall1_df %>% add_row(from = paste0("V",wave2$from),
                      to = paste0("V",wave2$to),
                      ms = max(msall1_df$ms)+1)
edges <- msall1_df %>%
  group_by(from, to) %>%
  summarize(edgeappears = n()) %>%
  arrange(desc(edgeappears)) %>% ungroup() %>%
  mutate(eid = row_number())

msall1_df2 <- left_join(msall1_df, edges, by = c("from" = "from", "to" = "to"))

p1 <- ggplot(data = msall1_df2) +
  geom_tile(aes(x = ms, y = -eid)) +
  annotate("text", x = c(-1, 84, 40), y = c(-30, -30, -50), label = c("Wave 1", "Wave 2", "Microsteps"), angle = c(90, 90, 0)) +
  geom_vline(xintercept = c(0.5,81.5)) +
  #scale_fill_continuous(low = "red", high = "blue") +
  ThemeNoNet +
  theme(legend.position = 'none', axis.ticks.y = element_blank(),
        axis.text.y = element_blank(), axis.title = element_blank()) +
  labs(x = "Microstep")


# rep 2
msall2 <- netvizinf::listMicrosteps(dat = wave1friends,
                      microsteps = filter(ansnullchainsw1w2, rep == 2))
msall2_df <- plyr::rbind.fill(lapply(X = seq_along(msall2), FUN = function(i, x) {
        x[[i]]$ms <- i - 1
        return(x[[i]])
    }, msall2))
msall2_df <- msall2_df %>% dplyr::select(from, to, ms)
msall2_df <- msall2_df %>% add_row(from = paste0("V",wave2$from),
                      to = paste0("V",wave2$to),
                      ms = max(msall2_df$ms)+1)

edges2 <- msall2_df %>%
  group_by(from, to) %>%
  summarize(edgeappears = n()) %>%
  arrange(desc(edgeappears)) %>% ungroup() %>%
  mutate(eid = row_number())

msall2_df2 <- left_join(msall2_df, edges2, by = c("from" = "from", "to" = "to"))

p2 <- ggplot(data = msall2_df2) +
  geom_tile(aes(x = ms, y = -eid)) +
  annotate("text", x = c(-1, 51, 25), y = c(-25, -25, -40), label = c("Wave 1", "Wave 2", "Microsteps"), angle = c(90, 90, 0))+
  geom_vline(xintercept = c(0.5,49.5)) +
  #scale_fill_continuous(low = "red", high = "blue") +
  ThemeNoNet +
  theme(legend.position = 'none', axis.ticks.y = element_blank(),
        axis.text.y = element_blank(), axis.title = element_blank()) #+
  #labs(x = "Microstep", y = "Edges (most frequent at top)")

# rep 3
# msall3 <- netvizinf::listMicrosteps(dat = wave1friends,
#                       microsteps = filter(ansnullchainsw1w2, rep == 3))
# msall3_df <- plyr::rbind.fill(lapply(X = seq_along(msall3), FUN = function(i, x) {
#         x[[i]]$ms <- i - 1
#         return(x[[i]])
#     }, msall3))
# msall3_df <- msall3_df %>% dplyr::select(from, to, ms)
# msall3_df <- msall3_df %>% add_row(from = paste0("V",wave2$from),
#                       to = paste0("V",wave2$to),
#                       ms = max(msall3_df$ms)+1)
#
# edges3 <- msall3_df %>%
#   group_by(from, to) %>%
#   summarize(edgeappears = n()) %>%
#   arrange(desc(edgeappears)) %>% ungroup() %>%
#   mutate(eid = row_number())
#
# msall3_df2 <- left_join(msall3_df, edges3, by = c("from" = "from", "to" = "to"))
#
# p3 <- ggplot(data = msall3_df2) +
#   geom_tile(aes(x = ms, y = -eid, fill = ms)) +
#   geom_vline(xintercept = c(0.5,106.5)) +
#   scale_fill_continuous(low = "red", high = "blue") +
#   ThemeNoNet +
#   theme(legend.position = 'none', axis.ticks.y = element_blank(),
#         axis.text.y = element_blank(), axis.title = element_blank()) #+
#   #labs(x = "Microstep", y = "Edges (most frequent at top)")

grid.arrange(p1, p2, nrow = 2)
@

%We also wanted to better understand the entire microstep process from the first wave, all the way through to the second wave. The number of steps taken from wave 1 to wave 2 varies. In one set of 1,000 simulations from Model 1, the smallest number of steps taken was 58, and the longest was 248, with a mean of 106 and a median of 103. In the 1,000 simulations, the standard deviation of the number of microsteps was 22.8. In Figure~\ref{fig:allsteps}, we see two simulations of the process from wave 1 to wave 2, with wave 1 shown on the left, and wave 2 shown on the right. In each of the three plots, the $y$-axis contains the edges sorted by how often they appear in the networks along the way. We can see that some edges are there in the beginning, but disappear and never come back, while others appear a few steps in, only to dispappear again. There are also some edges that were observed in wave 2 that don't appear at all in the microstep process in a given simulation. \st{So, even though the CTMC makes about the right \textit{number} of changes as it was designed to do, the changes it is making are not necessarily in the right direction.}

 %In addition, there is not a distinct pattern or structure that we can see in this particular edge visualization. The edges that the model starts with appear more often in the first 50 or so microsteps than any other set of edges. This makes sense, because we condition on the first wave, but it would be better if the model explored the space of possible edges more than it currently does. It could be that the simple model M1 is not complicated enough for the friendship dynamics in the data, or it could be that the chains in the MCMCs ``get stuck" when exploring the parameter space, but we cannot know this .}

% \st{The algorithms here are many!
% \begin{enumerate}
% %\item First, the microsteps / CTMC between each time point (gifs, other animations)
% \item Phases of RSiena: phase 1 = initial parameter estimate
% \item phase 2 = iterative update of parameters - trace plots
% \item phase 3 = simulations for convergence diagnostics
% \end{enumerate}
%  }

%\hh{The visualizations should address most elements of structure described before, i.e. follow along in the setup (for descriptive and teaching purposes), the fitting (understanding the model and interpreting the results) and then diagnostics (how well does the model fit the data - where are the most differences to the actual data/what are the sensitive parameters?) }

% \hh{Some of the visualizations we talked about
% \begin{enumerate}
% \item visualization of the MCMC process:  movie of individual actors' step-by-step decisions; could be done with the ndtv package, as long as we can get individual steps out of RSiena
% \item global picture: 1000 simulated networks + alpha blending should give a pretty good idea of the most important edges
% %\item global structure + principal components (tensor pca)
% \end{enumerate}}


\section{Discussion}\label{sec:discussion}

We have used traditional network visualization methods to explore the family of stochastic actor-oriented models for dynamic social network data in new ways. The guiding principles of model-vis: viewing the model in the data space, visualizing collections of models, and looking at the underlying algorithms, gave us knowledge and appreciation for these complicated and flexible models. When we look at the model in the data space by viewing the expected network simulated from a model, we are able to easily see the lack of model fit. When visualizing collections of models, we uncover concerning relationships between parameters. When we explore the algorithms, we gain a better understanding of the model and its stochastic behavior. Our three-pronged, model-vis attack has uncovered many properties of SAOMs. 

We have only just begun to scratch the surface of these multi-layered models for social networks. The \texttt{RSiena} software is incredibly powerful, and can fit much more flexible SAOMs than we have examined here. If, for example, a researcher thinks the network structure or an actor covariate affects the rate of change of the network, there is a way to incorporate that belief into the rate function of the SAOM. In addition, more than one actor-level covariate can be included in the model, and many more than three parameters can be included in the objective function itself. As we have shown, the rate and objective function parameters are often highly correlated, so researchers can now use model-vis techniques to gain a better understanding of these correlations and hopefully find ways to account for their effects in the model. In addition, SAOMs can model behavior change of the actors in the network, a capability we did not explore here. 

Furthermore, the data sets we have used are relatively small by network data standards, with only 16 and 100 nodes. Thus, some of the visualizations we have presented may not scale up to larger networks. Future SAOM and other network model visualization research could include interactive and/or three-dimensional graphics. Interactivity is the state-of-the-art in data visualization, and network visualization is just starting to catch on \citep[see e.g.\ ][]{refinery}. \citet{modelvis} relied on interactive tools like GGobi \citep{STLBC03} for interactivity, and for networks, there is some interactive support with the package \texttt{plotly} \citep{plotly} built into \texttt{geomnet}. 

We hope that the principles of model-vis continue to be used for learning more about the fit and behavior of complicated statistical models. Our model-vis application has uncovered many properties of SAOMs, and potential abounds for use in other areas of network study.  

%The flexibility of \texttt{RSiena} allows the user to specify which parameters lead to tie creation, and which parameters lead to tie dissolution. We have used ``evaluation" parameters, which are agnostic, allowing the data to decide which parameters affect creation or dissolution via the sign \citep{RSienamanual}. Finally, SAOMs and \texttt{RSiena} are able to also model behavior change of the actors in the network, which again is a capability we did not explore here.

\section*{Appendix: Additional model visualizations}

\subsection*{Section 4}

<<sigtesting, fig.show = 'hold', out.width='\\textwidth', fig.height=2, fig.cap="Significant effects for the two data sets, at a significance level of 0.10 or lower as calculated by the Wald-type test available in the SIENA software.", error=FALSE>>=
smfr_effs <- read_csv("data/effects_significance_smallFriends.csv")
smfr_effs$inter1[is.na(smfr_effs$inter1)] <- "none"
library(ggrepel)
set.seed(12345678)
smfr_effs %>% filter(type == "eval" & Waldpval <=0.10) %>%
ggplot(aes(x = estimate, y = Waldpval, shape = inter1)) +
  #geom_errorbarh(aes(xmin = estimate - se, xmax = estimate + se), color = 'black') +
  geom_vline(xintercept = 0) +
  geom_point(size = 3, alpha = .75) +
  #geom_text_repel(aes(label = shortName)) +
  scale_shape(name = "Interaction\nVariable") +
  ggtitle("Significant effects at level 0.10 for friendship data") +
  coord_flip() +
  ThemeNoNet
sen_effs <- read_csv("data/congress/senateModelSigEffects.csv")
#sen_effs$Waldpval <- parse_number(sen_effs$Waldpval)
sen_effs$inter1[is.na(sen_effs$inter1)] <- "none"
ggplot(data = sen_effs %>% filter(Waldpval <=0.10), aes(x = estimate, y = Waldpval, shape= inter1)) +
  # geom_errorbarh(aes(xmin = estimate - se, xmax = estimate + se), color = 'black') +
  geom_vline(xintercept = 0) +
  geom_point(size = 3) +
  #geom_text_repel(aes(label = shortName)) +
  scale_shape(name = "Interaction\nVariable") +
  ggtitle("Significant effects at level 0.10 for senate data") +
  coord_flip() +
  ThemeNoNet
@

In Figure~\ref{fig:sigtesting}, we see for the friendship data and the senate data that most of the significant effects have absolute value less than ten. In addition, the $p$-values for the effects from the friendship data are more spread out than the $p$-values for the senate data, which are concentrated at about 0.02 or less. This suggests that larger data sets tend to result generally in smaller $p$-values, which is consistent with the construction of Wald-type tests.

<<averagenet, results='hide', out.width='\\textwidth',fig.cap='The node-link diagrams from the three expected networks that we calculated are in the top row, and the true wave 1 and wave 2 data are shown in the bottom row above. There is some difference between the three models, but overall, these three models struggle to capture the structure in the true second wave of data.', fig.height=5>>=

combinedavgs$linewidth[combinedavgs$cat %in% c("1st Wave", "2nd Wave")] <- .5
combinedavgs$cat <- as.factor(combinedavgs$cat)
combinedavgs$cat <- ordered(combinedavgs$cat, levels = c("Model1", "Model2", "Model3", "1st Wave", "2nd Wave"))

ggplot(data = combinedavgs) +
  geom_net(aes(from_id = from, to_id = to, linewidth= linewidth),
           directed = T, curvature = .2, labelon=T, size = 3,
           arrow = arrow(angle = 20, type = "open", length = unit(.2, "cm")), ecolour = "#969696", fiteach = T,
           color = "#252525", hjust = .5, vjust = .5, labelcolour = 'white', fontsize = 2, arrowgap = .02,
           singletons = T) +
  ThemeNet + theme(panel.background = element_rect(color = 'black')) +
  facet_wrap(~cat)
@

To create the expected network visualization shown in Figure~\ref{fig:averagenet}, we follow the same procedure as in Section~\ref{sec:minds}, counting occurrences of each realized edge in the simulations.  The resulting summary network has weighted edges representing the proportion of edge appearances in all 1,000 simulations of wave 2. As in Figure~\ref{fig:summnetM1}, edges only appear in the expected networks if they appear more than 5\% of the time in the simulations. In Figure~\ref{fig:averagenet}, we show the  expected network from the three models we fit and the first and second waves of data. Comparing the three averages to waves 1 and 2, we see that they have very similar structure to wave 1 but that model M2, which included the transitive triplet parameter, results in more strongly connected components. The lack of fit is clear: none of the three average networks show node 16 gaining ties as it does in wave two, nor do they show nodes 4 and 7 becoming isolated. In model M2, however, the ties to node 7 appear much weaker than in model M1 or model M3, and the nodes $\{10,11,14\}$ are also strongly connected as they are in wave 2 of data, suggesting that M2 may be the best model of the three.

%Outdegree and reciprocity have the same inverse relationship for both data sets. For the friendship data, the inclusion of $\\beta_3$ has strong effect on the estimates of the other two parameters, but not for the senate data.
<<compareModels, fig.cap="Density plots of objective function parameter estimates from repeatedly fitting models M1, M2, and M3 to the example data. ", fig.height = 3>>=
simu2$data <- "friends"
senateEsts$data <- "senate"
allDataEsts <- rbind(senateEsts, simu2)
allDataEsts$parameter <- as.factor(allDataEsts$parameter)
allDataEsts$parameter <- ordered(allDataEsts$parameter, levels = c(paste0("alpha", 1:3),paste0("beta", 1:4)))
ggplot(allDataEsts %>% filter(str_detect(parameter, "beta"))) +
  geom_density(aes(x = estimate, fill = Model), alpha = .5) +
  facet_grid(data~parameter, scales = "free", labeller = "label_both") +
  scale_fill_brewer(palette = "Greys") + 
  ThemeNoNet +
  theme(legend.position = 'bottom')
@

The density plots in Figure~\ref{fig:compareModels} show the distribution of the parameter estimates for the objective function parameters in M1, M2, and M3 the two example data sets we used. The most notable difference is in the values of $\beta_1, \beta_2$. In the senate data, the inclusion of $\beta_3$ has no effect on the estimates of $\beta_1, \beta_2$, while the opposite is true for the friendship data. 

<<corplots, fig.cap="A matrix of plots demonstrating the strong correlations between parameter estimate in our SAOMs. The strongest correlation within each model is between $\\beta_1$ and $\\beta_2$.", fig.width=8>>=

simu2 <- read.csv("data/simulation-1000-M1-M2-M3.csv")
simu_spread <- simu2 %>% spread(parameter, estimate)
ggpairs(simu_spread, columns = 3:8, aes(colour=Model, alpha = .5)) + 
  theme(plot.title = element_text(size = 10,
                                            face = 'plain', angle = 0, family = "Times New Roman"),
                  axis.title.x = element_text(size = 10,
                                            face = 'plain', angle = 0, family = "Times New Roman", inherit.blank = T),
                  axis.title.y = element_text(size = 10,
                                            face = 'plain', angle = 90, inherit.blank = T),
                  axis.text.x= element_text(size = 10,
                                            face = 'plain', angle = 90, inherit.blank = TRUE),
                 # axis.text.x.bottom = element_text(size = 10,
             #                               face = 'plain', angle = 90, inherit.blank = TRUE),
                  axis.text.y = element_text(size = 10,
                                            face = 'plain', angle = 0, family = "Times New Roman", inherit.blank = T),
                #  axis.text.y.right = element_text(size = 12,
               #                             face = 'plain', angle = 0, family = "Times New Roman", inherit.blank = T),
                  strip.text.x = element_text(size = 12,
                                            face = 'plain', angle = 0, family = "Times New Roman",
                                            margin = margin(t = 3, r = 0, b = 3, l = 0, unit = "pt"), inherit.blank = T),
                  strip.text.y =element_text(size = 12,
                                            face = 'plain', family = "Times New Roman", angle = 90,margin = margin(t = 3, r = 3, b = 3, l = 3, unit = "pt"),inherit.blank = T),
                  strip.background = element_rect(colour = "black", fill = "white")
                    )
@

In Figure~\ref{fig:corplots}, we examine correlations between each of pair of parameters within each model and overall.\nocite{ggally} The strongest correlation within each model is between $\beta_1$ and $\beta_2$, with absolute correlation between them greater than 0.90 in all three models. The $\beta_1$ parameter is also highly correlated with the $\beta_3$ parameter within model M2, but it is not as highly correlated with the $\beta_4$ parameter in model M3. It might therefore be advisable to consider only models that either allow $\beta_1$ or $\beta_2$ but not both. Looking at the high correlation with $\alpha$, we might switch to a model without $\beta_1$.

\subsection*{Section 5}

<<alledgests, fig.height = 4, fig.cap="Visualizing all microsteps taken in 1,000 simulations from the model M1. The occurrence percent is split up into groups to correspond with its distribution: only about 10\\% of the possible edges appear more than 10\\% of the time in the 1,000 simulations, while about 60\\% appear less than 1\\% of the time. The first wave network is shown at microstep 0, and the second wave of the network is shown as the last microstep for comparison. We see that it is rare for a microstep process to last longer than 150 steps, and also that the edges that appear past the 150th step tend to be in either the first wave or the second wave.">>=
# get a plot of all microsteps from w1 to w2 by edge
alledges <- expand.grid(from = paste0("V", 1:16), to = paste0("V", 1:16))
alledges$eid <- 1:nrow(alledges)

sfmsall %>% left_join(alledges) -> sfmsall2

sfmsall2 %>% group_by(rep) %>% mutate(isWave2 = (ms == max(ms))) -> sfmsall2

sfmsall2 %>% group_by(from, to, eid) %>%
  summarize(total =n()) %>%
  arrange(desc(total)) %>%
  ungroup %>%
  mutate(plotorder = row_number()) -> sfmsallorder

sfmsall2 %>% group_by(ms, eid) %>%
  summarise(count = n()) %>%
  mutate(alpha = count/1000) %>%
  arrange(ms, eid) -> sfmsall3

left_join(sfmsall3, sfmsallorder, by = "eid") -> sfmsall4

#add true wave 2 to plot
wave2 <- na.omit(actual2)
wave2$from <- paste0("V", wave2$from)
wave2$to <- paste0("V", wave2$to)
wave2 <- left_join(wave2, alledges)
wave2 <- left_join(wave2, sfmsallorder)

sfmsall5 <- sfmsall4 %>% ungroup %>% add_row(ms = max(sfmsall4$ms) + 1,
        eid = wave2$eid,
        count = NA,
        alpha = 1,
        from = wave2$from,
        to = wave2$to,
        total = wave2$total,
        plotorder = wave2$plotorder)
sfmsall5$alphaFctr <- cut(sfmsall5$alpha, c(0,.01, .05, .10, .25, .5, .75, 1, 1.1))
ggplot(data = sfmsall5) +
  geom_tile(aes(x = ms, y = reorder(eid,-plotorder), fill = alphaFctr)) +
  #scale_fill_gradient(low = 'grey90', high = '#0868ac', name = "occurrence %") +
  scale_fill_brewer(palette = "Greys", name = "Occurrence %", labels = c("(0-1]%", "(1-5]%", "(5-10]%","(10-25]%", "(25-50]%", "(50-75]%", "(75-100]%")) +
  labs(x = "Microstep No.", y = "Edges (ordered by total # occurrences)") +
  ThemeNoNet +
  theme(axis.ticks.y = element_blank(), axis.text.y = element_blank(), panel.grid = element_blank())
@


Finally, we combine 1,000 simulations from model M1 into a visualization that displays the entire microstep process in Figure~\ref{fig:alledgests}. To make this visualization, we first assign each possible edge in the network an ID number so that we can keep track of it throughout all microsteps and all simlations. Then, we count up the number of times each edge appears in the network throughout the microstep process for each of the 1,000 simulations. We also count the number of times an edge occurs at each step number. Since the number of microsteps in the process varies, the number of times an edge occurs decreases as the step number increases. Next, we compute a proportion, which we call the occurrence percentage, which is the number of times the edge was in the network in step 1, step 2, etc. divided by 1,000. Finally, we visualize all this information together in Figure~\ref{fig:alledgests}. In this plot, we see that all possible edges appear at least once at some point in the microsteps of at least one simulated process. We also see, however, that the process struggles to focus in on the edges in the second wave of the data. Ideally, we would like to see more occurrences the edges in wave 2 that are not in wave 1. But, about half of the edges in wave two are in the bottom half of the figure, which means they do not appear as much as they would if the model was actually capturing the mechanisms of tie change in the network. More of the darkest areas of the figure should belong to ties in wave 2, but those are often the lightest. This solidifies what we found in Figure~\ref{fig:heatmap}: the model M1 and the SAOM fitting process do not explore the data space enough to adequately capture the network change mechanism.


\noindent \textbf{Online supplemental material:} Much of the code used to create the visualizations in this paper has been packaged and made available online at \url{https://github.com/sctyner/netvizinf} and \url{https://github.com/sctyner/geomnet} (also on CRAN). Additional, non-packaged code can be found at \url{https://github.com/sctyner/SAOM-removing-blindfold}. 