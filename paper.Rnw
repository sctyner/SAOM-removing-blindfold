\documentclass[12pt]{article}
% \usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
% \geometry{letterpaper}                   % ... or a4paper or a5paper or ...
%\usepackage{graphicx}
\usepackage[font=small,skip=5pt]{caption}
\usepackage{subcaption}
\usepackage{animate}
\usepackage{afterpage}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amsfonts}
% \usepackage{color}
\usepackage{multirow}
\usepackage{rotating}
\usepackage[dvipsnames,svgnames,table]{xcolor}
\usepackage{hyperref}
\graphicspath{{figure/}}
% \usepackage{endfloat} % Figures to the end of the document

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
%---------------------------------------------------
%                 Editing Commands
\newcommand{\hh}[1]{{\color{magenta} #1}}
\newcommand{\st}[1]{{\color{orange} #1}}

%---------------------------------------------------
%                 Placing Figures


%---------------------------------------------------
% Define new environment
\newtheorem{theorem}{Theorem}[section]
\newtheorem{algorithm}[theorem]{Algorithm}
%---------------------------------------------------

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%


\begin{document}

%\bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0\blind
{
  \title{\bf Model Visualization Techniques for a Social Network Model}
  \author{Samantha Tyner\thanks{
    The authors gratefully acknowledge funding from the National Science Foundation Grant \# DMS 1007697. All data collection has been conducted with approval from the Institutional Review Board IRB 10-347}\hspace{.2cm}\\
    Department of Statistics and Statistical Laboratory, Iowa State University\\
    and \\
    Heike Hofmann\\
    Department of Statistics and Statistical Laboratory, Iowa State University}
  \maketitle
} \fi

\if1\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Model Visualization Techniques for a Social Network Model}
\end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}
Social networks have been studied for decades, beginning with a few foundational works, including the 1967 study, ``The Small World Problem" by Stanley Milgram. In this paper, we concentrate on one type of model for dynamic social networks: the stochastic actor-oriented models (SAOMs), introduced by Snijders (1996). Unlike other network models, SAOMs are not very well understood. We use model visualization techniques introduced in Wickham et al (2015) in order to make them a little less murky. The SAOMs are a prime example of a set of models that can benefit greatly from application of model visualization. With the help of static and dynamic visualizations, we bring the hidden model fitting processes into the foreground, eventually leading to a better understanding and higher accessibility of stochastic actor-oriented models for social network analysts.
\end{abstract}

\noindent%
{\it Keywords:} social network analysis, model visualization, dynamic networks, network visualization, network mapping, animation
\vfill

\newpage
\spacingset{1.45} % DON'T change the spacing!

\tableofcontents
\newpage

<<setup, echo = FALSE, message = FALSE, warning = FALSE>>=
options(replace.assign=TRUE,width=70, digits=3)
require(knitr)
opts_chunk$set(fig.path='figure/', cache.path='cache/', fig.align='center', fig.pos='h', out.width='.99\\linewidth', par=TRUE, cache=FALSE, concordance=TRUE, autodep=TRUE, message=F, warning=F, echo = FALSE)#, root.dir = "~/Desktop/Dissertation/SAOM-removing-blindfold/")
@

<<pkgs>>=
library(tidyverse)
library(RSiena)
library(network)
library(sna)
library(geomnet)
library(GGally)
library(netvizinf)
library(RColorBrewer)
library(gridExtra)
library(cowplot)

ThemeNoNet <- theme_bw() %+replace% 
            theme(plot.title = element_text(size = rel(1.5),
                                            face = 'plain', angle = 0), 
                  axis.title.x = element_text(size = rel(1.5),
                                            face = 'plain', angle = 0),
                  axis.title.y = element_text(size = rel(1.5),
                                            face = 'plain', angle = 90),
                  axis.text.x.top = element_text(size = rel(1.5),
                                            face = 'plain', angle = 0), 
                  axis.text.x.bottom = element_text(size = rel(1.5),
                                            face = 'plain', angle = 0), 
                  axis.text.y.left = element_text(size = rel(1.5),
                                            face = 'plain', angle = 0),
                  axis.text.y.right = element_text(size = rel(1.5),
                                            face = 'plain', angle = 0),
                  strip.text.x = element_text(size = rel(2),
                                            face = 'plain', angle = 0, 
                                            margin = margin(t = 3, r = 0, b = 3, l = 0, unit = "pt")),
                  strip.text.y =element_text(size = rel(2),
                                            face = 'plain', angle = 90,margin = margin(t = 3, r = 3, b = 3, l = 3, unit = "pt")),
                  strip.background = element_rect(colour = "black", fill = "white")
                    )
ThemeNet <- theme_net() %+replace% 
            theme(plot.title = element_text(size = rel(2),
                                            face = 'plain', angle = 0), 
                  strip.text.x = element_text(size = rel(2),
                                            face = 'plain', angle = 0, 
                                            margin = margin(t = 3, r = 0, b = 3, l = 0, unit = "pt")),
                  strip.text.y =element_text(size = rel(2),
                                            face = 'plain', angle = 0,margin = margin(t = 3, r = 0, b = 3, l = 0, unit = "pt")),
                  strip.background = element_rect(colour = "black", fill = "white"))
@

\section{Introduction} 
% \st{
% \begin{itemize}
% %\item What? - model visualization and SAOMs. briefly dsecribe what these things are
% %\item Why? - bring light to the RSiena blackbox. Need ways to visualize network models, collections of networks. 
% \item How?
% \end{itemize}
% %also talk about other dynamic network models briefly to put SAOMs in context.
% }
%\hh{XXX - talk a bit about social networks first, why we want to model them, and that usually the models are not dynamic.}
\st{%XXX - okay, here is what i have done for that:
Social networks have been studied for decades, beginning with a few foundational works, including the 1967 study, "The Small World Problem" by Stanley Milgram \citep{goldenberg09}. Examples of social networks include collaboration networks between academic researchers, friendship networks in a school or university, and trade networks between nations. In recent years, the study of social networks has grown in popularity due to an increase in the availability and access to social network data. There are many kinds of social networks, but there are not as many statistical models for social network data.} Some network models that have been applied to social networks include the exponential random graph model and latent space models. These models, however, are only for single instance networks. If we only have one network observation, or only care about one state of a complex network in time, like a snapshot of the World Wide Web, using the well-established models for single network observating is not a problem. If, on the other hand, we have many observations of social network over time, these models may not be appropriate because they do not explicitly allow for the network to change as time passes. When studying a network over time, referred to as a \textit{dynamic network}, we need a model that can take the time aspect of the network into account. 
Models for dynamic social networks have a great deal of modelling potential because of how realistic their structure can be. A social network does not form spontaneously: it evolves over time. Ties are formed and dissolved, and new actors join the social structure. Modeling the underlying mechanisms that create network changes over time is very complex but also provides potential to uncover hidden truths. 

\par In this paper, we concentrate on one type of model for dynamic social networks: the stochastic actor-oriented models (SAOMs), introduced by \citet{saompaper}. These models are fundamentally different from other social network models because they allow us to incorporate network \textit{and} actor statistics, where other models only rely on the network statistics, to model the changes in the network. Allowing the actor-level statistics to directly effect the structure of the network leads to a more practical and relevant approach to model change in a social network. In the ``real" world we expect people with common interests to be more likely to form relationships, and SAOMs allow us to incorporate this intuition in the modeling process. 

\st{
%\par In order to estimate the parameters of  SAOMs the software SIENA, and its \texttt{R} implementation \texttt{RSiena}, was developed \cite{RSiena}. This software marks a huge contribution to the field of social network analysis., and when we began working with it we were eager to understand how it works. HH: we are still interested in this.
%\hh{The next sentence is very vague - make it more specific: what aspect of SAOMs is not well understood? I think what you want is something like ``SAOMs are not very tractable analytically with traditional methods, e.g. likelihood functions turn into very complex objects because of the sheer number of parameters involved. Therefore, computationally more tractable solutions are used to fit estimators, in particular, SAOMs are fit using ... }
Unlike other network models, SAOMs are not very well understood. \st{They are relatively new, especially compared to the classic exponential random graph models, and they are not very tractable analytically. Likelihood functions quickly very complex objects to analyze due to the dependency structure inherent in the data. Therefore, computationally more tractable solutions are used to fit estimators, and in particular, % Traditional modeling steps, like determining the likelihood function, are very complex. Because of this, 
SAOMs are often fit to data using a series of Markov chain Monte Carlo (MCMC) phases for finding method of moments estimators. In order to estimate the parameters of SAOMs, we use the software SIENA, and its \texttt{R} implementation \texttt{RSiena}, which was developed by \citet{RSiena}. This software marks a huge contribution to the field of social network analysis, but the many moving pieces involved in parameter estimation are largely ``behind the scenes" and hidden from the software user.} In this paper, in order to better understand the model-fitting process, we attempt to bring SAOM fits and the fitting process out of their black boxes, by combining the principals of network visualization with those of model visualization as discussed in \citet{modelvis}. %Here, we explore RSiena and SAOMs with innovative network visualizations, both static and dynamic.
By bringing some light to the underlying methods and structures that are ``behind the scenes" of when fitting SAOMs to network data, we aim to help researchers working with the models better understand the implications and analyses of these models.
}

\par The guiding principles of model visualization introduced in \citet{modelvis} are: 
\begin{enumerate}
\item Display the model in the data space.
\item Collections of models are more informative than singletons. 
\item Explore the fitting \textit{process} rather than just looking at the final result.
\end{enumerate}
Stochastic actor-oriented models are a prime example of a set of models that can benefit greatly from application of model visualization. For instance, the models themselves include a continuous-time Markov chain (CTMC) that is completely hidden from the analyst in the model fitting process. Bringing the CTMC out of the black box and into the light through model visualization can provide researchers with insights into the underlying features of the model. Furthermore, SAOMs can include a great deal of parameters to be added to the model structure, each of which is attached to a network statistic. These statistics are often somewhat, if not highly, correlated, which causes high correlation between the associated parameters in a SAOM. By visualizing collections of SAOMs, we gain a better understanding of these correlations and find ways to deal with them and rectify their effects in the model. In addition, the estimation of the parameters in a SAOM relies on a Robbins-Monro algorithm, and the convergence checks for these estimates rely on simulation from the fitted model. Again, each of these steps are largely kept in the background of the estimation process. With the help of static and dynamic visualizations we bring the hidden model fitting processes into the foreground, eventually leading to a better understanding and higher accessibility of stochastic actor-oriented models for social network analysts. 

In Section~\ref{sec:netintro}, we introduce basic concepts of networks and network visualizations. In Section~\ref{sec:saoms}, we present the family of stochastic actor-oriented models for social network analysis. In Section~\ref{sec:ModelVis}, we combine concepts from Sections~\ref{sec:netintro} and \ref{sec:saoms} in an application of the model-vis paradigm, and conclude with a discussion in Section~\ref{sec:discussion}.

\section{Networks and their Visualizations} \label{sec:netintro}

\subsection{Introduction to Network Structures}

Network data is of frequent interest to researchers in a wide array of fields. There are technological networks, like power grids or the internet, information networks, such as citation networks or the World Wide Web, biological networks, like neural networks, and social networks, just to name a few \citep{newman}. Each of these examples have one thing in common: their data \textit{structure}. There are always units of observation: the power stations, websites, neurons, and people, which we refer to throughout this paper as \textit{nodes} or \textit{actors}. There are also always connections of some kind between those units: the power lines, hyperlinks, electrical signals, and relationships, which we will call \textit{edges} or \textit{ties}. Networks might change over time, like when new websites and hyperlinks are added on the World Wide Web, or when there are new people and relationships in a friendship network. The nodes and edges themselves can also have inherent variables of interest, e.g.\ the institution of authors in a co-authorship network, or the number of times two authors have collaborated. 

The multiple layers of network data structures pose unique problems to network analysts. Some questions that network researchers may aim to answer are: How does the strength of a tie between two nodes affect the overall structure of the network? Do node-level differences affect the formation or dissolution of edges? Which views of the data are most informative for communicating significant effects and other results of statistical analyses of the network of interest? These are, of course, just a few broad questions, and we focus here on the latter, which we aim to answer through visual exploration of network data and models. 

\subsection{Visualizing Network Data} \label{sec:netvizintro}

Network visualization, also called network mapping, is a prominent subfield of network analysis. Visualizing network data is uniquely difficult because of the structure of the data itself.  Most, if not all, data visualizations rely on well-defined axes inherited from the data. If variables are numerical, histograms, scatterplots, or time series plots are straightforward to construct. If the variables are categorical, bar charts and mosaic plots are available to to the researcher. If the data are spatial, there is a well-defined region in which to view information. Network data, however, are much less cut-and-dried.

There are two primary methods used to visualize networks: node-link diagrams and adjacency matrix visualization \citep{knuth2000, adjmatpro}. As a toy example, let us assume that we have five nodes, $\{1,2,3,4,5\}$, connected by five directed edges:  $\{2 \to 4, 3 \to 4, 1 \to 5, 3 \to 5, 5 \to 4\}$ We use this toy data set to demonstrate the two visualization methods in Figure~\ref{fig:toyplots}.

<<toyex, echo = FALSE>>=
set.seed(81393)
nodes <- data.frame(id = 1:5, group = sample(LETTERS[1:2], size = 5, replace = T), stringsAsFactors = F)
g <- igraph::as_data_frame(igraph::random.graph.game(n = 5, p = .4))
toynet <- list(nodes = nodes, edges = rbind(g, c(5,4)))
@

The first method, the node-link diagram, represents nodes with points in 2D Euclidean space and then represents edges by connecting the points with lines when there is an edge between the two nodes. These lines can also have arrows on them indicating the direction of the edge for directed networks. But because there is typically no natural placement of the points unless they have important spatial locations, a random placement of the points is used, then adjusted via a layout algorithm, of which there are many \citep{drawingalgs}. 

<<toyplots, fig.show='hold', fig.cap="On the left, a node-link diagram of our directed toy network, with nodes placed using the Kamada-Kawai algorithm. On the right, the adjacency matrix visualization for that same network.", out.width='.49\\linewidth'>>=
toynetdat <- fortify(as.edgedf(toynet$edges), toynet$nodes)
ggplot(data = toynetdat) + 
  geom_net(aes(from_id=from_id, to_id=to_id), labelon = T, size = 11, labelcolour = 'white', ecolour = 'black', vjust = .5, hjust = .5, fontsize = 9, arrowgap = .05, directed = T) +
  ThemeNet + 
  labs(title = "Node-Link Diagram")

ggplot(data = toynet$edges) +
  #geom_tile(aes(x = from, y = -to), color = 'grey20', fill = 'black') +
  geom_tile(aes(x = to, y = -from), color = 'grey20', fill = 'black') +
  geom_segment(aes(x = x, xend = xend), y = -0.5, yend=-5.5, colour="grey30", data = data.frame(x = 0:5+.5, xend= 0:5+.5)) + 
  geom_segment(aes(y=-y, yend=-yend), x = 0.5, xend=5.5, colour="grey30", data = data.frame(y = -0:5+.5, yend= -0:5+.5)) + 
  scale_x_continuous(breaks = 1:5, labels = 1:5, limits = c(0.5,5.5), position = "top") +
  scale_y_continuous(breaks = (-5):(-1), labels = 5:1, limits = c(-5.5,-.5)) +
  ThemeNoNet + 
  theme(panel.grid = element_blank(), aspect.ratio = 1) +
  labs(title = "Adjacency Matrix Visualization", x = "to (alter)", y = "from (ego)")
@
%\hh{XXX Explain each of these visualizations and either comment on the differences in the visualizations (colors) or don't show them. }

Some commonly used layout algorithms, such as  the Kamada-Kawai layout \citep{kamadakawai} and the Fruchterman-Reingold layout \citep{fruchterman-reingold}, are designed to mimic physical systems, drawing the graphs based on the ``forces" connecting them. In these algorithms, the edges of the network act as springs pushing and pulling the nodes in a low dimensional (usually two-dimensional) space. Another algorithm uses multi-dimensional scaling, relying on distance metric and computing a matrix whose entries represent the ``distance" between every pair of nodes. There are also layout algorithms that use properties of the adjacency matrix, like its eigenstructure, to place the nodes in 2D space \citep{drawingalgs}. The node-link diagram using the Kamada-Kawai layout algorithm for our toy network is shown in Figure~\ref{fig:toyplots}. Unless otherwise stated, all other node-link diagrams in this paper will use the Kamada-Kawai layout. %Other algorithms use properties of the network's adjaceny matrix in order to place the nodes.
%hh{XXX how does the adjacency matrix feature here? The adjacency matrix is a different mathematical representation of a network, but describes the same network as a node-link diagram.} \st{XXX I'm taking this to mean I should describe adjacency matrices more, so I've added a discussion below. XXX }


\st{The second primary method for network visualization uses the adjacency matrix of the network. The adjaceny matrix of a network, $\mathbf{A}$, describes the edges of a network in matrix form. An entry $A_{ij}$ of $\mathbf{A}$, for two nodes $i\neq j$ in the network is defined as 
\[
  A_{ij} =
  \begin{cases}
                                   1 &  \text{if an edge exists } i\to j \\
                                   0 & otherwise
  \end{cases}
\]
Note here that our edge variables are binary: we only consider the presence or absence of an edge. If the network has weighted edges, for example an email network where edge weights represent the number of emails sent from one person to another, the entries in the adjacency matrix are the edge weights instead of zeroes and ones. In an undirected network, $A_{ij} = A_{ji}$, but in a directed graph this is only true if there is an edge from $i$ to $j$ and from $j$ to $i$. Thus, $\mathbf{A}$ is always symmetric for undirected networks, and is symmetric for directed networks only if every edge between two nodes is reciprocated.}
%\hh{XXX what is an adjacency matrix? - explain at this point in mathematical terms} \st{XXX see above XXX}
An adjacency matrix visualization for our toy example is also shown in Figure~\ref{fig:toyplots}.


Each type of visualization comes with its own advantages and disadvantages. For example, paths between two nodes in a network are easier to determine with node-link diagrams than with adjacency matrix visualizations \citep{adjmatviz}. In node-link diagrams, node-level information can be incorporated into the visualization by coloring or changing the shape of the points representing the nodes, and edge-level information can be incorporated by coloring the lines, or changing their thickness, linetype, or color. Incorporating a node-level variable into an adjacency matrix visualization is not as straightforward or simple, which is more focused on edges. Adjacency matrix visualization has been found to be particularly useful when the network is very complex, dense, or large, and experimental studies have shown adjacency matrix visualization to be superior to node-link diagrames for large networks. For example, for basic perceptual tasks on networks, including node and edge count, adjacency matrix visualizations outperform node-link diagrams as the size and density of the network increases \citep{adjmatviz}. One drawback of the adjacency matrix visualization that \citeauthor{adjmatviz} found was that edges are overrepresented for undirected graphs, due to the symmetry of $\mathbf{A}$: the edge $x_{ij}$ for $i \neq j$ appears in $\mathbf{A}$ twice: in $A_{ij}$ and $A_{ji}$, and so it also appears twice in the adjacency matrix visualization.  %\hh{XXX What is the exact logic here that Ghoniem used to come to the conclusion that edges in undirected graphs are over-represented? Why would directed graphs make that better? wouldn't that just add to the number of edges? } \st{XXX in undirected nets, the adj mat is symmetric, where one edge ($x_{ij}$) is represented in entry $i,j$ in the matrix and in entry $j,i$. So, there are many repetitions in the matrix, which makes the network appear more dense than it really is.} 
\st{This, however, may actually be an advantage for \emph{directed} graphs, where exactly the correct number of edges is represented in a matrix visualization, due to the fact that the edges $x_{ij}$ and $x_{ji}$ are not interchangeable. A node-link diagram, however, may underrepresent the edge count if the edges $x_{ij}$ and $x_{ji}$ both exist and are drawn on top of one another. Ultimately, however t}here is not one "correct" way to visualize network information, and we will be using both \st{the node-link and adjacency matrix visualization} methods throughout this paper to explore social networks and stochastic actor-oriented models. 

\section{Stochastic Actor-Oriented Models for Longitudinal Social Networks} \label{sec:saoms}

% \st{What do I need to communicate in this section? 
% \begin{itemize}
% \item What the models are for: dynamic social networks with actor-level covariates
% \item What are the model components? rate function, objective function (network statistics, actor covariates, interactions between covariates and network statistics), time points ($t_m = 1, \dots, M$)
% \item What is RSiena? What methods of estimation, convergence checks does it use? What are the algorithms? What kinds of models can it fit? 
% \end{itemize}}


%The stochastic actor-oriented models are very aptly named: they model dynamic networks that can account for the individual node characteristics when making the edge changes. The object of analysis is a social network that has been observed at several discrete time points. The nodes in the network can have associated covariate variables that may be incorporated into modeling the tie changes in the network. The model itself has two primary pieces: the rate of change of ties, and the objective function of the nodes, which determines the exact change that gets made. 

%We start with a set of network observations at $M$ discrete time points: $x(t_1), x(t_2) \dots, x(t_M)$. The first observation, $x(t_1)$ is always conditioned on. The number of nodes / actors / vertices is constant in all time points and is denoted $n$.  The tie variables are directed and  binary, and are denoted at $x_{ij}(t_m)$ for time point $m$, and this is sometimes abbreviated to $x_{ij}$ in notation. If there is a tie from node $i$ to node $j$, then $x_{ij}=1$, and $x_{ij}=0$ otherwise. 

%\hh{mostly lit review plus a detailed description of the model and fitting process.}

A Stochastic Actor-Oriented Model (SAOM) is a model that incorporates all three components of dynamic networks: edge, node, and time information. It models the change of a network over time, allowing for changes in network structure due to actor-level covariates. This model was first introduced by Snijders in 1996 \citep{saompaper}. The two titular properties of SAOMs, stochasticity and actor-orientation, are crucial to understanding networks as they exist naturally. Most social networks, even holding constant the set of actors over time, are ever-changing as relationships decay or grow in seemingly random ways, and most actors (or nodes) in social networks have inherent properties that could affect how they change their role within the network, and vice versa. 

\subsection{Definitions, Terminology, and Notation}

In this paper, the term \textit{dynamic network} refers to a network, consisting of a fixed set of $n$ nodes, that is changing over time, and is observed at $M$ discrete time points, $t_1, \dots, t_M$ with $t_1 \le t_2 \le \dots \le t_M$. We denote the network observation at timepoint $t_k$ by $x(t_k)$. In the modelling process, we condition on the first observation, $x(t_1)$. The SAOM assumes that this longitudinal network of discrete observations is embedded within a continuous time Markov chain (CTMC), which we will denote $X(T)$. This process is almost entirely unobserved: we assume that the beginning of the process, $X(0)$, is equivalent to the first network observation $x(t_1)$, while the end of the process,$X(\infty)$, is equivalent to the last observation $x(t_M)$. Nearly all other parts of the process are unseen, with the exception of $x(t_2), \dots, x(t_{M-1})$. Unlike the first and last observations of the network, these ``in-between" observations do not have direct correspondence with steps in the continuous time Markov chain. Thus, the ``in-between" observations are considered to be ``snapshots" of the network at some point between two steps in the CTMC. The whole process $X(T)$ is a series of single tie changes that happen according to some pre-defined rate function, where one actor at a time is given the opportunity to add or remove one outgoing tie, or to not make any changes. Once an actor is chosen at random according to the rate function, it is ``given" the chance to change a tie, and it tries to maximize its utility function based on the current and near future states of the network. We expand on the model description further in the subsequent sections. 

\subsubsection{The Rate Function}

For the network $x$ and each actor $i$ in the network, the number of times that an actor $i$ gets to change its ties, $x_{ij}$, to other nodes $j \neq i$ in the network is dictated by a \emph{rate function} $\rho(x, \mathbf{z}, \boldsymbol{\alpha})$, where $\boldsymbol{\alpha}$ are the parameters in the function $\rho$, and $x$ is the current network state, with covariates of interest $\mathbf{z}$. For this paper, we assume a simple rate \st{``function"}, $\rho(x, \mathbf{z}, \boldsymbol{\alpha}) = \alpha_m$ that is constant across all actors between observations at time $t_m$ to $t_{m+1}$, \st{thus our rate function is just a rate parameter in the overall model}. In general, SAOMs can incorporate covariate values and network statistics into the model, so that each node will have a different rate of change.  %\hh{XXX that means that the function is essentially just a rate parameter, right?} \st{XXX yes XXX}
Other modelling scenarios allow this rate to be more flexible,  e.g.\ a function that depends on the time period of observation, some actor-level covariates or some actor-level network statistics. 
%\hh{How is the rate parameter different from a rate parameter?} \st{I think you meant how is a rate fn different from a rate parameter? I updated the language XXX->}%In our model with the simple rate function, 
\st{In our simple model with a simple rate parameter instead of a rate function,} the rate parameter dictates how quickly an actor $i$ gets an opportunity to change one of its ties to the other nodes in the network, $x_{ij}$, for $j \in \{1, \dots, n\}$ in the time period from $t_{m}$ to $t_{m+1}$. If $j = i$, no change in the network is made. The model also assumes that the actors $i$ are conditionally independent given their ties, $x_{i1}, \dots, x_{in}$ at the current network state. \st{Let $\tau(i|x,m) $ be the wait time until actor $i$ makes its next change from its current state in the network $x$. Note that $m$ indicates the number of the wave that is conditioned on in the SAOM. For any time point, $T$, where $t_{m} \leq T < t_{m+1}$, the waiting time to the next change opportunity by actor $i$ is exponentially distributed with expected value $\alpha_m^{-1}$. The conditional independence assumption is expressed in Equation~\ref{eq:c1}.

\begin{equation} \label{eq:c1}
 \tau(i|x,m) | x_{i1}(m), \dots, x_{in}(m) \stackrel{\text{iid}}{\sim} Exp(\alpha_m)
\end{equation}

 The waiting time to the next change opportunity by \textit{any} actor in the network is also exponentially distributed with expected value $(n\alpha_m)^{-1}$. The distribution of waiting time for the whole network to change, $\tau(x|m) = \sum_i \tau_i(m) |x_{i1}(m), \dots, x_{in}(m) $ can then be written as 

\begin{equation} \label{eq:c2}
 \tau(x|m) \sim Exp(n\alpha_m)
\end{equation}

} The parameter for the wait time for the whole network $n\alpha_m$ is the rate at which any tie change occurs. The estimation of this parameter is straightforward: a the method of moments is used to estimate the rate with the statistic $$C = \sum_i\sum_j |x_{ij}(t_{m+1}) = x_{ij}(t_m)|$$ which is the total number of changes from observation at time $t_m$ to the observation at time $t_{m+1}$. %In order to achieve the memorylessness property of a Markov process, 

\hh{XXX we tried to estimate the rate based on the simulations - you could include a histogram of the number of changes for the 1000 simulations in the viz part XXX}
\st{XXX see line 1257 for plot. where should I put it? }
\hh{XXX Put a reference to the plot here.}
 
\subsubsection{The Objective Function} 

%\hh{shouldn't that be independence? - make the conditional independence assumption above a labelled formula (e.g. C1) and then call it here }
Because of the conditional \st{in}dependence assumptions given in Equation~\ref{eq:c1}, we can consider the objective function for each node separately, as only one tie from one node is \st{allowed to change} at a time. \st{The node $i$, which is the node that is chosen to change at the current time point, is called the \textit{ego} node. It has the potential to interact with all other nodes in the network, $j \neq i$. These nodes $j$, are referred to as \textit{alter} nodes, or simply \textit{alters}. These nodes are acted upon by the ego node, and they only act when they become the ego node at a subsequent time point in the CTMC.} For the ego node, $i$, in the current network state $x$, its objective function, which it tries to maximize, is written as 
\begin{equation} \label{eq:of1}
f_i(\boldsymbol{\beta}, x) = \sum_k \beta_k s_{ik}(x, \mathbf{Z}),
\end{equation}

for $x \in \mathcal{X}$, the space of all possible directed networks with the $n$ nodes, and $\mathbf{Z}$, the matrix of covariates. The vector $\boldsymbol{\beta}$ contains the parameters of the model with corresponding network and covariate statistics, $s_{ik}(x, \mathbf{Z})$, for $k = 1,\dots, K$. %\hh{XXX ego node isn't defined yet - the description of the charts would be a good place for it.} \st{XX I added it above the obj function equation. Is it ok there?} 
Given the ego node, $i$, there are $n$ possible steps for the actor $i$ to take: either one of all current ties $x_{ij} = 1$ will be destroyed, a new tie will be created thatis currently $x_{ij}=0$, or no change will occur. 

The parameters, $\boldsymbol{\beta}$, correspond to various actor-level network statistics, $s_{ik}(x)$. %\hh{'There are always' ... that seems to come from a Snijder paper? Re-phrase this to be less vague: In the current implementation of RSiena (snijder) a model has minimally two parameters...} 
According to \citet[p. 371]{snijders01}, there should be at least two parameters included in the model: $\beta_1$ for the outdegree of a node, and $\beta_2$ for the number of reciprocal ties held by a node. \st{These effects should seem familiar to readers used to working with the classical exponential random graph model (ERGM) for networks. The outdegree represents the propensity of nodes with a lot of outgoing ties to form more outgoing ties (the "rich get richer" effect), and the reciprocity parameter measures the tendency of outgoing ties to be returned within a network. The statistics corresponding to these effects are written in terms of the edge variables $x_{ij}$, for $i \neq j$.} In the \texttt{RSiena} software that we use to fit the SAOMs, there are over 80 possible parameters to add to the model. The formulas for the effects are provided in \citet{RSienamanual}. The parameters, $\beta_k$, in the model can be split up into two groups: first, the structural effects, whose estimation depends only on the structure of the network, like the outdegree and reciprocity parameters mentioned above. The parameters are included when the researcher hypothesizes that they will model underlying mechanisms of network change. They hope to answer questions such as, ``How does the existing network structure influence change in the network?" %\hh{XXX move the last colored section to the front of this paragraph. That sets up the structure nicely, e.g. `In determining the covariates of a SAOM, two sources of parameters are considered: ...} \st{XXX as I'm going through and uncommenting your comments (sorry....) this one has lost meaning a little bit. Could you give me more detail? I'm not sure which section is which at this point. (sorry again...)} \hh{I guess you addressed this comment very thoroughly :)}
% and ``How do the behavior and characteristics of the nodes influence change in the network?" put this sentence below somewhere
The second set of effects are referred to as the actor-level or covariate effects. These covariate effects also depend on the structure of the network, with the additional inclusion of node-level covariates of interest. The covariate effects are written in terms of the tie variables $x_{ij}$, but also in terms of the covariates, $\mathbf{Z}$. A table of some possible structural and covariate effects is given in Table~\ref{tab:effects}. For a complete list of the network and covariate statistics that can currently be included in the objective function, see \citet{RSienamanual}.

\begin{table}
\centering
\begin{tabular}{m{4.5cm}m{4.5cm}m{3cm}}
\multicolumn{3}{l}{\textbf{Structural Effects}} \\ \hline\hline
outdegree  & $s_{i1}(x) = \sum_j x_{ij}$ & \includegraphics[width=2cm]{img/outdegree}\\ \hline
reciprocity  & $s_{i2}(x) = \sum_j x_{ij}x_{ji}$ & \includegraphics[width=2cm]{img/reciprocity}\\ \hline
transitive triplets  & $s_{i3}(x) = \sum_{j,h} x_{ij}x_{jh}x_{ih}$ & \includegraphics[width=1.5cm]{img/transtrip}  \\
\\
\multicolumn{3}{l}{\textbf{Covariate Effects}}\\  \hline \hline
covariate-alter  & $s_{i4}(x) = \sum_j x_{ij}z_j$ & \includegraphics[width=3cm]{img/covaralter} \\  \hline
covariate-ego  & $s_{i5}(x) = z_i\sum_j x_{ij}$ & \includegraphics[width=3cm]{img/covarego} \\  \hline
same covariate & $s_{i6}(x) = \sum_j x_{ij} \mathbb{I}(z_i = z_j)$ & \includegraphics[width=3cm]{img/covarsim}%\\
%jumping transitive triplets  & $s_{i7}(x) = \sum_{j \neq h} x_{ij}x_{ih}x_{hj} \mathbb{I}(z_i = z_h \neq z_j)$
\end{tabular}
\caption{\label{tab:effects} Some of the possible effects to be included in the stochastic actor-oriented models in RSiena. There are many more possible effects, but we only consider a select few here. For a complete list, see the RSiena manual \citep{RSiena}.}
\end{table}

When node $i$ is given the chance to change a tie, %\hh{`we assume that they wish' sounds strange - don't assume wishes, state assumptions}
it attempts to maximize the value of its objective function $f_i(\boldsymbol{\beta}, x)$ plus a random element, $U_i(x)$, to account for unknown attraction between nodes. In \citet{modelsSnijders}, it is recommended that the $U_i(x)$ be random draws from a type 1 extreme value distribution. The additional random element is included to account for any random, unexplainable change in the network ties. % XX What is the thought behind that? } ``the type 1 extreme value distribution (or Gumbel distribution) with mean 0 and scale parameter 1" \citep[p. 368]{snijders01}. 
This distribution, which is also known as the log-Weibull distribution, has probability distribution function, using $\mu$ for the mean parameter and $\sigma$ for the scale parameter, of

\begin{equation} \label{eq:dist1}
g(u|\mu, \sigma) = \frac{1}{\sigma}\exp\left\{-\left(\frac{u-\mu}{\sigma} + \exp^{-\frac{u-\mu}{\sigma}}\right)\right\}.
\end{equation}

<<logweibull, fig.align='center', out.width = '.99\\linewidth', fig.height = 2, fig.cap='The probability distribution function for the type 1 extreme value distribution, also known as the log-weibull or Gumbel distribution with location parameter $ \\mu = 0$ and scale parameter $ \\sigma = 0$.'>>=
library(reliaR)
x <- seq(-3,10,.01)
y <- dgumbel(x = seq(-3,10,.01), sigma = 1, mu=0)
qplot(x = x, y= y, geom = "line") + 
  annotate('text', x = 6, y = .25, 
        label = "g(u) = exp(-(u + exp(-u)))",size=5)  + 
  labs(x = "u", y = "density of g(u)")
@

The probability density function for the type 1 extreme value distribution is shown in Figure~\ref{fig:logweibull}. Using the distribution given in Equation~\ref{eq:dist1} with mean $\mu = 0$ and scale $\sigma = 1$ as \citet{modelsSnijders} suggests %\hh{XXX I'm not following - which distribution do you mean by `this' distribution? The density above is the type 1 extreme value density. I got lost at the  ``objective function $f_i(\boldsymbol{\beta}, x)$ plus a random element, $U_i(x)$"} \st{XXX I hope it is clearer now above}
is convenient because it \st{leads to a simple probability formula for} the probability that actor $i$ chooses to change its tie to actor $j$ \st{that can be written \textit{only}} in terms of the objective function. Let $p_{ij}(\boldsymbol{\beta}, x)$ be the probability that actor $i$ chooses to change its tie to actor $j$. Next, we write the network $x$ in its potential future state, $x(i \leadsto j)$, where the tie $x_{ij}$ has changed to $1-x_{ij}$. Then, the probility that the tie $x_{ij}$ changes is 

\begin{equation}\label{eq:pij}
p_{ij}(\boldsymbol{\beta}, x) = \dfrac{\exp\left\{f_i(\boldsymbol{\beta}, x(i\leadsto j))\right\}}{\sum_{h \neq i} \exp\left\{f_i(\boldsymbol{\beta}, x(i \leadsto h))\right\}}
\end{equation}

When $i = j$ in $p_{ij}$, the numerator represents the exponential of the value of the objective function when evaluated at the current network state. When the value of the objective function is high at the current state, the probability of not making a change in a microstep is also high. In the CTMC, when actor $i$ may make a change, it chooses which tie $x_{i1, \dots, x_{in}}$ to change at random according to the probabilities $p_{ij}(\boldsymbol{\beta}, x)$. \st{The objective function and the resulting values of $p_{ij}$ are combined with the rate function, or in our case the rate parameter, to fully describe the CTMC that is used to model network change.}
\hh{XXX This paragraph ends a bit abruptly - what is the future state probability used for?} \st{XX The future state probability is used within the continuous time markov chain to move through the different network states. This needs more fleshing out. Adding to the to-do list.}


\subsubsection{Continuous Time Markov Chain (CTMC)}
\st{
In the continuous-time Markov chain literature, see for instance \citet{ctmc}, chains are characterized by their \textit{generator} or \textit{intensity} matrix $\mathbf{Q}$. This matrix describes the rate of change between two states of the CTMC process, and the rows of this matrix always add to zero. For directed networks with binary edge variables like the ones we will be working with, there are a very large number of possible states, $2^{n(n-1)}$: there are two possible states for an edge, and there are $n(n-1)$ edge relationships (excluding self-ties).  The intensity matrix for a CTMC in a SAOM is then a square matrix of dimension $2^{n(n-1)} \times 2^{n(n-1)}$. Only one tie changes at a time in the CTMC, resulting in $n(n-1)$ reachable states from the current network state. Thus, the intensity matrix $\mathbf{Q}$ is very sparse, with only $n(n-1) + 1$ non-zero entries in each row. Note that $n(n-1)$ of these entries represent the possible states that are one edge different from a given state, while the additional non-zero entry is for the state to remain unchanged. All other entries in a row are structural zeroes because those network states cannot be reached from the current state in a single change. 

The two pieces of a SAOM, the rate function/parameter and the objective function, each contribute to the entries of the intensity matrix to describe the rate of change between two network states. The entries of $\mathbf{Q}$ are defined as follows: let $b \neq c \in \{1, 2, \dots, 2^{n(n-1)} \}$ be indices of two different possible states of the network, $x^b, x^c \in \mathcal{X}$.  Then the $bc^{th}$ entry of $Q$ is:
\[ q_{bc} = \begin{cases} 
     q_{ij}= \alpha_m p_{ij}(\boldsymbol{\beta}, x^b) & \text{if } x^c \in \{x^b(i \leadsto j) | \text{ any } i \neq j \in \{1, \dots, n\}\} \\
      0 & \text{if } \sum_i \sum_j |x_{ij}^c -x_{ij}^b | > 1 \\
      -\sum_{i\neq j} q_{ij} & \text{if } x^b = x^c 
   \end{cases}
\]

Thus, the rate of change between any two states, $x^b$ and $x^c$, that differ by only one tie $x_{ij}$, is the product of the rate at which actor $i$ gets to change a tie and the probability that the tie that will change is the tie to node $j$. Estimating the parameters in these models is difficult, but thanks to the SIENA software \citep(Rsiena), we have an accessible way to fit SAOMs to network observations. 

XXX HH can you help? I need a way better transition than that XXX
}

%### A SAOM as a CTMC {#saomctmc}

%In order to fit this model definition back into the original context of the CTMC described in Section \@ref(dynamicnets), it must be written in terms of its intensity matrix, $\mathbf{Q}$.  This matrix describes the rate of change between states of the process. For networks, there are a very large number of possible states, $2^{n(n-1)}$, so the intensity matrix is a square matrix of that dimension. But, thanks to the property of SAOMs that the states are allowed to change only one tie at a time, there are only $n$ possible states given the current state, $n-1$ of which are uniquely determined by the node $i$ that is given the opportunity to change. Thus, the intensity matrix $\mathbf{Q}$ is very sparse, with only $n(n-1) + 1$ non-zero entries in each row. Note that $n(n-1)$ of these represent the possible states that are one edge different from a given state, and the additional non-zero entry is for the state to remain the same. All other entries in a row are zero because those column states cannot be reached from the row state by just one change as dictated by the SAOM. The entries of $\mathbf{Q}$ are defined as follows: let $b \neq c \in \{1, 2, \dots, 2^{n(n-1)} \}$ be indices of two different possible states of the network, $x^b, x^c \in \mathcal{X}$.  Then the $bc^{th}$ entry of $Q$ is:
% \[ q(x^b, x^c) = \begin{cases} 
%       q_{ij}(\alpha, \rho, \boldsymbol{\beta}, x^b) = \lambda_i(\alpha, \rho, x^b, m)p_{ij}(\boldsymbol{\beta}, x^b) & \text{if } x^c \in \{x^b(i \leadsto j) | \text{ any } i \neq j \in \mathcal{N}\} \\
%       0 & \text{if } x^c \text{ differs from } x^b \text{ by more than 1 tie} \\
%       -\sum_{i\neq j} q_{ij}(\alpha, \rho, \boldsymbol{\beta}, x^b)  & \text{if } x^b = x^c 
%    \end{cases}
% \]
% 
% Thus, the rate of change between any two states that differ by only one tie, $x_{ij}$, is the product of the rate at which actor $i$ gets to change a tie and the probability that the tie that will change is the tie to node $j$.^[Just to be clear, the change is from $x^b_{ij}$ to $x^c_{ij} = 1 - x^b_{ij}$.] Furthermore, the theory of continuous time Markov chains gives that the matrix of transition probabilities between observation times $t_{m-1}$ and $t_{m}$ is dependent only on the difference between timepoints, $t_m - t_{m-1}$. Following the same definition for transition probabilities in Section \@ref(dynamicnets), the matrix of transition probabilities is 
% $$e^{(t_m - t_{m-1})\mathbf{Q}},$$ 
% where $\mathbf{Q}$ is the matrix defined above and $e^X$ for a real or complex square matrix $X$ is equal to $\sum_{k=0}^{\infty} \frac{1}{k!} X^k$.

\subsection{Fitting Models to Data}

To fit a SAOM to observations of a dynamic network, we use the package \texttt{RSiena} \citep{RSiena}. This package uses simulation methods to estimate parameter values using either the method of moments or maximum likelihood estimation. In this paper, use the method of moments estimation because the theory behind it was established in \citet{saompaper}, while the maximum likelihood estimation methods were not fully established until \citet{saomsml2010}, though \texttt{RSiena} contains capabilities to use maximum likelihood estimation. We also use the score function method for estimating the derivatives of the expected values, as opposed to the finite differences method, both of which are outlined in detail in \citet{SienaAlgs}.

\par For the score function method, the SIENA software uses a Robbins-Monro algorithm (see \citet{robbinsmonro}) to estimate the solution of the moment equation
$$ E_{\theta}S = s_{obs}$$
where $\theta$ is the vector of rate and objective function parameters, and $s_{obs}$ is the observed vector of model statistics, $S$. The entire algorithm is provided in \citet{SienaAlgs}.

There are three phases in the the SIENA algorithm, as described in \citet{RSienamanual, SienaAlgs}. The first phase performs initial estimation of the score functions for use in the Robbins-Monro procedure for method-of-moments estimation. The second phase carries out the Robbins-Monro algorithm and obtains estimates of the parameter values through iterative updates and simulation from the CTMC at current parameter values. The third phase uses the parameter vector estimated in phase two to estimate the score functions and covariance matrix of the parameter estimate, and also carries out convergence checks. In each of the the first two phases, the estimation procedure also uses ``microsteps" that simulate from the model as it exists in its current state in order to update either the score functions or the parameter estimates. These simulated microsteps are observed instances of the continuous-time Markov chain that is the backbone of the stochastic actor-oriented model. In Section~\ref{sec:ModelVis}, we further explore these phases in the SIENA method-of-moments algorithm through visualization, bringing them out of the ``black-box" and into the light. 

\subsection{Model Goodness-of-Fit}

The \texttt{RSiena} software that fits the models to data also includes a goodnes-of-fit function for examining model fit, \texttt{sienaGOF()}. This function ``assess[es] the fit of the model with respect to auxiliary statistics of networks" \citep[p.~53]{RSienamanual}. Examples of auxiliary statistics include the out- or indegree distribution on the nodes, with the option for users to input their own statistics to examine. The goodness-of-fit is evaluated as follows:
\begin{enumerate}
\item The auxiliary statistics are computed on the observed data and on $N$ simulated observations from the model. Typically, $N=1000$. 
\item The mean vector and covariance matrix of the statistics on the simulations from the model are computed.
\item The Mahalanobis distance from the observed statistics to the distribution of the simulated statistics is computed using the mean and covariance found in step 2. 
\item The Mahalanobis distrance from each of the $N$ simulations to the same distribution is computed, and the Mahalanobis distance of the observed data is compared to this distribution of distances.
\item A $p$-value is found by computing the proportion of simulated distances found in step 4 that are as large or larger than the Mahalanobis distance from the data. A SAOM is thus considered a good fit if $p$ is large.
\end{enumerate}
 
<<sienagof, fig.cap="Goodness of fit measures for two network statistics computed on the first simulated wave of a SAOM: indegree and outdegree cumulative distributions. On the x-axis is the degree value, and on the y-axis is the number of times that degree value occurs in the network. \\st{The p-value shown on the x-axis is the proportion of observations simulated from the model that have observed statistic values as large or larger than the values observed in the data.}", cache = TRUE, message = FALSE, warning = FALSE, fig.height=5>>=
load("data/ansnullpaper.rda")
library(lattice)
gof1 <- sienaGOF(ansnull, OutdegreeDistribution, varName ="friendship", join = FALSE)
gof2 <- sienaGOF(ansnull, IndegreeDistribution, varName ="friendship", join = FALSE)

sims1 <- data.frame(gof1$`Period 1`$Simulations)
dat1 <- gof1$`Period 1`$Observations
dat1 %>% data.frame() %>% gather(outdegree, val) %>% 
  mutate(outdegree = parse_number(outdegree)-1) -> dat1
p1 <- sims1 %>% 
  mutate(sim = row_number()) %>% 
  gather(outdegree, val, X1:X9) %>% 
  mutate(outdegree = parse_number(outdegree) - 1) %>% 
  filter(outdegree <= 6) %>%  
  ggplot(aes(x = outdegree, y = val)) + 
  geom_boxplot(aes(group = outdegree), size = .5, outlier.shape = "x", outlier.size = 5) + 
  geom_violin(aes(group = outdegree), bw = 2, fill = NA) + 
  geom_point(data = dat1, color = 'red') + 
  geom_line(data= dat1, color = 'red') + 
  geom_text(data = dat1, aes(label = val), hjust = -.5) + 
  labs(x = "Outdegree (p = 0.154)", y = "Statistic", 
       title = "Goodness-of-Fit: Outdegree distribution period 1") + 
  ThemeNoNet

sims2 <- data.frame(gof2$`Period 1`$Simulations)
dat2 <- gof2$`Period 1`$Observations
dat2 %>% data.frame() %>% gather(indegree, val) %>% 
  mutate(indegree = parse_number(indegree)-1) -> dat2
p2 <- sims2 %>% 
  mutate(sim = row_number()) %>% 
  gather(indegree, val, X1:X9) %>% 
  mutate(indegree = parse_number(indegree) - 1) %>% 
  filter(indegree <= 6) %>%  
  ggplot(aes(x = indegree, y = val)) + 
  geom_boxplot(aes(group = indegree), outlier.shape = "x", outlier.size = 5) + 
  geom_violin(aes(group = indegree), bw = 2, fill = NA) + 
  geom_point(data = dat2, color = 'red') + 
  geom_line(data= dat2, color = 'red') + 
  geom_text(data = dat2, aes(label = val), hjust = -.5) + 
  labs(x = "Indegree (p = 0.063)", y = "Statistic", 
       title = "Goodness-of-Fit: Indegree distribution period 1") + 
  ThemeNoNet


gridExtra::grid.arrange(p1, p2)
# sum(gof1$`Period 1`$SimulatedTestStat >= gof1$`Period 1`$ObservedTestStat)/1000
# [1] 0.154
# > sum(gof2$`Period 1`$SimulatedTestStat >= gof2$`Period 1`$ObservedTestStat)/1000
# [1] 0.063

@

The \texttt{plot.sienaGOF()} function allows us to visualize this fit. In Figure~\ref{fig:sienagof}, we provide an example of what the goodness-of-fit plot output looks like for indegree and outdegree statistics for a small model. Box plots and violin plots are drawn on top of each other in the figure in order to show the distribution of the simulated network observations compared to the true data shown in red points connected by red lines. If the red points lie "well within" the simulated values, the model is a good fit to the data. The model examined in Figure~\ref{fig:sienagof} appears to do a better job of capturing the outdegree distribution than the indegree distribution of the data. 

\subsection{Example Data}

<<adjmatviz, fig.align='center', fig.cap="A visualization of the adjacency matrices of the three waves of network observations in the ``Teenage Friends and Lifestyle Study'' data. The subset we will be using is outlined in red.", fig.width=8, results='hide',fig.show='asis', fig.width=8, fig.height=3>>=
source("code/00e_small_friends.R")
view_rs2$wave <- factor(view_rs2$wave)
levels(view_rs2$wave) <- c("Wave 1", "Wave 2", "Wave 3")
ggplot(data= view_rs2, aes(x = y, y=-x)) + 
  geom_tile(aes(fill = as.factor(value))) + 
  scale_fill_manual("i likes j", labels=c("no", "yes"), 
                    values = c('white', 'black')) +
  geom_rect(data= NULL, inherit.aes = FALSE, color = 'red',
            aes(xmin = 19.5, xmax = 35.5, ymax = -19.5, ymin = -35.5, fill =NA)) + 
  facet_wrap(~wave) + theme_bw() +
  labs(x = "j", y = "i") + 
  ThemeNoNet + 
  theme(aspect.ratio=1, axis.text = element_blank(), axis.ticks = element_blank(), panel.grid = element_blank()) 
@

<<alldataviz, fig.cap="The smaller friendship network data we will be modelling throughout the paper.", out.width='.99\\linewidth', fig.height=3>>=
actual1 <- merge(data.frame(as.edgelist(as.network(fd2.w1))), 
                 data.frame(id = 1:16, drink = drink2[,1]),
                 by.x = "X1", by.y = "id", all = T)
actual1$Wave <- 1 
actual2 <- merge(data.frame(as.edgelist(as.network(fd2.w2))),
                 data.frame(id = 1:16,drink = drink2[,2]),
                 by.x = "X1", by.y = "id", all = T)
actual2$Wave <- 2
actual3 <- merge(data.frame(as.edgelist(as.network(fd2.w3))),
                 data.frame(id = 1:16,drink = drink2[,3]),
                 by.x = "X1", by.y = "id", all = T)
actual3$Wave <- 3
alldat <- rbind(actual1, actual2, actual3)
#alldat$X1 <- paste0("V", alldat$X1)
alldat$X1 <- as.factor(alldat$X1)
#alldat$X2 <- ifelse(!is.na(alldat$X2), paste0("V", alldat$X2), NA)
alldat$X2 <- as.factor(alldat$X2)
#write_csv(alldat, "smallfriends4Geomnet.csv")
alldataviz <- ggplot(data = alldat, 
       aes(from_id = X1, to_id = X2, color = as.ordered(drink), group = as.ordered(drink))) +
  geom_net(fiteach = FALSE, directed=TRUE, layout.alg = 'fruchtermanreingold', size = 5, arrowsize = .25, arrowgap = .03, ecolour = 'grey40', labelon=T, labelcolour = 'black', fontsize = 4, vjust = .5) +
  scale_color_brewer(palette = "YlOrRd", name = "Drinking\nBehavior", labels = c("never", "1-2 times per year", "once per month", "once per week")) + 
  ThemeNet +
  facet_wrap(~Wave, labeller = "label_both") + 
  theme(legend.position = 'bottom', panel.background = element_rect(fill = NA, color = "grey30"))
alldataviz
@

To guide our visual exploration of stochastic actor-oriented models, we use two data sources. The first is a subset of the 50 actor dataset from the ``Teenage Friends and Lifestyle Study" that is provided on the \texttt{RSiena} webpage. These data come from \citet{friendsdata}, and we chose to only work with a subset of the data to make network visualizations less busy and to make any changes in the network more noticeable. The subset contained actors 20 through 35 and the ties between them, as well as the drinking behavior of each actor at each of the three waves. This specific subset was chosen because it showed somewhat higher connectivity than other subsets, as we've emphasized in the visualizations of the three network adjacency matrices in Figure~\ref{fig:adjmatviz}. For model fitting, we condition on wave 1 and estimate the parameters of the models from the second and third waves. We will also be working with one actor level categorical covariate, drinking behavior. This variable has five values in the original data: (1) does not drink, (2) drinks once or twice a year, (3) drinks once a month, (4) drinks once a week, and (5) drinks more than once a week. The network and the actor covariate values are visualized using a node-link diagram in Figure~\ref{fig:alldataviz}.

\st{The second data example we use is a collaboration network in the United States Senate during the $111^{th}$ through $114^{th}$ Congresses. These sessions of congress correspond to the years of Barack Obama's presidency, from 2009-2016.\footnote{Details of how this data can be downloaded are provided by Franois Briatte at \url{https://github.com/briatte/congress}}. In this network, ties are directed from senator $i$ to senator $j$ when senator $i$ signs on as a cosponsor to the bill that senator $j$ authored. There are (somewhat surprisingly) many hundreds of ties between senators when they are connected in this way, so we simplify the network by computing a single value for each senator-senator collaboration called the \textit{weighted propensity to cosponsor} (WPC). This value is defined in \citet{senate} as 
\begin{equation}\label{eq:sen1}
WPC_{ij} = \dfrac{\sum\limits_{k=1}^{n_j} \frac{Y_{ij(k)}}{c_{j(k)}}}{\sum\limits_{k=1}^{n_j} \frac{1}{c_{j(k)}}}
\end{equation}

where $n_j$ is the number of bills in a congressional session authored by senator $j$, $c_{j(k)}$ is the number of cosponsors on senator $j$'s $k^{th}$ bill, where $k \in \{1,\dots, n_j\}$, and $Y_{ij(k)}$ is a binary variable that is 1 if senator $i$ cosponsored senator $j$'s $k^{th}$ bill, and is 0 otherwise. This measure ranges in value from 0 to 1, where $WPC_{ij} = 1$ if senator $i$ is a cosponsor on every one of senator $j$'s bills and $WPC_{ij} = 0$ if senator $i$ is never a cosponsor any of senator $j$'s bills. 

<<senateplot, fig.cap='The collaboration network in the four senates during the Obama years, 2009-2016. Edges are shown only if the weighted propensity to cosponsor from one senator to another is greater than 0.25. We use the Fruchterman-Reingold algorithm to layout the node-link diagram.', fig.height=3>>=
seobama <- read_csv("data/congress/senateobamapres_gsw_25.csv")
ggplot(data = seobama) + 
  geom_net(directed = T, labelon=F, arrowsize = .25, singletons= T, fiteach = T, linewidth = .25, layout.alg = 'fruchtermanreingold',
           aes(from_id = source, to_id = target, color = party),size = 1) + 
  ThemeNet + 
  scale_color_manual(values = c("royalblue", "forestgreen","firebrick")) + 
  theme(legend.position = 'bottom') + 
  facet_wrap(~senate, nrow = 1)
@

Because we require binary edges for our models, we focus only on very strong collaborations. For our senate collaboration networks, $x$, edges are defined as 
\[
  x_{ij} =
  \begin{cases}
                                   1 &  \text{if } WPC_{ij} > 0.25 \\
                                   0 & \text{if } WPC_{ij} \leq 0.25.
  \end{cases}
\]
The networks we constructed for the four senates during President Obama's administration are shown in Figure~\ref{fig:senateplot}. In Section~\ref{sec:ModelVis}, we fit several stochastic actor-oriented models to these data sets use those models to guide our further exploration of SAOMs. 
}


\section{Model Visualizations} \label{sec:ModelVis}

Every good data analysis includes both numerical and visual summaries of the data, so why restrict model description and diagnostics to numerical summaries? The concept of model visualization was developed to complement traditional model diagnostic tools. Typically, numerical summaries such as $R^2$ are used to assess model fit, and the occasional visualization, like a residual plot, are used to determine how well the model fits the data. \citeauthor{modelvis} outline three separate ideas, each of which are can be referred to simply as the "model": the model family, the model form, and the fitted model. The latter is primarily what one thinks of first when considering a model in a data analysis, where specified a model is fit to data, and parameter estimates and other numerical summaries, such as $R^2$ are reported. In the context of SAOMs, the fitted model contains the form of the rate and objective functions, the estimated rate parameters, and the estimated objective function parameters. The model form describes the the model \textit{before} the fitting process, defining which parameters are in the model within the context of the larger model family. In SAOMs, the model form includes description of the rate and objective functions and the variables therein that describe how the network evolves over time. Finally, the model family is the broadest description of the model. This is the type of model that you wish to fit to the data, and is chosen based on the problem, data, and knowledge at hand. For example, we chose to use a SAOM to model network data over an exponential random graph model (ERGM) because we believe that actor-level variables effect network structure and formation, and we wanted to model the network changes over time. These three uses of the term ``model" can each be visualized according to the three principals of model visualization: display the model in the data space, look at a collection of models, and explore the process of fitting the model, not just the end result. We turn our focus to the fitted model and the model form. Specifically, we want to know how the model form affects the fitted model. Using our example data sets and our visualization toolbox, we hope to answer this question for SAOMs.

\subsection{The Models}

We first consider the 16 actor subset of the teenage friends and lifestyle data available on the \texttt{RSiena} website \citep{RSiena}. To this data, we fit three different SAOMs. Each SAOM used a simple rate function, $\alpha_m$, and an objective function with two or three parameters. The first model, $M1$, contains the absolute minimum number of parameters in the objective function $f_i(x)$:  $$f_i(x)^{M1} = \beta_1s_{i1} + \beta_2s_{i2},$$ where $s_{i1}$ is the density network statistic and $s_{i2}$ is the reciprocity network statistic for actor $i$ at the current network state $x$. The second and third models, $M2$ and $M3$, contain one additional parameter each in the objective function which were determined by a Wald-type test provided in the \texttt{RSiena} software to be significant, with $p$-values less than 0.05 \citep{RSienaTest}. The $M2$ model contains an actor-level covariate parameter, and the $M3$ model contains an additional strutural effect in the objective function.
\begin{align*}
f_i(x)^{M2} & = \beta_1s_{i1} + \beta_2s_{i2} + \beta_3s_{i3} \\
f_i(x)^{M3} & = \beta_1s_{i1} + \beta_2s_{i2} + \beta_4s_{i4}, 
\end{align*} 

\begin{figure}\label{fig:structures}
\begin{subfigure}[t]{.45\textwidth}
<<jtt, echo=FALSE, fig.width=2, fig.height=2, message = FALSE, warning = FALSE, fig.align='center',out.width='.49\\linewidth'>>=
jTTe <- data.frame(from = c('i', 'i', 'h'), to = c('h', 'j', 'j'))
jTTn <- data.frame(id = letters[8:10], group = c(1,1,2))

jTT <- merge(jTTe, jTTn, by.x = 'from', by.y = "id", all = T)

set.seed(12345) 
ggplot(data = jTT, aes(from_id = from, to_id = to)) + 
  geom_net(aes(shape = as.factor(group)), directed = T, labelon = T, 
           labelcolour='grey80',vjust = 0.5, hjust =0.5, arrowgap = .15, 
           colour = 'black', size=10, fontsize = 4,
           ecolour = c("red", "grey40", "grey40", "grey40")) + 
  expand_limits(x=c(-0.1,1.1), y=c(-0.1,1.1)) +
  ThemeNet +
  theme(legend.position = "none")
@
\caption{\label{fig:jtt}Realization of a jumping transitive triplet, where $i$ is the focal actor, $j$ is the target actor, and $h$ is the intermediary. The group of the actors is represented by the shape of the node.}
\end{subfigure}\hfill
\begin{subfigure}[t]{.45\textwidth}
<<dab, echo=FALSE, fig.width=2, fig.height=2, message = FALSE, warning = FALSE, fig.align='center', out.width='.49\\linewidth'>>=
dade <- data.frame(from = c('i', 'i', 'h', 'j'), to = c('h', 'j', 'k', 'k'))
dadn <- data.frame(id = letters[8:11], group = c(1,1,1,1))

dad <- merge(dade, dadn, by.x = 'from', by.y = "id", all = T)

set.seed(12345) 
ggplot(data = dad, aes(from_id = from, to_id = to)) + 
  geom_net(aes(shape = as.factor(group)), directed = T, labelon = T, labelcolour='grey80',vjust = 0.5, hjust =0.5, arrowgap = .15, colour = 'black', size=10, fontsize = 4) + 
  expand_limits(x=c(-0.1,1.1), y=c(-0.1,1.1)) +
  ThemeNet +
  theme(legend.position = "none")
@
\caption{\label{fig:dad}Doubly achieved distance between actors $i$ and $k$.}
\end{subfigure}
\caption{The additional network effects included in our models fit to the friends data. On the left, a jumping transitive triplet (JTT). On the right, a doubly achieved distance between $i$ and $k$.}
\end{figure}

where $s_{i3} = \sum_{j \neq h} x_{ij}x_{ih}x_{hj} \mathbb{I}(z_i = z_h \neq z_j)$, and $s_{i4} =  |\{j : x_{ij} = 0, \sum\limits_h x_{ih}x_{hj} \geq 2\}|$. These statistics are known as the number of jumping transitive triplets and the number of doubly achieved distances two effect, respectively. The first statistic emphasizes triad relationships that are formed between actors from different covariate groups, while the other emphasizes indirect ties between actors. These two effects are visually represented and further described in Figure~\ref{fig:jtt} and Figure~\ref{fig:dad}, respectively.

<<tableofmeans>>=
simu2 <- read.csv("data/simulation-1000-M1-M2-M3.csv")
senM1est <- read.csv("data/congress/senateM1ests.csv")
senM2est <- read.csv("data/congress/senateM2ests.csv")
senM3est <- read.csv("data/congress/senateM3ests.csv")
senM1est$Model <- "M1"
senM2est$Model <- "M2"
senM3est$Model <- "M3"
senM1est$Sim <- 1:nrow(senM1est)
senM2est$Sim <- 1:nrow(senM2est)
senM3est$Sim <- 1:nrow(senM3est)
senM1est %>% gather(parameter, estimate, Rate:recip) %>% dplyr::select(Sim, Model, parameter, estimate) -> senM1est2 
senM2est %>% gather(parameter, estimate, Rate:jumpXTransTrip) %>% dplyr::select(Sim, Model, parameter, estimate) -> senM2est2 
senM3est %>% gather(parameter, estimate, Rate:nbrDist2twice) %>% dplyr::select(Sim, Model, parameter, estimate) -> senM3est2 
senateEsts <- rbind(senM1est2, senM2est2, senM3est2)
senateEsts$parameter <- as.factor(senateEsts$parameter)
levels(senateEsts$parameter) <- c("beta1", "beta3", "beta4", "alpha1", "alpha2", "alpha3", "beta2")
SSE <-
  senateEsts %>% group_by(Model, parameter) %>% 
  summarize(meanEst = mean(estimate), sdEst = sd(estimate)) %>% 
  mutate(val = paste0(sprintf("%.3f",meanEst), " ", "(", sprintf("%.3f", sdEst), ")")) %>% 
  dplyr::select(Model, parameter, val) %>% 
  spread(Model, val) %>% arrange(as.character(parameter))
SFE <- simu2 %>% group_by(Model, parameter) %>% 
  summarize(meanEst = mean(estimate), sdEst = sd(estimate)) %>% 
   mutate(val = paste0(sprintf("%.3f",meanEst), " ", "(", sprintf("%.3f", sdEst), ")")) %>% 
  select(Model, parameter, val) %>% 
  dplyr::select(Model, parameter, val) %>% 
  spread(Model, val) %>% arrange(parameter)
@

\begin{table}
\begin{tabular}{l|ccc|ccc }
  \hline
  & \multicolumn{3}{|c|}{Friendship Data} & \multicolumn{3}{|c|}{Senate Data} \\
  \hline
  M1 & M2 & M3 & M1 & M2 & M3 \\
  $\alpha_1$ & \Sexpr{SFE$M1[1]} & \Sexpr{SFE$M2[1]} & \Sexpr{SFE$M3[1]} & \Sexpr{SSE$M1[1]} & \Sexpr{SSE$M2[1]} & \Sexpr{SSE$M3[1]} \\
  $\alpha_2$ & \Sexpr{SFE$M1[2]} & \Sexpr{SFE$M2[2]} & \Sexpr{SFE$M3[2]} & \Sexpr{SSE$M1[2]} & \Sexpr{SSE$M2[2]} & \Sexpr{SSE$M3[2]}  \\
  $\alpha_3$ & -- & -- & -- & \Sexpr{SSE$M1[3]} & \Sexpr{SSE$M2[3]} & \Sexpr{SSE$M3[3]} \\
  $\beta_1$ & \Sexpr{SFE$M1[3]} & \Sexpr{SFE$M2[3]} & \Sexpr{SFE$M3[3]} & \Sexpr{SSE$M1[4]} & \Sexpr{SSE$M2[4]} & \Sexpr{SSE$M3[4]} \\
  $\beta_2$ & \Sexpr{SFE$M1[4]} & \Sexpr{SFE$M2[4]} & \Sexpr{SFE$M3[4]} & \Sexpr{SSE$M1[5]} & \Sexpr{SSE$M2[5]} & \Sexpr{SSE$M3[5]}\\
$\beta_3$ & -- & \Sexpr{SFE$M2[5]} & -- & -- & \Sexpr{SSE$M2[6]} & -- \\
  $\beta_4$ & -- & --  & \Sexpr{SFE$M3[6]} & -- & -- & \Sexpr{SSE$M3[7]} \\
  \hline
\end{tabular}
\caption{\label{tab:meanests} The means (standard deviations) of parameter values estimated from repeated fittings of $M1, M2, M3$ to the small friendship network and the senate collaboration network. Each model was fit 1,000 times to the friend data, while each model was fit 100 times to the senate data.}
\end{table}

For comparison, we also fit models $M1$, $M2$, and $M3$ to the senate collaboration data. Comparison of the parameter estimates of these models fit to the two different data sets is given in Table~\ref{tab:meanests}. In addition, we fit two more models, $M4$ and $M5$ to the senate data that were found to be significant according to the Wald-type test:

\begin{align*}
f_i(x)^{M4} & = \beta_1s_{i1} + \beta_2s_{i2} + \beta_5s_{i5} \\
f_i(x, z)^{M5} & = \beta_1s_{i1} + \beta_2s_{i2} + \beta_6s_{i6}.
\end{align*}

In $M4$, $s_{i5} = \mathbb{I}(x_{i+} = x_{+i} = 0)$ is an indicator variable for isolated nodes, called the network-isolate effect. This parameter attempts to measure the effect of a node being completely isolated in the network. If the value of the parameter $\beta_5$ is high, nodes are encouraged to remain isolated. In $M5$,  $s_{i6} = \sum\limits_j x_{ij}x_{ji}\mathbb{I}(z_i = z_j)$ is a measure of reciprocity between senators in the same party, $z_i, z_j \in \{\text{Democrat, Independent, Republican}\}$, generally called the same covariate reciprocity effect. This parameter attempts to measure the propensity for forming reciprocal ties between nodes with the same covariate value. If the value of the parameter $\beta_6$ is high, nodes belonging to the same covariate group are encourage to reciprocate incoming ties. By exploring these models more visually, we hope to understand these effects and the model fitting process better. 

\subsection{View the model in the data space}\label{sec:minds}

The first way we hope to better understand stochastic actor-oriented models is by viewing the model(s) in the data space. In \citeauthor{modelvis}, they define the \textit{data space} as ``the region over which we can reliably make inferences, usually a hypercube containing the data" \citep[p.~206]{modelvis}. But what does this definition mean for network data? For dynamic social networks, there are a few different data ``spaces": 
\begin{enumerate}
\item the actors and their corresponding covariates,
\item the edges and their correpsonding variables that describe the relationships between the nodes, and 
\item the time over which the network evolves
\end{enumerate}
These three data pieces can be visualized together in various ways. The traditional node-link visualization uses one of many algorithms to layout the actors as points in 2D space, then draws segments connecting the points in 2D if there is an edge between two nodes, and draws nothing otherwise. The time aspect can be visualized by drawing each network observation in time and placing the observed timepoints side-by-side. 

One tool that can bring these three data spaces together in this way is the \texttt{R} package \texttt{geomnet} \citep{geomnet}. Different aesthesic aspects of each piece of the node-link diagram can be tied to the underlying node or edge data. The color, size, and shape of the points can be used to represent variables in the node data, while the color, linewidth, and linetype of the lines between points can be used to represent the edge variables. In a social network, node data might be age, gender, and occupation of the person in the network, and edge data might be length of connection between two people, how the people first met (school, work, church, etc.), and how often they interact, and we can view the network at different timepoints side-by-side to see its evolution. Pulling all of this information together with \texttt{geomnet} allows the entire data space to be viewed at once. 

<<funsenatevis, fig.height=3, fig.cap='The 111th Senate separated into ``BC" and ``AC": before Clinton left the Senate to become Secretary of State (on the right), and after she left (on the left).' >>=
se111clint <- filter(seobama, senate == 111)
se111noclint <- filter(seobama, senate == 111)
se111noclint$target[which(se111noclint$target == "Hillary Rodham Clinton")] <- NA
se111clint$Clinton <- 'Yes'
se111noclint$Clinton <- 'No'
clintonSenate <- rbind(se111clint, se111noclint)

ggplot(data = clintonSenate) + 
  geom_net(directed = T, labelon=F, arrowsize = .3, singletons= T, fiteach = T, arrowgap = .01, layout.alg = 'fruchtermanreingold',
           aes(from_id = source, to_id = target, color = party, linewidth = gsw, shape = sex,size = n_au)) + 
  #theme_net() + 
  ThemeNet + 
  scale_shape_manual(values = c(17,16)) + 
  scale_color_manual(values = c("royalblue", "forestgreen","firebrick")) + 
  theme(legend.position = 'bottom') + 
  facet_wrap(~Clinton, nrow = 1, labeller = 'label_both')
@

To demonstrate, we use \texttt{geomnet} to visualize the connections in the $111^{th}$ at two different points: when Hillary Clinton was in the senate, and after she left to become Secretary of State. Clinton was only in the $111^{th}$ senate for 17 days, from January 3, 2009, to January 20, 2009, when she was in the middle of her second term in the senate. In that time, she authored two bills and was a cosponsor on 17 other bills. With Clinton included in the node-link diagram, the senate looks much more collaborative than it does without her in the diagram. We can compare the number of bills authored throughout the senate by mapping the size of the node to the numer of bills authored by that senator. We also map shape to gender, and keep color mapped to party of the senator. In addition, we can see the strength of the tie by mapping the linewidth of the edge to the $WPC$ value between the two senators. In this single visualization, we have viewed node information (number of bills authored by a senator and the gender of that senator), edge infortmation (the direction and strength of ties between two senators), and time (before and after Clinton left the senate). 
% ST: I don't like this paragraph
% The visual aspects of the node-link diagram connecting the node, edge, and time data spaces are themselves part of the data space, because they can be viewed in different ways. Another way to connect the node and the edge data, as discussed in Section~\ref{sec:netvizintro}, is through the network's adjaceny matrix. The time dimension can also be incorporated with side-by-side adjacency matrix visualizations. 

% below chunk contains some repetitive stuff. Haven't figured out what it's purpose is yet. 
<<summnetM1, message=FALSE, fig.cap='On the left, the first wave of observed data that is conditioned on in the model. On the right, the second wave of observed data. In the middle, a summary network from the first model fit to the data. This summary network represents 1,000 simulations of wave 2 using the values from the simple fitted model $M1$.',results='hide', fig.height=3>>=
library(netvizinf)
friendshipData <- array(c(fd2.w1, fd2.w2, fd2.w3), dim = c(16, 16, 3))
drink1 <- s50a[20:35,1]
drink2 <- s50a[20:35,2]
drink3 <- s50a[20:35,3]
friendship <- sienaDependent(friendshipData)
alcohol <- varCovar(cbind(drink1,drink2, drink3))
mydata <- sienaDataCreate(friendship, alcohol)
M1eff <- getEffects(mydata)
M2eff <- includeEffects(M1eff, jumpXTransTrip, interaction1 = "alcohol")
M3eff <- includeEffects(M1eff, nbrDist2twice)
simu2 <- read.csv("data/simulation-1000-M1-M2-M3.csv")
means <- simu2 %>% group_by(Model, parameter) %>% summarize(
  means = mean(estimate)
)
M1parms <- (means %>% filter(Model == "M1"))$means
M2parms <- (means %>% filter(Model == "M2"))$means
M3parms <- (means %>% filter(Model == "M3"))$means
set.seed(4231352)
M1sims1000 <- netvizinf::saom_simulate(dat = mydata, 
                            struct = M1eff,
                       parms = as.numeric(M1parms), 
                       N = 1000)
M2sims1000 <- netvizinf::saom_simulate(dat = mydata, 
                            struct = M2eff,
                       parms = as.numeric(M2parms), 
                       N = 1000)
M3sims1000 <- netvizinf::saom_simulate(dat = mydata, 
                            struct = M3eff,
                       parms = as.numeric(M3parms), 
                       N = 1000)
M1simsdf <- sims_to_df(M1sims1000)
M2simsdf <- sims_to_df(M2sims1000)
M3simsdf <- sims_to_df(M3sims1000)
M1avgW2 <- M1simsdf %>% filter(!is.na(to) & wave == 1) %>%
  group_by(from, to) %>% 
  summarise(count = n()) %>% 
  mutate(weight = ifelse(from == to, 0, count))
M2avgW2 <- M2simsdf %>% filter(!is.na(to) & wave == 1) %>%
  group_by(from, to) %>% 
  summarise(count = n()) %>% 
  mutate(weight = ifelse(from == to, 0, count))
M3avgW2 <- M3simsdf %>% filter(!is.na(to) & wave == 1) %>%
  group_by(from, to) %>% 
  summarise(count = n()) %>% 
  mutate(weight = ifelse(from == to, 0, count))

# make a df of wave 2, wave 1, and the three averages and facet. 
names(actual1)[1:2] <- c("from", "to")
names(actual2)[1:2] <- c("from", "to")
actual1$count <- 1
actual1$weight <- 1
actual2$count <- 1
actual2$weight <- 1
actual1 <- actual1 %>% dplyr::select(from,to, count, weight)
actual2 <- actual2 %>% dplyr::select(from,to, count, weight)
actual1$cat <- "1st Wave"
actual2$cat <- "2nd Wave"

avgW2M1 <- M1avgW2 %>% ungroup() %>% 
  filter(weight > 50) %>% 
  mutate(from = as.factor(from), to = as.factor(to),
         cat = "Model1")
add1 <- setdiff(as.character(1:16), unique(c(as.character(avgW2M1$from), as.character(avgW2M1$to)))) 
avgW2M1 %>% add_row(from = add1, to = NA, count = NA, weight = NA, cat = "Model1") -> avgW2M1
avgW2M2 <- M2avgW2 %>% ungroup() %>% 
  filter(weight > 50) %>% 
  mutate(from = as.factor(from), to = as.factor(to),
         cat = "Model2")
add2 <- setdiff(as.character(1:16), unique(c(as.character(avgW2M2$from), as.character(avgW2M2$to)))) 
avgW2M2 %>% add_row(from = add2, to = NA, count = NA, weight = NA, cat = "Model2") -> avgW2M2
avgW2M3 <- M3avgW2 %>% ungroup() %>% 
  filter(weight > 50) %>% 
  mutate(from = as.factor(from), to = as.factor(to),
         cat = "Model3")
add3 <- setdiff(as.character(1:16), unique(c(as.character(avgW2M3$from), as.character(avgW2M3$to)))) 
avgW2M3 %>% add_row(from = add3, to = NA, count = NA, weight = NA, cat = "Model3") -> avgW2M3

combinedavgs <- rbind(actual1, actual2, avgW2M1, avgW2M2, avgW2M3)
combinedavgs %>% group_by(cat) %>% 
  mutate(linewidth = weight / max(weight,na.rm = T)) -> combinedavgs

combinedavgs %>% filter(cat == "Model1") %>% mutate(logweight = log(weight)) -> t1 
colors <- tweenr::tween_color(data = c("#969696", "#67000d"), n = 26, ease = 'linear')
t1 %>% arrange(logweight) -> t1
t1$color <- NA
t1$color[1:26] <- colors[[1]]

p1 <- ggplot(data = t1) +
  geom_net(aes(from_id = from, to_id = to, linewidth= linewidth),
           directed = T, curvature = .2, labelon=T, size = 5,
           arrow = arrow(angle = 20, type = "open", length = unit(.2, "cm")), ecolour = t1$color,
           color = "#252525", hjust = .5, vjust = .5, labelcolour = 'white', fontsize = 4, arrowgap = .02,
           singletons = T) +
  labs(title = "Average: M1") + ThemeNet +
  theme(plot.background = element_rect(color = 'black'))

p2 <- ggplot(data = filter(combinedavgs, cat == "1st Wave")) +
  geom_net(aes(from_id = from, to_id = to, linewidth= linewidth/3),
           directed = T, curvature = .2, labelon=T, size = 5,
           arrow = arrow(angle = 20, type = "open", length = unit(.2, "cm")), ecolour = "#969696",
           color = "#252525", hjust = .5, vjust = .5, labelcolour = 'white', fontsize = 4, arrowgap = .03,
           singletons = T) +
   labs(title = "Wave 1") + ThemeNet + 
    theme(plot.background = element_rect(color = 'black'))
p3 <- ggplot(data = filter(combinedavgs, cat == "2nd Wave")) +
  geom_net(aes(from_id = from, to_id = to, linewidth= linewidth/3),
           directed = T, curvature = .2, labelon=T, size = 5,
           arrow = arrow(angle = 20, type = "open", length = unit(.2, "cm")), ecolour = "#969696",
           color = "#252525", hjust = .5, vjust = .5, labelcolour = 'white', fontsize = 4, arrowgap = .03,
           singletons = T) +
  labs(title = "Wave 2")  +  ThemeNet + 
  theme(plot.background = element_rect(color = 'black')) 
grid.arrange(p2, p1, p3, nrow = 1)
@

Another way to view the model in the data space is through simulation from the model. No single network alone simulated from a SAOM is going to look like the data or represent the model, just as no single value simulated from the standard normal distribution will look like a bell-shaped curve. It would therefore better to visualize many simulations together. From a statistician's point of view, a stumbling block with statistical network models generally is the lack of an ``average network" measure. Statisticians frequently rely on averages and expected values in data analyses, but statistical network models, especially those as complex as SAOMs, lack a single, intuitive expected value measure. We could talk about expected values of parameters, but the parameters can be hard to interpret. Furthermore, there is no way to talk about the expected value of an observation from a statistical network model. How then, can we arrive at an ``average" network? We propose to answer this question through visualization. For network data, one way we propose to come up with an "average" network is through a summary network drawn using the traditional node-link diagram. In Figure~\ref{fig:summnetM1}, we show an \textit{average network} created with 1,000 simulations of the second wave of the network from Model 1. \st{To make this average network, we first simulated 1,000 wave 2 and wave 3 observations of our small friendship example data from model $M1$, for which parameters had previously been estimated. We then combine the 1,000 instances of wave 2, and count up the number of times each edge appears in a simulation. Then, we combine these 1,000 networks into a single network with edgeweight equal to the proportion of time that edge appears in our 1,000 simulations. This is the network we draw using the node-link diagram.} An edge is only drawn in the average network if it appears in more than 5\% of the simulations (in at least 51 of the 1000 simulations), with edges that appear more frequently emphasized by thicker linewidths and a darker color. On either side of the average network in Figure\ref{fig:summnetM1}, we show the actual data, wave 1 on the left, and wave 2 on the right. \st{We can see that the structure of the average network is much more similar to the first network observation than to the second network observation. The simulations, however, are supposed to represent the second wave of data, which is shown on the right in Figure~\ref{fig:summnetM1}. This is an indication that our simple model, $M1$ is doing a very poor job of capturing the change mechanism from the first to the second wave of observation.} % XXX ST - I was looking at the wrong one. I had wave == 2 instead of wave == 1 in the code. This means I was looking at the wrong simulations, so they really do look more like the first data obs than the second obs. XXX Comparing the three visualizations side-by-side shows us that the summary network of simulations of wave 2 looks more like the true wave 2 than wave 1, which is a good indication that the model is capturing the true data structure. It's not ideal, however, because there appear to be too many reciprocated ties in the summary network, far more than there are in the data. This could indicate that we should remove the reciprocity parameter from the model. 



<<gof2, fig.cap="An example of viewing the distribution of a statistic of interest from a SAOM. Here, the Mahalanobis distance of the cumulative outdegree values computed on a simulated network to the mean value simulated is shown. The distance computed from the data is shown as a vertical line.", fig.height=3>>=
qplot(x = log(gof1$`Period 1`$SimulatedTestStat), binwidth = .2) + 
  geom_vline(xintercept = log(gof1$`Period 1`$ObservedTestStat)) + 
  ThemeNoNet + 
  labs(x = "log(Statistic Value)", title = "Distribution of log Mahalanobis Distance")

@

\st{Because longitudinal network data consist of three different ``spaces" of data, viewing the model in the data space can depend on which aspect of the model and the data you are interested in viewing. The SIENA software includes a method for modelling covariate change in the network by incorporating the network structure, assuming that the ties between nodes can affect how nodes behave over time. In this instance, a time series plot of predicted covariate values for the nodes as the network evolves is viewing the model in the time and node spaces. If, as in Figure~\ref{fig:sienagof}, summary statistics computed on the network are of interest, distributions of these statistics computed on networks simulated from the model show the model in the edge data space. For an example, see Figure~\ref{fig:gof2}, where the distribution of the log of the Mahalanobis distance is shown for the outdegree distribution auxiliary statistics, and the log Mahalanobis distance for the observed data is shown as a vertical line. 

\begin{figure}
\centering
\includegraphics[width=.75\linewidth]{img/Model1LUSmallFriends.png}
\caption{\label{fig:lineup} A small lineup of node-link diagrams showing the second wave of our small friendship network among five networks simulated from model $M1$.}
\end{figure}

Another potential goodness-of-fit visualization is a lineup like those proposed in \citet{buja:2009}. A \texttt{lineup} ``asks the witness to identify the plot of the real data from among a set of decoys, the null plots, under the veil of ignorance" \citep[p. 4369]{buja:2009}. It can be thought of like a police lineup, where the "suspect" is in a lineup among several innocent lookalike fillers, and a witness picking the suspect out of the lineup is considered evidence against the suspect. In data and model visualization, the "suspect" is a plot of the true data, while the "filler" is composed of several plots of mock data, simulated from a hypothesized model. If the true data stands out among the simulated data, that is taken as evidence of poor model fit, whereas if the true data is difficult to identify among the simulated data, that is taked as evidence of good model fit. In Figure~\ref{fig:lineup}, the second wave of the small friendship network is shown among five simulated networks from model $M1$ using parameter values estimated from the data. The lineup allows us to view the model in the data space by placing instances of the model side-by-side with the data and examining the differences.} 

% \st{
% \begin{itemize}
% \item WHat is the model? the network model (SAOM model is just one of many)
% \item What is the data? Edge data, node data, observed time points, unobserved timepoints (CTMC)
% \item Note: specifically dealing with dynamic networks observed at discrete time points. 
% \end{itemize}
% What are some things that could go in here? need more data examples; not just the friends data.
% Harry Potter data??? I want to find my own data, too.}

\subsection{Collections are more informative than singletons}

The second principle of model visualization is that ``collections are more informative than singletons" \citep[p.~210]{modelvis}. The authors describe several different methods for producing such collections, but we focus on only a few here. We look at collections produced ``from exploring the space of all possible models," ``by varying model settings", ``by fitting the same model to different datasets", and ``by treating each iteration as a model" \citep[p.~210-11]{modelvis}. Because fitting a SAOM requires MCMC methods, we also fit the same model many times, resulting in distributions of fitted parameters for one data set.

To our small frienship network, we fit three models, $M1$, $M2$, and $M3$, using \texttt{RSiena} 1,000 times each. We then looked at the distribution of the fitted values, which are shown in Figure~\ref{fig:distests}. We can see from these distributions that the inclusion of the jumping transitive triplet parameter, $\beta_3$ is obviously affecting the distributions of the other four parameters included in all models, $\alpha_1$, $\alpha_2$, $\beta_1$, and $\beta_2$. In comparison with models M1 and M3, model M2 typically has higher estimates of the rate parameter, meaning that the inclusion of the covariate statistic in the model leads to higher estimates of the number of times, on average, a node gets to change its ties. It is not clear, however, that the addition of a parameter to the objective function \textit{should} effect the estimation of the rate parameters, so we continue to explore the collection of parameter estimates.

To further investigate this potentially problematic relationship between the parameter values, we look at correlations between each of the parameter estimates in each model. In Figure~\ref{fig:corplots}, we examine correlations between each of pair of parameters within each model and overall.\nocite{ggally} The strongest correlation within each model is between $\beta_1$ and $\beta_2$, with absolute value of correlation between those two parameter values greater than 0.90 in all three models. The $\beta_1$ parameter is also highly correlated with the $\beta_3$ parameter within model M2, but it is not as highly correlated with the $\beta_4$ parameter in model M3. 

<<getdata>>=
nulls <- read.csv("data/distribution_null_model.csv")
alt <- read.csv("data/distribution_jumpTT_model.csv")
alt2nd <- read.csv("data/distribution_dblpairs_model.csv")
means <- simu2 %>% group_by(Model, parameter) %>% summarize(
  means = mean(estimate)
)
null_mod_sv <- as.numeric(nulls[,-c(1,6,7)] %>% summarise_each(funs(mean)))
alt_mod_sv <- as.numeric(alt[,-c(1, 7,8)] %>% summarise_each(funs(mean)))
alt_mod2_sv <- as.numeric(alt2nd[,-c(1,7,8)] %>% summarise_each(funs(mean)))
@

<<distests, fig.cap="Distribution of fitted parameter values for our three SAOMs. The inclusion of $\\beta_3$ or $\\beta_4$ clearly has an effect on the distribution of the rate parameters, $\\alpha_1$ and $\\alpha_2$.", fig.height=4>>=

#simu2 <- read.csv("data/simulation-1000-M1-M2-M3.csv")

ggplot(data = simu2) + 
  geom_density(aes(x = estimate, fill = Model), alpha = .5) + 
  facet_wrap(~parameter, scales = 'free') + 
  ThemeNoNet + 
  theme(legend.position = 'bottom')
@


<<corplots, fig.cap="A matrix of plots demonstrating the strong correlations between parameter estimate in our SAOMs. The strongest correlation within each model is between $\\beta_1$ and $\\beta_2$.", fig.width=8>>=

#simu2 <- read.csv("data/simulation-1000-M1-M2-M3.csv")
simu_spread <- simu2 %>% spread(parameter, estimate)
ggpairs(simu_spread, columns = 3:8, ggplot2::aes(colour=Model, alpha = .5)) + theme(plot.title = element_text(size = rel(1.5),
                                            face = 'plain', angle = 0), 
                  axis.title.x = element_text(size = rel(1.5),
                                            face = 'plain', angle = 0, inherit.blank = T),
                  axis.title.y = element_text(size = rel(1.5),
                                            face = 'plain', angle = 90, inherit.blank = T),
                  axis.text.x.top = element_text(size = rel(1.5),
                                            face = 'plain', angle = 90, inherit.blank = TRUE), 
                  axis.text.x.bottom = element_text(size = rel(1.5),
                                            face = 'plain', angle = 90, inherit.blank = TRUE), 
                  axis.text.y.left = element_text(size = rel(1.5),
                                            face = 'plain', angle = 0, inherit.blank = T),
                  axis.text.y.right = element_text(size = rel(1.5),
                                            face = 'plain', angle = 0, inherit.blank = T),
                  strip.text.x = element_text(size = rel(2),
                                            face = 'plain', angle = 0, 
                                            margin = margin(t = 3, r = 0, b = 3, l = 0, unit = "pt"), inherit.blank = T),
                  strip.text.y =element_text(size = rel(2),
                                            face = 'plain', angle = 90,margin = margin(t = 3, r = 3, b = 3, l = 3, unit = "pt"),inherit.blank = T),
                  strip.background = element_rect(colour = "black", fill = "white")
                    )
@

<<corplotsSenate, eval = FALSE>>=
senM4est <- read.csv("data/congress/senateM4ests.csv")
senM5est <- read.csv("data/congress/senateM5ests.csv")
names(senM4est) <- c("alpha1", "alpha2", "alpha3", "beta1", "beta2", "beta5")
names(senM5est) <- c("alpha1", "alpha2", "alpha3", "beta1", "beta2", "beta6")
senM4est$Model <- "M4"
senM5est$Model <- "M5"
senM4est$Sim <- 1:nrow(senM4est)
senM5est$Sim <- 1:nrow(senM5est)
senM4est2 <- senM4est %>% gather(parameter, estimate, alpha1:beta5) 
senM5est2 <- senM5est %>% gather(parameter, estimate, alpha1:beta6) 
senM45est <- rbind(senM4est2, senM5est2)
senM45est %>% spread(parameter, estimate)-> senM45est2
ggpairs(senM45est2, columns = 3:9, ggplot2::aes(colour=Model, alpha = .5))
@

We also explore simulations from these different models given parameter values. Using the mean values of from the 1,000 fitted parameter values as the parameters in our models, we simulated 1,000 observations from each of our three models. In this process, we condition on the first friendship network observation, and the second and third observations are simulated from the SAOM models with the given parameter values. From these simulations, we first create a visualization that represents an average network. To do this, we follow the same procedure as in Section~\ref{sec:minds}, counting occurrences of each possible edge in the simulations, resulting in a summary network with weighted edges representing the number of times an edge appeared in the simulated wave 2 when simulating from the SAOM 1,000 times. As in Figure~\ref{fig:summnetM1}, edges only appear in the average network if they appear more than 5\% of the time in the simulations. In Figure~\ref{fig:averagenet}, we show the first wave, the ``average'' network from the three models we fit, and the second wave. \st{Comparing the three averages to waves 1 and 2, we see that they have very similar structure to wave 1. Model 2, which included the transitive triplet parameter, seems to have created a larger connected component overall than models 1 and 3. In particular, if we look at the group of nodes $\{10,11,14\}$, we see they are very strongly connected within the three average networks, and they are completely separate from the other nodes in the true wave 2. None of the three average networks show node 16 gaining ties as it does in wave two, nor do they show nodes 4 and 7 becoming isolated. In Model 2, however, the ties to node 7 appear much weaker than in Model 1 or Model 2, suggesting that of the three, Model 2 may be the best fit for our data.}  %XXXX ST commented out after I realized I had made the same mistake as figure 7. XX This could be an indication that, despite the highly significant nature of the parameter $\beta_3$ according to the $t$-tests in \texttt{RSiena}, the transitive triplet parameter should not be included because it does not result in a structure similar to the data. 

<<averagenet, results='hide', out.width='99%',fig.cap='The three "average" networks in the middle, with the actual network observations oon either side. There is some difference between the three models, but overall, these three models cannot capture the structure in the true second wave of data.', fig.height=6>>=
#friend.data.w1 <- as.matrix(read.table("s50_data/s50-network1.dat"))
#friend.data.w2 <- as.matrix(read.table("s50_data/s50-network2.dat"))
#friend.data.w3 <- as.matrix(read.table("s50_data/s50-network3.dat"))
#drink <- as.matrix(read.table("s50_data/s50-alcohol.dat"))
friendshipData <- array(c(fd2.w1, fd2.w2, fd2.w3), dim = c(16, 16, 3))
friendship <- sienaDependent(friendshipData)
alcohol <- varCovar(drink[20:35,])
mydata <- sienaDataCreate(friendship, alcohol)
M1eff <- getEffects(mydata)
M2eff <- includeEffects(M1eff, jumpXTransTrip, interaction1 = "alcohol")
M3eff <- includeEffects(M1eff, nbrDist2twice)
data(M1ests_bigfriends)
M1parms <- (means %>% filter(Model == "M1"))$means
M2parms <- (means %>% filter(Model == "M2"))$means
M3parms <- (means %>% filter(Model == "M3"))$means
# M1sims1000 <- saom_simulate(dat = mydata, 
#                             struct = M1eff,
#                        parms = as.numeric(M1parms), 
#                        N = 1000)
# M2sims1000 <- saom_simulate(dat = mydata, 
#                             struct = M2eff,
#                        parms = as.numeric(M2parms), 
#                        N = 1000)
# M3sims1000 <- saom_simulate(dat = mydata, 
#                             struct = M3eff,
#                        parms = as.numeric(M3parms), 
#                        N = 1000)
# M1simsdf <- sims_to_df(M1sims1000)
# M2simsdf <- sims_to_df(M2sims1000)
# M3simsdf <- sims_to_df(M3sims1000)
M1avg <- M1simsdf %>% filter(!is.na(to) & wave == 1) %>%
  group_by(from, to) %>% 
  summarise(count = n()) %>% 
  mutate(weight = ifelse(from == to, 0, count))
M1avgW2 <- M1simsdf %>% filter(!is.na(to) & wave == 2) %>%
  group_by(from, to) %>% 
  summarise(count = n()) %>% 
  mutate(weight = ifelse(from == to, 0, count))
M1avg2 <- M1simsdf %>% filter(!is.na(to)) %>%
 group_by(from, to, wave) %>%
 summarise(count = n()) %>%
 mutate(weight = ifelse(from == to, 0, count))
M2avg <- M2simsdf %>% filter(!is.na(to) & wave == 1) %>%
  group_by(from, to) %>% 
  summarise(count = n()) %>% 
  mutate(weight = ifelse(from == to, 0, count))
M2avgW2 <- M2simsdf %>% filter(!is.na(to) & wave == 2) %>%
  group_by(from, to) %>% 
  summarise(count = n()) %>% 
  mutate(weight = ifelse(from == to, 0, count))
M2avg2 <- M2simsdf %>% filter(!is.na(to)) %>%
 group_by(from, to, wave) %>%
 summarise(count = n()) %>%
 mutate(weight = ifelse(from == to, 0, count))
M3avg <- M3simsdf %>% filter(!is.na(to) & wave == 1) %>%
  group_by(from, to) %>% 
  summarise(count = n()) %>% 
  mutate(weight = ifelse(from == to, 0, count))
M3avgW2 <- M3simsdf %>% filter(!is.na(to) & wave == 2) %>%
  group_by(from, to) %>% 
  summarise(count = n()) %>% 
  mutate(weight = ifelse(from == to, 0, count))
M3avg2 <- M3simsdf %>% filter(!is.na(to)) %>%
 group_by(from, to, wave) %>%
 summarise(count = n()) %>%
 mutate(weight = ifelse(from == to, 0, count))

# M1avg$from <- paste0("V", M1avg$from)
# M1avg$to <- paste0("V", M1avg$to)
# M1avg2$from <- paste0("V", M1avg2$from)
# M1avg2$to <- paste0("V", M1avg2$to)
# M1avgW2$from <- paste0("V", M1avgW2$from)
# M1avgW2$to <- paste0("V", M1avgW2$to)
# M2avg$from <- paste0("V", M2avg$from)
# M2avg$to <- paste0("V", M2avg$to)
# M2avgW2$from <- paste0("V", M2avgW2$from)
# M2avgW2$to <- paste0("V", M2avgW2$to)
# M2avg2$from <- paste0("V", M2avg2$from)
# M2avg2$to <- paste0("V", M2avg2$to)
# M3avg$from <- paste0("V", M3avg$from)
# M3avg$to <- paste0("V", M3avg$to)
# M3avgW2$from <- paste0("V", M3avgW2$from)
# M3avgW2$to <- paste0("V", M3avgW2$to)
# M3avg2$from <- paste0("V", M3avg2$from)
# M3avg2$to <- paste0("V", M3avg2$to)
combinedavgs$linewidth[combinedavgs$cat %in% c("1st Wave", "2nd Wave")] <- .5
ggplot(data = combinedavgs) + 
  geom_net(aes(from_id = from, to_id = to, linewidth= linewidth),
           directed = T, curvature = .2, labelon=T, size = 5,
           arrow = arrow(angle = 20, type = "open", length = unit(.2, "cm")), ecolour = "#969696", fiteach = T, 
           color = "#252525", hjust = .5, vjust = .5, labelcolour = 'white', fontsize = 4, arrowgap = .02,
           singletons = T) + 
  ThemeNet + theme(panel.background = element_rect(color = 'black')) + 
  facet_wrap(~cat, nrow = 2)

# wave2drink <- drink2[,2]
# wave2drink <- as.data.frame(wave2drink)
# wave2drink$id <- paste0("V", 1:16)
# names(wave2drink)[1] <- "drink"
# wave3drink <- drink2[,3]
# wave3drink <- as.data.frame(wave3drink)
# wave3drink$id <- paste0("V", 1:16)
# names(wave3drink)[1] <- "drink"
# M1avgcov <- merge(M1avg, wave2drink, by.x = "from", by.y = "id", all = T)
# M2avgcov <- merge(M2avg, wave2drink, by.x = "from", by.y = "id", all = T)
# M3avgcov <- merge(M3avg, wave2drink, by.x = "from", by.y = "id", all = T)
# M1avgW2cov <- merge(M1avgW2, wave3drink, by.x = "from", by.y = "id", all = T)
# M2avgW2cov <- merge(M2avgW2, wave3drink, by.x = "from", by.y = "id", all = T)
# M3avgW2cov <- merge(M3avgW2, wave3drink, by.x = "from", by.y = "id", all = T)
# 
# M1avg[,-3] %>% spread(to, weight) %>% data.frame-> distmatM1
# rownames(distmatM1) <- distmatM1$from
# distmatM1 <- distmatM1[,-1]
# distmatM1 <- as.matrix(sqrt(1/distmatM1))
# distmatM1[is.na(distmatM1)] <- 0
# M1avgW2[,-3] %>% spread(to, weight) %>% data.frame-> distmatM1W2
# rownames(distmatM1W2) <- distmatM1W2$from
# distmatM1W2 <- distmatM1W2[,-1]
# distmatM1W2 <- as.matrix(sqrt(1/distmatM1W2))
# distmatM1W2[is.na(distmatM1W2)] <- 0
# M2avg[,-3] %>% spread(to, weight) %>% data.frame-> distmatM2
# rownames(distmatM2) <- distmatM2$from
# distmatM2 <- distmatM2[,-1]
# distmatM2 <- as.matrix(sqrt(1/distmatM2))
# distmatM2[is.na(distmatM2)] <- 0
# M2avgW2[,-3] %>% spread(to, weight) %>% data.frame-> distmatM2W2
# rownames(distmatM2W2) <- distmatM2W2$from
# distmatM2W2 <- distmatM2W2[,-1]
# distmatM2W2 <- as.matrix(sqrt(1/distmatM2W2))
# distmatM2W2[is.na(distmatM2W2)] <- 0
# M3avg[,-3] %>% spread(to, weight) %>% data.frame-> distmatM3
# rownames(distmatM3) <- distmatM3$from
# distmatM3 <- distmatM3[,-1]
# distmatM3 <- as.matrix(sqrt(1/distmatM3))
# distmatM3[is.na(distmatM3)] <- 0
# M3avgW2[,-3] %>% spread(to, weight) %>% data.frame-> distmatM3W2
# rownames(distmatM3W2) <- distmatM3W2$from
# distmatM3W2 <- distmatM3W2[,-1]
# distmatM3W2 <- as.matrix(sqrt(1/distmatM3W2))
# distmatM3W2[is.na(distmatM3W2)] <- 0

# cols <- brewer.pal(n = 4, "YlOrRd")
# 
# plotM1W2 <- ggplot(data = M1avgcov) + 
#   geom_net(aes(from_id = from, to_id = to, linewidth = weight/251, color = as.factor(drink)), 
#            arrowgap = .03, size = 3,arrowsize = .5,
#            directed = TRUE, curvature = .2,
#            #layout.alg = "fruchtermanreingold"
#            layout.alg = "kamadakawai",
#            layout.par = list(elen = distmatM1)
#            ) + 
#   scale_color_brewer(palette = "YlOrRd", name = "Drinking\nBehavior") + 
#   #labs(title = "Wave 2, M1") + 
#   scale_size_continuous(name = "Weight") + 
#   theme_net() + 
#   theme(plot.title = element_text(hjust = .5), legend.position = 'none',plot.margin=unit(c(0,0,0,0),"mm")) + 
#   labs(title = "Model 1 Simulated Wave 2")
# plotM1W3 <- ggplot(data = M1avgW2cov) + 
#   geom_net(aes(from_id = from, to_id = to, linewidth = weight/388.5, color = as.factor(drink)), 
#            arrowgap = .03, size = 3,arrowsize = .5,
#            directed = TRUE, curvature = .2,
#            #layout.alg = "fruchtermanreingold"
#            layout.alg = "kamadakawai",
#            layout.par = list(elen = distmatM1W2)
#            ) + 
#   scale_color_manual(values = cols[2:4], name = "Drinking\nBehavior") + 
#   #labs(title = "Wave 3, M1") + 
#   scale_size_continuous(name = "Weight") + 
#   theme_net() + 
#   theme(plot.title = element_text(hjust = .5), legend.position = 'none',plot.margin=unit(c(0,0,0,0),"mm"))
# plotM2W2 <- ggplot(data = M2avgcov) + 
#   geom_net(aes(from_id = from, to_id = to, linewidth = weight/325, color = as.factor(drink)), 
#            arrowgap = .03, size = 3,arrowsize = .5,
#            directed = TRUE, curvature = .2,
#            #layout.alg = "fruchtermanreingold"
#            layout.alg = "kamadakawai",
#            layout.par = list(elen = distmatM2)
#            ) + 
#   scale_color_brewer(palette = "YlOrRd", name = "Drinking\nBehavior") + 
#   #labs(title = "Wave 2, M2") + 
#   scale_size_continuous(name = "Weight") + 
#   theme_net() + 
#   theme(plot.title = element_text(hjust = .5), legend.position = 'none',plot.margin=unit(c(0,0,0,0),"mm")) + 
#   labs(title = "Model 2 Simulated Wave 2")
# plotM2W3 <- ggplot(data = M2avgW2cov) + 
#   geom_net(aes(from_id = from, to_id = to, linewidth = weight/357, color = as.factor(drink)), 
#            arrowgap = .03, size = 3,arrowsize = .5,
#            directed = TRUE, curvature = .2,
#            #layout.alg = "fruchtermanreingold"
#            layout.alg = "kamadakawai",
#            layout.par = list(elen = distmatM2W2)
#            ) + 
#   scale_color_manual(values = cols[2:4], name = "Drinking\nBehavior") + 
#   #labs(title = "Wave 3, M2") + 
#   scale_size_continuous(name = "Weight") + 
#   theme_net() + 
#   theme(plot.title = element_text(hjust = .5), legend.position = 'none',plot.margin=unit(c(0,0,0,0),"mm"))
# plotM3W2 <- ggplot(data = M3avgcov) + 
#   geom_net(aes(from_id = from, to_id = to, linewidth = weight/258, color = as.factor(drink)), 
#            arrowgap = .03, size = 3,arrowsize = .5,
#            directed = TRUE, curvature = .2,
#            #layout.alg = "fruchtermanreingold"
#            layout.alg = "kamadakawai",
#            layout.par = list(elen = distmatM3)
#            ) + 
#   scale_color_brewer(palette = "YlOrRd", name = "Drinking\nBehavior") + 
#   #labs(title = "Wave 2, M3") + 
#   scale_size_continuous(name = "Weight") + 
#   theme_net() + 
#   theme(plot.title = element_text(hjust = .5), legend.position = 'none',plot.margin=unit(c(0,0,0,0),"mm")) + 
#   labs(title = "Model 3 Simulated Wave 2")
# plotM3W3 <- ggplot(data = M3avgW2cov) + 
#   geom_net(aes(from_id = from, to_id = to, linewidth = weight/372.5, color = as.factor(drink)), 
#            arrowgap = .03, size = 3,arrowsize = .5,
#            directed = TRUE, curvature = .2,
#            #layout.alg = "fruchtermanreingold"
#            layout.alg = "kamadakawai",
#            layout.par = list(elen = distmatM3W2)
#            ) + 
#   scale_color_manual(values = cols[2:4], name = "Drinking\nBehavior") + 
#   #labs(title = "Wave 3, M3") + 
#   scale_size_continuous(name = "Weight") + 
#   theme_net() + 
#   theme(plot.title = element_text(hjust = .5), legend.position = 'none',plot.margin=unit(c(0,0,0,0),"mm"))
# 
# origW1 <- ggplot(data = alldat %>% filter(Wave==1), 
#        aes(from_id = X1, to_id = X2, color = as.ordered(drink))) +
#   geom_net(fiteach = FALSE, directed=TRUE, layout.alg = "kamadakawai", size = 3, arrowsize = .5, curvature = .2,
#            arrowgap = .03) +
#   scale_color_brewer(palette = "YlOrRd", name = "Drinking\nBehavior") + 
#   theme_net() +
#   theme(legend.position = 'none',plot.margin=unit(c(0,0,0,0),"mm"))
# origW2 <- ggplot(data = alldat %>% filter(Wave==2), 
#        aes(from_id = X1, to_id = X2, color = as.ordered(drink))) +
#   geom_net(fiteach = FALSE, directed=TRUE, layout.alg = "kamadakawai", size = 3, arrowsize = .5, curvature = .2,
#            arrowgap = .03) +
#   scale_color_brewer(palette = "YlOrRd", name = "Drinking\nBehavior") + 
#   theme_net() +
#   theme(legend.position = 'none',plot.margin=unit(c(0,0,0,0),"mm")) + 
#   labs(title = "Observed Wave 2")
# origW3 <- ggplot(data = alldat %>% filter(Wave==3), 
#        aes(from_id = X1, to_id = X2, color = as.ordered(drink))) +
#   geom_net(fiteach = FALSE, directed=TRUE, layout.alg = "kamadakawai", size = 3, arrowsize = .5, curvature = .2,
#            arrowgap = .03) +
#   scale_color_manual(values = cols[2:4], name = "Drinking\nBehavior") + 
#   theme_net() +
#   theme(legend.position = 'none',plot.margin=unit(c(0,0,0,0),"mm"))
# 
# grid.arrange(origW2, plotM1W2,
#               plotM2W2,plotM3W2, 
#               ncol=2, clip = T)
# alldataviz2 <- alldataviz + theme(legend.position = 'none')
# ggdraw() +
#   draw_plot(alldataviz2, x=0, y=.75, width=1, height=.25) + 
#   draw_plot(plotM1W2, .1, .5, .5, .25) +
#   draw_plot(plotM1W3, .3, .5, .5, .25) +
#   draw_plot(plotM2W2, .1, .25, .5, .25) +
#   draw_plot(plotM2W3, .3, .25, .5, .25) +
#   draw_plot(plotM3W2, .1, 0, .5, .25) +
#   draw_plot(plotM3W3, .3, 0, .5, .25) + 
#   draw_plot_label(label = c("Original Data", "M1: Wave 2", "M1: Wave 3", "M2: Wave 2", "M2: Wave 3", "M3: Wave 2", "M3: Wave 3"))
# 
# M1avg2$Model <- "M1"
# M2avg2$Model <- "M2"
# M3avg2$Model <- "M3"
# allavg <- rbind(as.data.frame(M1avg2), 
#                 as.data.frame(M2avg2),
#                 as.data.frame(M3avg2))
# 
# 
# ggplot(data = allavg) + 
#   geom_histogram(aes(x = weight), binwidth = 25) + 
#   facet_grid(wave~Model, labeller = "label_both") + 
#   labs(x = "Number of times an edge occurs in 1,000 simulations") +
#   theme_bw()

# ggplot(data = M3avgcov) + 
#   geom_net(aes(from_id = from, to_id = to, linewidth = weight/251, color = as.factor(drink)), 
#            directed = TRUE, curvature = .2,
#            ealpha = .75,
#            layout.alg = "kamadakawai",
#            layout.par = list(elen = distmatM2)
#            ) + 
#   scale_color_brewer(palette = "YlOrRd", name = "Drinking\nBehavior") + 
#   scale_size_continuous(name = "Weight") + 
#   theme_net()
# M1avg2$wave <- M1avg2$wave + 1
# alldrink <- data.frame(drink2[,2:3])
# alldrink <- gather(alldrink, wave,drink) %>% mutate(wave = parse_number(wave), id = rep(paste0("V",1:16),2))

# M1avg2cov <- merge(M1avg2, alldrink, by.x = c("from", "wave"), by.y = c("id", "wave"), all = T)
# ggplot(data = M1avg2cov) + # edge appears > 50% of the time. (is this actually what that is????) 
#   geom_net(aes(from_id = from, to_id = to, linewidth = weight/388, color = as.factor(drink)), 
#            directed = TRUE, curvature = .2, labelon = F,
#            #layout.alg = "fruchtermanreingold"
#            layout.alg = "kamadakawai", fiteach = T,
#            layout.par = list(elen = distmat)
#            ) + 
#   scale_color_brewer(palette = "YlOrRd", name = "Drinking\nBehavior") + 
#   facet_wrap(~wave, labeller = "label_both") + theme_net() + labs(title = "Average Wave 2 and Wave 3 Networks from Model M1")
# ggplot(data = M1avg %>% filter(weight >= 51)) + 
#   geom_net(aes(from_id = from, to_id = to, linewidth = weight/500), 
#            #directed = TRUE,
#            layout.alg = "mds", 
#            layout.par = list(var = 'user', dist = 'none', exp = 3,vm = distmat))
@


\st{What are some things that could go in here?:
\begin{itemize}
\item "average" networks -- you've already done this! Write it up, make several examples. How does the procedure for averaging generalize? What is it good for? 
\item distributions of fitted parameters. again, you've already done this!!! just write it up and put it in here. 
\item view correlations between model parameters under different model sets. eg 2 betas, 3 betas, 4 betas, etc.
\end{itemize}
 }

\subsection{Explore algorithms, not just end result}

\st{The last principle of model visualization is to explore the process of fitting the model, instead of just focusing on the end result. This principle is perhaps the most important for SAOMs because} the model fitting process in \texttt{RSiena} involves several simulation steps that are hidden from the user. Hiding the MCMC steps is practical and efficient if a researcher is primarily interested in fitting one model to a set of longitudinal network data, obtaining parameter estimates, \st{and drawing conclusions or making predictions}. We are more interested in \textit{how} the models are fit, so we extracted and explored the different steps of that process. 

A key component of each step of the SIENA method of moments algorithm is the ``microstep" process. A series of microsteps is obtained by simulating from the model in its current state, $x(t_m)$ with current parameter values $\mathbf{\theta}_{0} = \{\alpha_{1_0}, \dots \alpha_{{m-1}_0}, \beta_{1_0}, \dots, \beta_{K_0}\}$, to the next state, $x(t_{m+1})$. This microstep process stops when the simulated network has acheived the same number of differences, $C$, from $x(t_m)$ as $x(t_{m+1})$, where 
$$C = \sum_{i \neq j} |x_{ij}(t_{m+1}) - x_{ij}(t_{m})|.$$ 
This simulation process follows the steps of the continuous-time Markov chain. \st{Each tie change in the CTMC is referred to as one ``microstep". At each microstep}, an ``ego node" is selected to make a change, and the chosen ego node randomly makes one change in its ties according to the probabilities, $\{p_{ij}: i\neq j \in \{1, \dots, n\}\}$, determined by its objective function. The options for change are (1) removing a current tie, (2) adding a new tie, or (3) making no change at all. Saving and exploring all of these steps is not computationally efficient if one is only interested in estimating parameter values, but they can be saved and extracted using options in \texttt{RSiena}, which is exactly what we did to create our visualizations. Between two network observations $x(t_m)$ and $x(t_{m+1})$, there can be dozens, hundreds, or even thousands of microsteps, depending on the size of the network and the number of changes between two network observations. We wanted to view these in-between steps in order to better understand the behavior of the underlying continuous-time Markov chain. 

The first vizualization we present here is an animation of microsteps simulated to emulate the transition steps of the CTMC from wave 1 to wave 2 of the small friendship network example shown in Figure ~\ref{fig:alldataviz}. Movies similar to this animation were used to visualize the changes of dynamic networks in \citet{dynnetviz}. When each ego node is selected in a microstep, it is emphasized in the animation, then the associated edge either appears or disappears. If there are no changes at a particular microstep, no changes are seen. Some frames of the animations are shown in Figure~\ref{fig:makinggif}, and the full movie can be viewed at INSERT LINK TO GIF. 

<<makinggif, fig.cap='A selection of images in the microstep animation. The selected edges and nodes are emphasized by changing the size and the color, then when edges are removed, they fade out, shrinking in size, while the nodes change color and shrink to blend in with the other nodes.', cache=TRUE, message = FALSE, warning = FALSE, fig.height=3>>=
# Need old version of dplyr to get this to work: 
if (!require("dplyr050",character.only = TRUE)) {
      devtools::install_github("sctyner/dplyr050")
}

friend.data.w1 <- as.matrix(read.table("data/s50_data/s50-network1.dat"))
friend.data.w2 <- as.matrix(read.table("data/s50_data/s50-network2.dat"))
friend.data.w3 <- as.matrix(read.table("data/s50_data/s50-network3.dat"))
drink <- as.matrix(read.table("data/s50_data/s50-alcohol.dat"))
fd2.w1 <- friend.data.w1[20:35,20:35]
fd2.w2 <- friend.data.w2[20:35,20:35]
fd2.w3 <- friend.data.w3[20:35,20:35]
friendshipData <- array(c(fd2.w1, fd2.w2,fd2.w3), dim = c(16, 16, 3))

# running Siena
#friendship <- sienaDependent(friendshipData)
#alcohol <- varCovar(drink[20:35,])
#mydata <- sienaDataCreate(friendship, alcohol)
#myeffnull <- getEffects(mydata)
#myalgorithm <- sienaAlgorithmCreate(projname = 's16_3')
set.seed(823746)

ansnullchains <- get_chain_info(ansnull)
ansnullchains %>%
  filter(period == 1) %>%  #only look at chains from wave 1 to wave 2
  group_by(rep) %>%
  dplyr::select(rep, from = ego, to = alter) %>%
  mutate(val = as.numeric(!from == to),
         from = paste0("V", parse_number(from)+1), # make the chains
         to = paste0("V", parse_number(to)+1)) -> ansnullchainsw1w2
colnames(fd2.w1) <- paste0("V", 1:16)
rownames(fd2.w1) <- paste0("V", 1:16)
wave1friends <- fortify(as.adjmat(fd2.w1))
ms1 <- listMicrosteps(dat = wave1friends,
                      microsteps = filter(ansnullchainsw1w2, rep == 1))
microsteps <- ms1[1:20]

microsteps_df <- getMicrostepsDF(microsteps)
pte <- pretween_edges(microsteps = microsteps)
ptv <- pretween_vertices(pte = pte, layoutparams = list(n = 16))
# step 1 is remove, step 9 is add. 
# get tweened data for all microsteps
pall <- tween_microsteps(ptv, pte, microsteps_df)

static_pall <- filter(pall, .frame %in% c(15,20,22,24,26:28, 71:73, 75, 79, 81, 84))

static_pall$type <- NA
static_pall$type[which(static_pall$.frame %in% c(15,20,22,24,26:28))] <- "Remove Edge"
static_pall$type[which(static_pall$.frame %in% c(71:73, 75, 79, 81, 84))] <- "Add Edge"

set.seed(34569234)
create_net_animate(static_pall) + 
  facet_wrap(~.frame, nrow=2, labeller = 'label_both') + 
  ThemeNet + 
  theme(panel.background = element_rect(color = 'black'))
@

The top row of Figure~\ref{fig:makinggif} shows an edge being removed, and the bottom row shows one being added. In both cases, the ego nodes chosen to act change color from black to red, and they also increase greatly in size. In the case of an edge being removed, shown in the top row, the edge that currently exists is emphasized with the same color and size change that the node gets, and as the animation proceeds the edge shrinks to nothing, as the ego node shrinks and changes color back to its original black. If an edge is being added, as in the bottom row of the figure, the ego node's appearances changes in the sampe way as when the edge is being removed, whil the edge appears colored red from nothing, and grows to a large size, then changes color and size to match the rest of the edges, while the node shrinks and changes color to match the other nodes.  

We also use animation to view the changing structure of the adjacency matrix the microsteps. The adjacency matrices for the three waves of friendship data as shown in Figure~\ref{fig:alldataviz} are ordered by node id. There are 16 nodes in the data, numbered 1-16, and that order is used on the $x$ and $y$ axes for the matrix visualization. Viewing the adjacency matrices with this arbitrary ordering does not provide much information to the viewer about the underlying structure of the network. This lack of perceived structure would be exacerbated in an animation, so we adjust the ordering so that the viewer can perceive more of the structure of the graph. This process is known as matrix seriation \citep{seriation}. 

<<adjmatorder, message = FALSE, warning = FALSE, fig.cap="On the left, the starting friendship network represented in adjacency matrix form, ordered by vertex id. On the right, the same adjacency matrix is presented after ordering the vertices by one repitition of the microstep simulation process from wave one to wave two.", fig.width=8, fig.height=3>>=
# first present the original wave 1 adj mat, then present the adj mat after being ordered by the 1st pc of the big adjmat A. 
# reinstall new dplyr
numordersf <- data.frame(fd2.w1) %>% 
  mutate(from = rownames(fd2.w1)) %>% 
  gather(key = to ,value = value, -from)
pcaordersf <- numordersf

numordersf$from <- factor(numordersf$from, levels = paste0("V", 1:16))
numordersf$to <- factor(numordersf$to, levels = rev(paste0("V", 1:16)))

p1 <- ggplot(data = numordersf, aes(x = from, y = to)) + geom_tile(color = 'black', aes(fill = as.factor(value))) + 
  scale_fill_manual(values = c('white', 'black')) +
  scale_x_discrete(position = "top", labels = as.character(1:16)) +
  scale_y_discrete(labels = rev(as.character(1:16)))  + 
  ThemeNoNet + 
  theme(legend.position = 'none') + 
  coord_fixed() 

# histogram of number of MS in 1000 reps
# sfmsall %>% group_by(rep) %>% summarize(noMs = max(ms)) %>% ggplot() + geom_histogram(aes(x = noMs), binwidth = 5)

sfmsall <- read_csv("data/smallfriendsmicrosteps.csv")

sfmsall %>% 
  group_by(from, to) %>% 
  filter(rep == 1) %>% 
  summarise(count = n()) -> sfmsall_summ
sfmsall_summ$from <- factor(sfmsall_summ$from, levels = paste0('V', 1:16))
sfmsall_summ$to <- factor(sfmsall_summ$to, levels = paste0('V', 1:16))
sfmsall_summ %>% 
  spread(to, count, fill = 0) %>% 
  data.frame -> allmsrep1_adjmat
allmsrep1_adjmat$V16 <- 0
allmsrep1_adjmat <- allmsrep1_adjmat[,-1]
allmsrep1_adjmat <- as.matrix(allmsrep1_adjmat)
rownames(allmsrep1_adjmat) <- paste0("V", 1:16)

pca1 <- prcomp(allmsrep1_adjmat)
order1 <- names(sort(pca1$rotation[,1], decreasing = T))
order2 <- names(sort(abs(pca1$rotation[,1]), decreasing = T))

pcaordersf$from <- factor(pcaordersf$from, levels = rev(order1))
pcaordersf$to <- factor(pcaordersf$to, levels = order1)

p2 <- ggplot(data = pcaordersf, aes(x = to, y = from)) + geom_tile(color = 'black', aes(fill = as.factor(value))) + 
  scale_fill_manual(values = c('white', 'black')) +
  scale_x_discrete(position = "top", labels = str_replace(order1, "V", "")) + 
  scale_y_discrete(labels = rev(str_replace(order1, "V", ""))) + 
  ThemeNoNet + 
  theme(legend.position = 'none') +
  coord_fixed() 

grid.arrange(p1,p2, nrow = 1)
@

To reorder the vertices for the matrix visualization, we first constructed a cumulative adjaceny matrix, $\mathbf{A}^{cum}$, for the series of microsteps simulating the network from $x(t_m)$ to $x(t_{m+1})$. A single entry in the cumulative adjacency matrix, $\mathbf{A}^{cum}_{ij}$, is the total number of times the edge from node $i$ to node $j$ appears in the network from the initial observation, $x(t_m) \equiv X(0)$ to the final result of the last microstep, $X(R)$, where $R$ is the total number of microsteps taken: 
$$ \mathbf{A}^{cum}_{ij} = \sum_{r = 0}^R X_{ij}(r). $$
We then performed a principal component analysis on $\mathbf{A}^{cum}$, and used the values of the first principal component to order the vertices on the $x$ and $y$ axes for the adjacency matrix animation. For one such series of microsteps simulated by \texttt{RSiena}, we present the adjacency matrix ordered by the (arbitrary) vertex id alongside the seriated adjacency microsteps using the first principal component loading on the cumulative adjacency matrix, $\mathbf{A}^{cum}$, in Figure~\ref{fig:adjmatorder}. 

<<adjmatani, fig.cap='The adjacency matrix visualization animation for one series of microsteps. At the beginning of the animation, there are two connected components. By the end, the components appear to be more spread out.', fig.show='animate', warning = FALSE, message = FALSE, interval=.2, results='hide', fig.height=4>>=
adjmatall <- matrix(0, nrow = 16, ncol = 16)
rownames(adjmatall) <- paste0("V", 1:16)
colnames(adjmatall) <- paste0("V", 1:16)
dat2 <- sfmsall %>% filter(rep == 1)
adjmatall2 <- NULL
for(i in 0:55){
  MS <- dat2[dat2$ms == i,c("from", "to")]
  myadjmat <- adjmatall
  for(j in 1:nrow(MS)){
    myadjmat[MS[j,"from"][[1]],MS[j,"to"][[1]]] <- 1
  }
  adjmatall2 <- rbind(adjmatall2, myadjmat)
}

adjmatall2 <- cbind(adjmatall2, rep(0:55, each = 16))
adjmatall2 <- data.frame(adjmatall2)
adjmatall2$from <- rep(paste0("V", 1:16), 56)
names(adjmatall2)[17] <- "ms"
adjmatall2 %>% gather(to, val, V1:V16) -> adjmatall3 

adjmatall3$to <- factor(adjmatall3$to, levels = order1)
adjmatall3$from <- factor(adjmatall3$from, levels = rev(order1))
plots <- list()
for(i in 0:55){
plots[[i + 1]] <- ggplot(data = adjmatall3 %>% filter(ms == i)) + 
  geom_tile(aes(x = to, y = from, fill = as.factor(val)), color = 'black') + 
  scale_fill_manual(values = c("white", "black")) + 
  scale_x_discrete(position = "top", labels = str_replace(order1, "V", "")) + 
  scale_y_discrete(labels = rev(str_replace(order1, "V", ""))) + 
  ThemeNoNet + 
  theme(legend.position = 'none') +
  coord_fixed() 
}
plots 
@

Using the principal component analysis on $\mathbf{A}^{cum}$ to order the rows and columns of the adjacency matrix visualization clearly shows the two distinct connected components in the first wave of the network, which are difficult to find in the arbitrarily ordered visualization. We use this layout to fix the layout in the animation of one of the microstep process simulations. This animation, which is shown in Figure~\ref{fig:adjmatani} is very simple: a square appears or disappears in the animation as that edge appears or disappears in the microstep process. Through this animation, we can see edges appearing, and then later on disappearing. These in-between steps are not shown when we look at the network at our discrete observation points. By viewing this animation, we gain a better understanding of the dynamics of this model. 
<<histogramalters, fig.width = 8, fig.height=11, fig.cap="Each panel shows the theoretical (as lines) and empirical (as points) probabilities of the chosen ego node changing its tie to each of the other nodes. The color of the line indicates whether the tie from the ego to the alter node is being added, removed, or if there is no change to the network in this step.">>=
ansnullchains %>% group_by(period, rep) %>% 
  mutate(ms = row_number()) %>% 
  filter(ms == 1, period == 1) %>% 
  mutate(alterProb = exp(logAlterProb),
         ego = as.numeric(as.character(ego)),
         alter = as.numeric(as.character(alter))) %>% 
  ungroup() %>% 
  group_by(ego, alter) %>% 
  summarize(count = n(), alterProb = unique(alterProb)) %>% ungroup() %>% 
  group_by(ego) %>% 
  mutate(csum = cumsum(alterProb)) %>% 
  arrange(ego, csum) %>% 
  mutate(totego = sum(count), 
         empprob = count / totego) -> cumprobs 
cumprobs$ego <- factor(paste0("V",cumprobs$ego+1), levels = paste0("V", 1:16))
cumprobs$alter <- factor(paste0("V",cumprobs$alter+1), levels = paste0("V", 1:16))

# is the tie that's added new, removed, or does the network stay the same 

changes <- cumprobs %>% dplyr::select(ego, alter) %>% unique 

wave1friends2 <- wave1friends[,-3]
wave1friends2$type <- "remove"
changes2 <- left_join(changes, wave1friends2, c("ego" = "from", "alter" = "to"))

changes2$type[is.na(changes2$type)] <- ifelse(changes2$ego[is.na(changes2$type)] == changes2$alter[is.na(changes2$type)], "noChange", "add")

cumprobs %>% left_join(changes2) -> changes3

changes3$ego <- factor(changes3$ego, levels = paste0("V", 1:16), labels = 1:16) 
changes3$alter <- factor(changes3$alter, levels = paste0("V", 1:16), labels = 1:16) 

ggplot(data = changes3) + 
  geom_segment(
    aes(x = alter , xend = alter, y = 0, yend = alterProb, color=type), size = 2) + 
  geom_point(aes(x = alter, y = empprob), shape = 4, size = 2, alpha = .5) + 
  labs(x = "alter node", y = "probability of alter node") + 
  coord_flip() + 
  facet_wrap(~ego, labeller = "label_both") + 
  ThemeNoNet
# 
# cumprobs %>% ggplot() + geom_histogram(aes(x = alter, weight = count), color = 'black', fill = 'white', binwidth = 1) + scale_x_continuous(labels = paste0("V",1:16), breaks = 0:15) + coord_flip() + facet_wrap(~ego, labeller = "label_both") + theme_gray()

# cprobslist <- split(cumprobs, cumprobs$ego)
# plots <- list()
# for(i in 1:length(cprobslist)){
#   cprobslist[[i]] %>%
#   ggplot() + 
#   geom_histogram(aes(x = as.numeric(as.character(alter)),
#                      y = alterProb), stat = 'identity', fill = 'white', color = 'black') + 
#   geom_line(aes(x = as.numeric(as.character(alter)), 
#                 y = csum)) + 
#   geom_point(aes(x = as.numeric(as.character(alter)), 
#                 y = csum)) +
#   scale_x_continuous(limits = c(-.5, 15), breaks = 0:15, labels = 0:15) +   
#   geom_hline(yintercept = 1, color = 'red', linetype = 'dashed') + 
#   labs(x = "alter", y = expression(p[ij]), title = paste("Ego node:", names(cprobslist)[i])) + # ylab(expression(p[ij])) + 
#   theme_gray() + 
#     theme(plot.title = element_text(size = 10), 
#           axis.title = element_text(size = 8), 
#           plot.margin = unit(rep(0,4),"pt")) -> p
#   plots[[i]] <- p
# }
# grid.arrange(plots[[1]], plots[[2]], plots[[3]], plots[[4]],
#              plots[[5]], plots[[6]], plots[[7]], plots[[8]],
#              #plots[[9]], plots[[10]], plots[[11]], plots[[12]],
#              #plots[[13]], plots[[14]], plots[[15]], plots[[16]], 
#              ncol = 2, padding = unit(0, 'line'))
@

We also attempt to better understand the microstep process by visualizing the observed transition probabilites for the first microstep in the process. We only do the first step of many for now because the \texttt{RSiena} transition probabilities after the first step are only directly comparable for identical steps due to the conditioning on the current network state in the model. In other words, for $i \neq j \neq k \neq \ell \in \{1, \dots, n\}$, the probability of tie $x_{k\ell}$ changing in the second microstep is different in the case of $x_{ij}$ changing in the first step from the case of $x_{ij}$ not changing in the first step. Thus we have one transition probability for the first microstep taken for each of the 1,000 simulations to visualize. In Figure~\ref{fig:histogramalters}, we present each acting node, (ego node) and the resulting probabilities of tie changes. The probability shown by the bars is the theoretical probability, according to the objective function, of the ego node changing its tie to the alter node, while the probability shown by the points is the empirical probability of that change being made. The empirical probability is calculated by counting all instances of the ego node first, then computing the proportion of each different alter node change. In most cases, they are almost identical, which demonstrates that the algorithm is performing as expected. 

<<heatmap, fig.align='center', fig.cap='A heatmap showing the empirical transition probabilities for the first microstep in 1,000 simulations. The acting node is on the x-axis, and the receiving node is on the y-axis. Missing squares represent ties that did not occur.', fig.height=4>>=
ansnullchains %>% 
  filter(period == 1) %>% 
  group_by(rep) %>% 
  mutate(msno = row_number()) %>% 
  filter(msno == 1) %>% 
  mutate(ego = paste0("V",as.numeric(as.character(ego))+1),
         alter = paste0("V",as.numeric(as.character(alter))+1), 
         prob = exp(logAlterProb)) -> probsDat

# probsDat %>% group_by(ego, alter) %>% summarize(count = n()) %>% dim
# 103, 3
ggplot(data = probsDat, 
       aes(x = factor(ego, levels = order1), 
             y = factor(alter, levels = rev(order1)), 
             fill = prob)) + 
  geom_tile(color = 'black') + 
  scale_fill_gradient(low = 'white', high = 'blue', limits = c(0,1)) +
  labs(x = "ego", y = "alter")+
  scale_x_discrete(position = 'top', labels = str_replace(order1, "V", "")) +
  scale_y_discrete(labels = rev(str_replace(order1, "V", ""))) +
  ThemeNoNet + theme(panel.grid = element_blank()) + 
  coord_fixed()
@


Another way to view these transition probabilities is through the adjacency matrix visualization. In Figure~\ref{fig:heatmap}, we build on the concept of the ordered adjacency matrix of Figure~\ref{fig:adjmatorder}. This heatmap shows the transition probabilities of all ties that are changed in the first microstep of the 1,000 simulations. The heatmap is noticeably sparse: %showing the lack of coverage in the model.  
of the 256 possible steps, only 103, or about 40\%, are taken in the 1,000 simulated chains. 

<<allsteps, fig.cap="Two simulations (out of 1,000) of the microstep process from wave 1 to wave 2. The $x$ axis is the microstep number, with step 0 representing the first wave of data and the final step representing the second wave of data. We can see that many edges are underrepresented in this process: they are in the second wave, but never appear in the microsteps.">>=
smallfriends <- read_csv("data/smallfriends4Geomnet.csv")
names(smallfriends)[1:2] <- c("from", "to")
msall1 <- netvizinf::listMicrosteps(dat = wave1friends,
                      microsteps = filter(ansnullchainsw1w2, rep == 1))
msall1_df <- plyr::rbind.fill(lapply(X = seq_along(msall1), FUN = function(i, x) {
        x[[i]]$ms <- i - 1
        return(x[[i]])
    }, msall1))
msall1_df <- msall1_df %>% dplyr::select(from, to, ms)
wave2 <- smallfriends %>% filter(Wave == 2) %>% na.omit %>% data.frame()
msall1_df <- msall1_df %>% add_row(from = paste0("V",wave2$from),
                      to = paste0("V",wave2$to), 
                      ms = max(msall1_df$ms)+1)
edges <- msall1_df %>% 
  group_by(from, to) %>%
  summarize(edgeappears = n()) %>% 
  arrange(desc(edgeappears)) %>% ungroup() %>%
  mutate(eid = row_number())

msall1_df2 <- left_join(msall1_df, edges, by = c("from" = "from", "to" = "to"))

p1 <- ggplot(data = msall1_df2) + 
  geom_tile(aes(x = ms, y = -eid, fill = ms)) + 
  geom_vline(xintercept = c(0.5,81.5)) + 
  scale_fill_continuous(low = "red", high = "blue") + 
  ThemeNoNet + 
  theme(legend.position = 'none', axis.ticks.y = element_blank(),
        axis.text.y = element_blank(), axis.title = element_blank()) +
  labs(x = "Microstep")


# rep 2 
msall2 <- netvizinf::listMicrosteps(dat = wave1friends,
                      microsteps = filter(ansnullchainsw1w2, rep == 2))
msall2_df <- plyr::rbind.fill(lapply(X = seq_along(msall2), FUN = function(i, x) {
        x[[i]]$ms <- i - 1
        return(x[[i]])
    }, msall2))
msall2_df <- msall2_df %>% dplyr::select(from, to, ms)
msall2_df <- msall2_df %>% add_row(from = paste0("V",wave2$from),
                      to = paste0("V",wave2$to), 
                      ms = max(msall2_df$ms)+1)

edges2 <- msall2_df %>% 
  group_by(from, to) %>%
  summarize(edgeappears = n()) %>% 
  arrange(desc(edgeappears)) %>% ungroup() %>%
  mutate(eid = row_number())

msall2_df2 <- left_join(msall2_df, edges2, by = c("from" = "from", "to" = "to"))

p2 <- ggplot(data = msall2_df2) + 
  geom_tile(aes(x = ms, y = -eid, fill = ms)) + 
  geom_vline(xintercept = c(0.5,49.5)) + 
  scale_fill_continuous(low = "red", high = "blue") +
  ThemeNoNet + 
  theme(legend.position = 'none', axis.ticks.y = element_blank(),
        axis.text.y = element_blank(), axis.title = element_blank()) #+
  #labs(x = "Microstep", y = "Edges (most frequent at top)")

# rep 3 
msall3 <- netvizinf::listMicrosteps(dat = wave1friends,
                      microsteps = filter(ansnullchainsw1w2, rep == 3))
msall3_df <- plyr::rbind.fill(lapply(X = seq_along(msall3), FUN = function(i, x) {
        x[[i]]$ms <- i - 1
        return(x[[i]])
    }, msall3))
msall3_df <- msall3_df %>% dplyr::select(from, to, ms)
msall3_df <- msall3_df %>% add_row(from = paste0("V",wave2$from),
                      to = paste0("V",wave2$to), 
                      ms = max(msall3_df$ms)+1)

edges3 <- msall3_df %>% 
  group_by(from, to) %>%
  summarize(edgeappears = n()) %>% 
  arrange(desc(edgeappears)) %>% ungroup() %>%
  mutate(eid = row_number())

msall3_df2 <- left_join(msall3_df, edges3, by = c("from" = "from", "to" = "to"))

# p3 <- ggplot(data = msall3_df2) + 
#   geom_tile(aes(x = ms, y = -eid, fill = ms)) + 
#   geom_vline(xintercept = c(0.5,106.5)) + 
#   scale_fill_continuous(low = "red", high = "blue") +
#   ThemeNoNet + 
#   theme(legend.position = 'none', axis.ticks.y = element_blank(),
#         axis.text.y = element_blank(), axis.title = element_blank()) #+
#   #labs(x = "Microstep", y = "Edges (most frequent at top)")

grid.arrange(p1, p2, nrow = 2)
@

We also wanted to view the complete microstep process from the first wave, which we condition on in the model, to the second wave. The number of steps taken from wave 1 to wave 2 varies. In one set of 1,000 simulations from Model 1, the smallest number of steps taken was 58, and the longest was 248, with a mean of 106 and a median of 103. In the 1,000 simulations, the standard deviation of the number of microsteps was 22.8. In Figure~\ref{fig:allsteps}, we see two simulations of the process from wave 1 to wave 2, with wave 1 shown on the left, and wave 2 shown on the right. In each of the three plots, the $y$-axis contains the edges sorted by how often they appear in the networks along the way. We can see that some edges are there in the beginning, but disappear and never come back, while others appear a few steps in, only to dispappear again. There are also some edges that were observed in wave 2 that don't appear at all in the microstep process in a given simulation. 

<<alledgests, fig.height = 4, fig.cap="Visualizing all microsteps taken in 1,000 simulations from the model M1. The occurrence percent is split up into groups to correspond with its distribution: only about 10\\% of the edges appear more than 10\\% of the time in the 1,000 simulations, while about 60\\% appear less than 1\\% of the time. The first wave network is shown at microstep 0, and the second wave of the network is shown as the last microstep for comparison. We see that it is rare for a microstep process to last longer than 150 steps, and also that the edges that appear past the 150th step tend to be in either the first wave or the second wave.">>=
# get a plot of all microsteps from w1 to w2 by edge
alledges <- expand.grid(from = paste0("V", 1:16), to = paste0("V", 1:16))
alledges$eid <- 1:nrow(alledges)

sfmsall %>% left_join(alledges) -> sfmsall2 

sfmsall2 %>% group_by(rep) %>% mutate(isWave2 = (ms == max(ms))) -> sfmsall2

sfmsall2 %>% group_by(from, to, eid) %>% 
  summarize(total =n()) %>% 
  arrange(desc(total)) %>% 
  ungroup %>% 
  mutate(plotorder = row_number()) -> sfmsallorder 

sfmsall2 %>% group_by(ms, eid) %>% 
  summarise(count = n()) %>%
  mutate(alpha = count/1000) %>% 
  arrange(ms, eid) -> sfmsall3  

left_join(sfmsall3, sfmsallorder, by = "eid") -> sfmsall4 

#add true wave 2 to plot
wave2 <- na.omit(actual2)
wave2$from <- paste0("V", wave2$from)
wave2$to <- paste0("V", wave2$to)
wave2 <- left_join(wave2, alledges)
wave2 <- left_join(wave2, sfmsallorder)

sfmsall5 <- sfmsall4 %>% ungroup %>% add_row(ms = max(sfmsall4$ms) + 1, 
        eid = wave2$eid, 
        count = NA, 
        alpha = 1, 
        from = wave2$from,
        to = wave2$to,
        total = wave2$total, 
        plotorder = wave2$plotorder) 
sfmsall5$alphaFctr <- cut(sfmsall5$alpha, c(0,.01, .05, .10, .25, .5, .75, 1, 1.1))
ggplot(data = sfmsall5) + 
  geom_tile(aes(x = ms, y = reorder(eid,-plotorder), fill = alphaFctr)) +
  #scale_fill_gradient(low = 'grey90', high = '#0868ac', name = "occurrence %") + 
  scale_fill_brewer(palette = "Blues", name = "Occurrence %", labels = c("(0-1]%", "(1-5]%", "(5-10]%","(10-25]%", "(25-50]%", "(50-75]%", "(75-100]%")) + 
  labs(x = "Microstep No.", y = "Edges (ordered by total # occurrences)") + 
  ThemeNoNet + 
  theme(axis.ticks.y = element_blank(), axis.text.y = element_blank(), panel.grid = element_blank())
@

\st{We also combine 1,000 simulations from model M1 into a visualization like the one shown in Figure~\ref{fig:allsteps}. We first assign each possible edge an edge ID number so that we can keep track of it throughout all the microsteps and all the simlations. Then, we count up the total number of times each edge appears in the microstep process in each of the 1,000 simulations for use as an ordering variable later. We also count up the number of times an edge occurs in each microstep number in the 1,000 simulations. Since the number of microsteps in the process varies, the number of times an edge occurs decreases as the microstep number increases. Next, we compute a proportion, which we call the occurrence percentage, which is the number of times the edge occurred in a microstep divided by 1,000. Finally, we visualize all this information together in Figure ~\ref{fig:alledgests}. In this plot, all possible edges are shown, and we see that every one of the $16 \times 15 = 240$ possible edges in the network occurs at some point in the simulation process. We also see, however, that the process struggles to focus in on the edges in the second wave of the data. Ideally, we would like to see more occurrences of the second wave of data generally. About half of the edges in wave two are in the bottom half when ordered by number of occurrences, meaning they aren't appearing as much as they would if the model was truly excellent at capturing the mechanisms of tie change in the network. In addition, there is not a distinct pattern or structure that we can see in this particular edge visualization. The edges that the model starts with appear more often in the first 50 or so microsteps than any other set of edges. This makes sense, but it would be better if the model explored the space more than it currently does. It could be that the simple model M1 isn't complicated enough for the friendship dynamics in the data, or it could be that the chains in the MCMCs "get stuck" when exploring the parameter space.}

% \st{The algorithms here are many!
% \begin{enumerate}
% %\item First, the microsteps / CTMC between each time point (gifs, other animations)
% \item Phases of RSiena: phase 1 = initial parameter estimate
% \item phase 2 = iterative update of parameters - trace plots
% \item phase 3 = simulations for convergence diagnostics
% \end{enumerate}
%  } 

\hh{The visualizations should address most elements of structure described before, i.e. follow along in the setup (for descriptive and teaching purposes), the fitting (understanding the model and interpreting the results) and then diagnostics (how well does the model fit the data - where are the most differences to the actual data/what are the sensitive parameters?) }

% \hh{Some of the visualizations we talked about
% \begin{enumerate}
% \item visualization of the MCMC process:  movie of individual actors' step-by-step decisions; could be done with the ndtv package, as long as we can get individual steps out of RSiena
% \item global picture: 1000 simulated networks + alpha blending should give a pretty good idea of the most important edges
% %\item global structure + principal components (tensor pca)
% \end{enumerate}}


\section{Discussion} \label{sec:discussion}

We have used novel visualization methods in order to better understand the family of models known as stochastic actor-oriented model for social network. By looking at the underlying algorithms, exploring collections of these models, and viewing the model in the data space, we have been able to gain knowledge and appreciation for these complicated models and everything that goes into them. 

We have only just begun to scratch the surface of these complicated and multi-layered models for social networks. The \texttt{RSiena} software is incredibly powerful, and can fit a whole slew of much more flexible stochastic actor-oriented models than we have examined here. If a researcher thinks the network structure or an actor covariate effects the rate of change of the networ, there is a way to incorporate that belief into the rate function of the SAOM. More than one actor-level covariate can be included in the model, and way more than three parameters can be included in the objective function itself. In addition, \texttt{RSiena} allows the user to tell it which parameters lead to tie creation, and which parameters lead to tie endowment, or dissolution. We have used ``evaluation" parameters, which assume that creation and endowment are equal \citep{RSienamanual}. Finally, SAOMs and \texttt{RSiena} are able to also model behavior change of the actors in the network, which again is a capability we did not explore here. 

\bibliographystyle{asa}
\bibliography{references}

\end{document}
